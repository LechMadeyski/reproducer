ExtractExperimentData = function(DataSet,#
                                 ExperimentNames,#
                                 idvar = "ParticipantID",#
                                 timevar = "Period",#
                                 ConvertToWide = TRUE) {#
  # This algorithm reads the data for each experiment in a family (or a study)  and converts to the "wide" data format #
#
  NumExp = length(Filenames)#
  ExpData = list()#
  for (i in 1:NumExp) {#
    # Read each of the data sets into a separate list entry#
		tempdata =as.data.frame(base::subset(DataSet,ExperimentID==ExperimentNames[i]))#
#
# Find the wide version of the dataset based on the Period variable if required#
		if (ConvertToWide) {		#
			tempdata=stats::reshape(tempdata,idvar=idvar,timevar=timevar,direction="wide")}#
# Return the data in the tibble format		#
		ExpData[[i]] =tibble::as_tibble(tempdata)#
#
	}#
#
	return(ExpData)#
}
ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")
Metrics=c("Comprehension","Modification")
Groups=c("A","B","C","D")
Type=c(rep("4G",4))
StudyID="S2"
dataset2 = ExtractExperiment(dataset2,ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
dataset2 = ExtractExperimentData(dataset2,ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
ExtractExperimentData = function(DataSet,#
                                 ExperimentNames,#
                                 idvar = "ParticipantID",#
                                 timevar = "Period",#
                                 ConvertToWide = TRUE) {#
  # This algorithm reads the data for each experiment in a family (or a study)  and converts to the "wide" data format #
#
  NumExp = length(ExperimentNames)#
  ExpData = list()#
  for (i in 1:NumExp) {#
    # Read each of the data sets into a separate list entry#
		tempdata =as.data.frame(base::subset(DataSet,ExperimentID==ExperimentNames[i]))#
#
# Find the wide version of the dataset based on the Period variable if required#
		if (ConvertToWide) {		#
			tempdata=stats::reshape(tempdata,idvar=idvar,timevar=timevar,direction="wide")}#
# Return the data in the tibble format		#
		ExpData[[i]] =tibble::as_tibble(tempdata)#
#
	}#
#
	return(ExpData)#
}
dataset2 = ExtractExperimentData(dataset2,ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM
library("reproducer")
dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM
ReshapedData = ExtractExperimentData(dataset2,ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
ReshapedData[[1]]
paste()
paste
cbind
as.numeric
importFrom
ShortExperimentNames=c("E1","E2","E3","E4")
FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")
ReshapedData=ExtractExperimentData(dataset2, FileNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
Level1RData=ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
#script to obtain correlation coefficients#
#
#' @title CalculateRLevel1#
#' @description This function calculates the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for each sequence group and each outcome metric. The function returns both the exact r value and the r value based on pooled variances for each sequnce group and outcome metric#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @param Dataset This holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data set should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Groups This is a list that defined the sequence group identifiers used in the dataset.#
#' @param Metrics This is a list of metrics, e.g., ("Correctness","Time","Efficiency").#
#' @param ExperimentName This an identifiers used to define the specific experiment in the output from this function.#
#' @param Type this is a character string specifying whether the experiment is a two sequence group of four sequence group experiment.#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return table this is a tibble holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' # importFrom stats#
#' # importFrom var#
#' # importFrom tibble#
#' @example#
#' # ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' # ShortExperimentNames=c("E1","E2","E3","E4")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from a file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric for the first experiment.#
#'  CalculateRLevel1(Dataset=ReshapedData[[1]],StudyID,Groups=c("A","B","C","D"),ExperimentName=ShortExperimentNames[1],Metrics,Type=Type[1],Control)#
#' # A tibble: 8 x 15#
#'   Study Exp   Group Metric        Id    n     ControlFirst var1     var2    varp     ControlVarProp TPVarProp vardiff r       r.p    #
#'   <chr> <chr> <chr> <chr>         <chr> <chr> <chr>        <chr>    <chr>   <chr>    <chr>          <chr>     <chr>   <chr>   <chr>  #
#' 1 S2    E1    A     Comprehension S2E1A 6     FALSE        0.01826  0.01631 0.01728  0.472          0.528     0.04495 -0.3008 -0.3003#
#' 2 S2    E1    B     Comprehension S2E1B 6     TRUE         0.02007  0.03262 0.02634  0.381          0.381     0.01795 0.6788  0.6593 #
#' 3 S2    E1    C     Comprehension S2E1C 6     FALSE        0.003697 0.01554 0.009617 0.808          0.192     0.02139 -0.1421 -0.112 #
#' 4 S2    E1    D     Comprehension S2E1D 6     TRUE         0.0173   0.02011 0.0187   0.462          0.462     0.01479 0.6064  0.6047 #
#' 5 S2    E1    A     Modification  S2E1A 6     FALSE        0.05275  0.03835 0.04555  0.421          0.579     0.07348 0.1958  0.1934 #
#' 6 S2    E1    B     Modification  S2E1B 6     TRUE         0.01847  0.04818 0.03332  0.277          0.277     0.05083 0.2651  0.2373 #
#' 7 S2    E1    C     Modification  S2E1C 6     FALSE        0.00655  0.02438 0.01546  0.788          0.212     0.04467 -0.5437 -0.4443#
#' 8 S2    E1    D     Modification  S2E1D 6     TRUE         0.02222  0.02662 0.02442  0.455          0.455     0.06952 -0.4253 -0.4236#
CalculateRLevel1 = function(Dataset,#
                            StudyID,#
                            Groups = c("A", "B", "C", "D"),#
                            ExperimentName,#
                            Metrics,#
                            Type,#
                            Control) {#
  if (Type == "2G")#
    Groups = Groups[1:2]#
  else#
    Groups = Groups[1:4]#
#
  NumReps = length(Groups)#
  table = NULL#
#
  r.table = NULL#
  NumMets = length(Metrics)#
  N = length(Dataset$SequenceGroup.1)#
  rvalidreps = 0#
  for (j in 1:NumMets) {#
    for (i in 1:NumReps) {#
      # Analyse the wide data for each Sequence Group in a specific experiment for a specific metric#
      ss = base::subset(Dataset, SequenceGroup.1 == Groups[i])#
      # Select the data for the specific metric corresponding to each time period#
      name1 = base::paste(Metrics[j], "1", sep = ".")#
      name2 = base::paste(Metrics[j], "2", sep = ".")#
      res1 = base::subset(ss, select = name1)#
      n1 = length(ss$SequenceGroup.1)#
      res2 = base::subset(ss, select = name2)#
      n2 = length(ss$SequenceGroup.2) # n1 should equal n2#
      if (n1 != n2)#
        return("Incorrect subset")#
      # Analyse the difference data for each Metric#
      diff = res2 - res1#
#
      if (n1 > 1) {#
        vardiff = as.numeric(stats::var(diff))#
        var1 = as.numeric(stats::var(res1))#
        var2 = as.numeric(stats::var(res2))#
      }#
      else {#
        # Cannot construct a sensible variance#
        vardiff = 0#
        var1 = 0#
        var2 = 0#
      }#
#
	 if (vardiff > 0 & var1 > 0 & var2 > 0) {#
#
       # Ensure all variances are non-zero otherwise do not add to the row#
#
      # Identify which variance corresponds to the control condition#
      ControlFirst = FALSE#
      if (ss$Treatment.1[1] == Control)#
        ControlFirst = TRUE#
      if (ControlFirst)#
        controlvar = var1#
      else#
        controlvar = var2#
     	ControlVarProp = controlvar / (var1 + var2)#
      	TimePeriodVarProp = var1 / (var1 + var2)#
        # Calculate r without assuming variance homogeneity#
        r = as.numeric((var1 + var2 - vardiff) / (2 * sqrt(var1 * var2)))#
        r = signif(r, 4)#
        #  Calculate r assuming variance homogeneity#
        pooled.var = (var1 + var2) / 2#
        r.p = as.numeric((2 * pooled.var - vardiff) / (2 * pooled.var))#
        r.p = signif(r.p, 4)#
        var1 = signif(var1, 4)#
        var2 = signif(var2, 4)#
        vardiff = signif(vardiff, 4)#
        ControlVarProp = signif(ControlVarProp, 3)#
        TimePeriodVarProp = signif(TimePeriodVarProp, 3)#
    # Construct an Id that is unique for a specific study, experiment and sequence group but treats r-values for different metrics as repeated measures.#
        Id = base::paste(StudyID, ExperimentName, sep = "")#
        Id = base::paste(Id, Groups[i], sep = "")#
        row = base::cbind(#
          Study = StudyID,#
          Exp = ExperimentName,#
          Group = Groups[i],#
          Metric = Metrics[j],#
          Id = Id,#
          n = n1,#
          ControlFirst = ControlFirst,#
          var1 = var1,#
          var2 = var2,#
          varp = pooled.var,#
          ControlVarProp = ControlVarProp,#
          TPVarProp = TimePeriodVarProp,#
          vardiff = vardiff,#
          r = r,#
          r.p = r.p#
        )#
        table = tibble::as_tibble(rbind(table, row))#
      }#
    }#
#
  }#
  	#Coerce table columns to correct format#
	table$n=as.integer(table$n)#
	table$ControlFirst=as.logical(table$ControlFirst)#
	table$var1=as.numeric(table$var1)#
	table$var2=as.numeric(table$var2)#
	table$varp=as.numeric(table$varp)#
	table$ControlVarProp=as.numeric(table$ControlVarProp)#
	table$TPVarProp=as.numeric(table$TPVarProp)#
	table$vardiff=as.numeric(table$vardiff)#
	table$r=as.numeric(table$r)#
	table$r.p=as.numeric(table$r.p)#
  return(table)#
#
}
#' @title ConstructLevel1ExperimentRData#
#' @description This function returns the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for a group of 1 or more experiments for each sequence group and each outcome metric. For sets of 2 or more experiments, the experiments are assumed to be replicates and to report the same sets of Metrics and have the same Control treatment and use the same squence Group identifiers, but are not necessarily the same Type. We return both the exact r value and the r value based on pooled variances for each sequence group and outcome metric.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ConstructLevel1ExperimentRData#
#' @param Data This is a list parameter each entry in the list holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Group This is a list that defined the sequence group identifiers used in the dataset.#
#' @param ExperimentNames This a list of identifiers used to define each experiment in the output from this function.#
#' @param Metrics This is a list of of character strings identifying each outcome metric reported in each of the experiments in the set of replicated experiments.#
#' @param Type this is a list of character strings specifying for each experiment whether the experiment is a 2-group or 4-group experiment#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return R.Data.Table this is a dataframe holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' @example#
#' # #
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from each file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric in each experiment#
#' ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
#' #    Study Exp Group        Metric    Id n ControlFirst     var1    var2        varp ControlVarProp TPVarProp vardiff        r      r.p#
#' # 1     S2  E1     1 Comprehension S2E11 6        FALSE 0.018260 0.01631 0.017283333          0.472     0.528 0.04495 -0.30080 -0.30030#
#' # 2     S2  E1     2 Comprehension S2E12 6         TRUE 0.020070 0.03262 0.026341667          0.381     0.381 0.01795  0.67880  0.65930#
#' # 3     S2  E1     3 Comprehension S2E13 6        FALSE 0.003697 0.01554 0.009616667          0.808     0.192 0.02139 -0.14210 -0.11200#
#' # 4     S2  E1     4 Comprehension S2E14 6         TRUE 0.017300 0.02011 0.018703333          0.462     0.462 0.01479  0.60640  0.60470#
#' # 5     S2  E1     1  Modification S2E11 6        FALSE 0.052750 0.03835 0.045546667          0.421     0.579 0.07348  0.19580  0.19340#
#' # 6     S2  E1     2  Modification S2E12 6         TRUE 0.018470 0.04818 0.033321667          0.277     0.277 0.05083  0.26510  0.23730#
#' # 7     S2  E1     3  Modification S2E13 6        FALSE 0.006550 0.02438 0.015463333          0.788     0.212 0.04467 -0.54370 -0.44430#
#' # 8     S2  E1     4  Modification S2E14 6         TRUE 0.022220 0.02662 0.024416667          0.455     0.455 0.06952 -0.42530 -0.42360#
#' # 9     S2  E2     1 Comprehension S2E21 6        FALSE 0.019400 0.04250 0.030948333          0.687     0.313 0.04886  0.22710  0.21070#
#' # 10    S2  E2     2 Comprehension S2E22 6         TRUE 0.019820 0.01918 0.019496667          0.508     0.508 0.06696 -0.71730 -0.71720#
#' # 11    S2  E2     3 Comprehension S2E23 5        FALSE 0.095400 0.01452 0.054960000          0.132     0.868 0.05012  0.80340  0.54400#
#' # 12    S2  E2     4 Comprehension S2E24 5         TRUE 0.074630 0.00933 0.041980000          0.889     0.889 0.04517  0.73500  0.46200#
#' # 13    S2  E2     1  Modification S2E21 6        FALSE 0.018380 0.00567 0.012023333          0.236     0.764 0.03691 -0.62990 -0.53480#
#' # 14    S2  E2     2  Modification S2E22 6         TRUE 0.060300 0.01350 0.036896667          0.817     0.817 0.07579 -0.03494 -0.02701#
#' # 15    S2  E2     3  Modification S2E23 5        FALSE 0.092650 0.02255 0.057600000          0.196     0.804 0.07700  0.41790  0.33160#
#' # 16    S2  E2     4  Modification S2E24 5         TRUE 0.009380 0.07337 0.041375000          0.113     0.113 0.06493  0.33960  0.21530#
ConstructLevel1ExperimentRData = function(Data,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Groups,#
                                          Metrics,#
                                          Type,#
                                          Control) {#
  # This calls the algorithm that constructs the correlation values for each metric and each independent sequence group in an experiment. It identifies the data for each sequnce group in each experiment for each experiment and collates the reurned r-values for each sequence group#
  NumExp = length(ExperimentNames)#
#
  R.Data.Table = NULL#
#
  for (i in 1:NumExp) {#
    r.data = CalculateRLevel1(#
      Data[[i]],#
      Groups = Groups,#
      StudyID = StudyID,#
      ExperimentName = ExperimentNames[i],#
      Metrics,#
      Type = Type[i],#
      Control = Control#
    )#
#
    R.Data.Table = tibble:as_tibble(rbind(R.Data.Table, r.data))#
  }#
#
  return(R.Data.Table)#
}
ExtractExperimentData = function(DataSet,#
                                 ExperimentNames,#
                                 idvar = "ParticipantID",#
                                 timevar = "Period",#
                                 ConvertToWide = TRUE) {#
  # This algorithm reads the data for each experiment in a family (or a study)  and converts to the "wide" data format #
#
  NumExp = length(ExperimentNames)#
  ExpData = list()#
  for (i in 1:NumExp) {#
    # Read each of the data sets into a separate list entry#
		tempdata =as.data.frame(base::subset(DataSet,ExperimentID==ExperimentNames[i]))#
#
# Find the wide version of the dataset based on the Period variable if required#
		if (ConvertToWide) {		#
			tempdata=stats::reshape(tempdata,idvar=idvar,timevar=timevar,direction="wide")}#
# Return the data in the tibble format		#
		ExpData[[i]] =tibble::as_tibble(tempdata)#
#
	}#
#
	return(ExpData)#
}
ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")
ShortExperimentNames=c("E1","E2","E3","E4")
dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM
ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)
ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
Control="SC"
ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
ConstructLevel1ExperimentRData = function(Data,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Groups,#
                                          Metrics,#
                                          Type,#
                                          Control) {#
  # This calls the algorithm that constructs the correlation values for each metric and each independent sequence group in an experiment. It identifies the data for each sequnce group in each experiment for each experiment and collates the reurned r-values for each sequence group#
  NumExp = length(ExperimentNames)#
#
  R.Data.Table = NULL#
#
  for (i in 1:NumExp) {#
    r.data = CalculateRLevel1(#
      Data[[i]],#
      Groups = Groups,#
      StudyID = StudyID,#
      ExperimentName = ExperimentNames[i],#
      Metrics,#
      Type = Type[i],#
      Control = Control#
    )#
#
    R.Data.Table = tibble::as_tibble(rbind(R.Data.Table, r.data))#
  }#
#
  return(R.Data.Table)#
}
ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
sum
signif
CalculateLevel2ExperimentRData = function(Level1Data,#
                                          Groups,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Metrics,#
                                          Type) {#
  NumExp = length(ExperimentNames)#
#
  RExp.Table = NULL#
  NumMets = length(Metrics)#
  for (i in 1:NumExp) {#
    # Extract the variance data for a specific experiment#
    ExpData = base::subset(Level1Data, Exp == ExperimentNames[i])#
    if (Type[i] == "2G")#
      SequenceGroups = Groups[1:2]#
    else#
      SequenceGroups = Groups[1:4]#
#
    NumGroups = length(SequenceGroups)#
    ExpID = ExpData$Exp[1]#
    ID = base::paste(StudyID, ExpID, sep = "")#
    for (j in 1:NumMets) {#
      # Find the sequence group statistics for a specific Metric#
      MetData = base::subset(ExpData, Metric == Metrics[j])#
      # Find the number of participants in the experiment#
      NumParticipants = sum(MetData$n)#
#
      # Calculate the pooled sequence group variance and difference variance#
      PooledVar1 = sum(MetData$var1 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      PooledVar2 = sum(MetData$var2 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      VarProp = PooledVar1 / (PooledVar1 + PooledVar2)#
      PooledVar = sum(MetData$varp * (MetData$n - 1)) / (NumParticipants -#
                                                           NumGroups)#
      PooledDiffVar = sum(MetData$vardiff * (MetData$n - 1)) / (NumParticipants -#
                                                                  NumGroups)#
      # Calculate r#
      r = (2 * PooledVar - PooledDiffVar) / (2 * PooledVar)#
      # Tidy up calculated values#
      r = signif(r, 4)#
      PooledVar1 = signif(PooledVar1, 4)#
      PooledVar2 = signif(PooledVar2, 4)#
      PooledVar = signif(PooledVar, 4)#
      VarProp = signif(VarProp, 4)#
      PooledDiffVar = signif(PooledDiffVar, 4)#
      # Put the output into a format that identifes the study, experiment, experiment size and metric and the average r for the experiment#
      row = #
        cbind(#
          PooledVar1 = PooledVar1,#
          PooledVar2 = PooledVar2,#
          VarProp = VarProp,#
          PooledVar = PooledVar,#
          PooledDiffVar = PooledDiffVar,#
          r.Exp = r#
        )#
      RExp.Table = rbind(#
        RExp.Table,#
        cbind(#
          StudyID = StudyID,#
          ExpID = ID,#
          N = NumParticipants,#
          Metric = Metrics[j],#
          row#
        )#
      )#
#
    }#
#
  }#
#
	RExp.Table=tibble::as_tibble(RExp.Table)#
	# Coerce the data items to the correct formats#
	RExp.Table$N=as.integer(RExp.Table$N)#
	RExp.Table$PooledVar1=as.numeric(RExp.Table$PooledVar1)#
	RExp.Table$PooledVar2=as.numeric(RExp.Table$PooledVar2)#
	RExp.Table$PooledDiffVar=as.numeric(RExp.Table$PooledDiffVar)#
	RExp.Table$r.Exp=as.numeric(RExp.Table$r.Exp)#
	return(RExp.Table)#
#
}
Lev1Data= ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ShortExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
Lev1Data= ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)
CalculateLevel2ExperimentRData(Lev1Data,Groups=Groups,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Metrics=Metrics,Type=Type)
CalculateRLevel1(Dataset=ReshapedData[[1]],StudyID,Groups=c("A","B","C","D"),ExperimentName=ShortExperimentNames[1],Metrics,Type=Type[1],Control)
q()
69/(185+69)
40/(40+114)
q()
help(package="reproducer")
KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello15EMSE
reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello15EMSE
reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM
#' @title calculateSmallSampleSizeAdjustment#
#' @description Function calculates the small sample size adjustment for standardized mean effect sizes#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export calculateSmallSampleSizeAdjustment#
#' @param df A vector of degrees of freedom#
#' @param exact Default value=TRUE, If exact==TRUE the function returns the exact value of the adjustment(s) which is suitable for small values of df, if exact==FALSE the function returns the approximate version of the adjustment(s). See Hedges and Olkin 'Statistical methods for Meta-Analysis' Academic Press 1985.#
#' @return small sample size adjustment value#
#' @examples#
#' df <- 2#
#' c <- calculateSmallSampleSizeAdjustment(df)#
#'#
#' df=c(5,10,17)#
#' adjexact=calculateSmallSampleSizeAdjustment(df)#
#' # adjexact=0.8407487 0.9227456 0.9551115#
#' # Hedges and Olkin values 0.8408, 0.9228,0.9551#
#' adjapprox=calculateSmallSampleSizeAdjustment(df,FALSE)#
#' # adjapprox=0.8421053 0.9230769 0.9552239#
#' # Another example:#
#' df=c(10,25,50)#
#' calculateSmallSampleSizeAdjustment(df,exact=TRUE)#
#' calculateSmallSampleSizeAdjustment(df,exact=FALSE)#
calculateSmallSampleSizeAdjustment=function(df,exact=TRUE) {#
	exactvec=c(rep(exact,length(df)))#
	# If exact is TRUE but the df is too large gamma cannot be calculated and the approximate value is used#
  c =ifelse(exactvec & df<340,sqrt(2/df)*gamma(df/2)/gamma((df-1)/2) , (1 - 3 / (4 * df - 1)))#
  	return(c)#
}#
#' @title varStandardizedEffectSize#
#' @description Function calculates the exact variance of a standardized effect size based on the relationship between t and the standardized effect size, see Morris and DeShon, Combining Effect Size Estimates in Meta-Analysis With Repeated Measures and Independent-Groups Designs, Psychological Methods, 7 (1), pp 105-125.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export varStandardizedEffectSize#
#' @param d An unadjusted standardized effect size #
#' @param A The squared constant linking t and d i.e. t*sqrt(A)=d#
#' @param f The degrees of freedom of the t value#
#' @param returnVarg if set to TRUE return the variance of the small sample size adjusted standardized effect size (g), otherwise returns var(d) where d is the input parameter#
#' @return if returnVarg if set to TRUE, return var(g) otherwise var(d)#
#' @examples#
#' d=0.5#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=FALSE)#
#' # [1] 0.2010239#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=TRUE)#
#' # [1] 0.1904167#
varStandardizedEffectSize=function(d,A,f,returnVarg=TRUE){#
#
	c=reproducer::calculateSmallSampleSizeAdjustment(f)#
	g=d*c # g is a better estimate of the population standardized effect size delta than d#
	var=(f/(f-2))*(A + g^2) - g^2/c^2 # best estimate of the variance of d#
	if (returnVarg) var=c^2*var # best estimate of the variance of g#
	return(var)#
}#
#' @title RandomizedBlocksAnalysis#
#' @description The function performs a heteroscedastic test of a two treatment by J blocks randomized blocks effect size. The data are assumed to be stored in $x$ in list mode, a matrix or a data frame. If in list mode, length(x) is assumed to correspond to the total number of groups. All groups are assumed to be independent. Missing values are automatically removed.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedBlocksAnalysis#
#' @param x the structure holding the data. In list format, for a 2 treatment by J block randomized blocks experiments, there are  2J list elements each one specifying the outcome for a specific block and a specific treatment.#
#' @param con is a 2J list containing the contrast coefficients that are used to calculate the mean effet size.#
#' @param alpha=0.05 is the Type 1 error level used for the test of significance#
#' @return The t-test and its associated metrics (i.e. critical value stansard error and degrees of freedom) and the estimate of the contrast with its upper and lower confidence interval bounds and p-value.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' vec=c(-1,1,-1,1)/2#
#' RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)#
#
#' # $n#
#' # [1] 10 10 10 10#
#' # $test#
#' #      test     crit        se       df#
#' # [1,] 4.432644 2.038622 0.2798104 31.33793#
#' # $psihat#
#' #      psihat  ci.lower ci.upper      p.value#
#' # [1,] 1.2403 0.6698721 1.810728 0.0001062952#
RandomizedBlocksAnalysis<-function(x,con=c(-0.5,0.5,-0.5,0.5),alpha=.05){#
#
if(is.data.frame(x))x=as.matrix(x)#
#
flag<-TRUE#
if(alpha!= .05 && alpha!=.01)flag<-FALSE#
if(is.matrix(x))x<-listm(x)#
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")#
con<-as.matrix(con)#
if (ncol(con)>1) stop("Only one linear contrast permitted for a standard randomized blocks experiment")#
J<-length(x)#
sam=NA#
h<-vector("numeric",J)#
w<-vector("numeric",J)#
xbar<-vector("numeric",J)#
for(j in 1:J){#
xx<-!is.na(x[[j]])#
val<-x[[j]]#
x[[j]]<-val[xx]  # Remove missing values#
sam[j]=length(x[[j]])#
h[j]<-length(x[[j]])#
   # h is the number of observations in the jth group.#
w[j]<-((length(x[[j]])-1)*var(x[[j]]))/(h[j]*(h[j]-1)) # The variance of the jth group#
xbar[j]<-mean(x[[j]]) # The mean of the jth group#
}#
#if(sum(con^2)>0){#
if(nrow(con)!=length(x)){#
stop("The number of groups does not match the number of contrast coefficients.")#
}#
psihat<-matrix(0,1,4)#
dimnames(psihat)<-list(NULL,c("psihat","ci.lower","ci.upper",#
"p.value"))#
test<-matrix(0,1,4)#
dimnames(test)<-list(NULL,c("test","crit","se","df"))#
df<-0#
psihat[1,1]<-sum(con[,1]*xbar)#
sejk<-sqrt(sum(con[,1]^2*w)) # The pooled standard error of the contrast#
#
test[1,1]<-sum(con[,1]*xbar)/sejk # The value of the t-test#
df<-(sum(con[,1]^2*w))^2/sum(con[,1]^4*w^2/(h-1)) # Degrees of freeedom allowing for heterogeneity#
vv=(1-alpha/2) #
crit<-qt(vv,df) #The critical value of the t-test for the degrees of freedom#
test[1,2]<-crit#
test[1,3]<-sejk#
test[1,4]<-df#
psihat[1,2]<-psihat[1,1]-crit*sejk#
psihat[1,3]<-psihat[1,1]+crit*sejk#
psihat[1,4]<-2*(1-pt(abs(test[1,1]),df))#
list(n=sam,test=test,psihat=psihat)#
}#
#'  @title Kendalltaupb#
#'  @description  Computes point bi-serial version of  Kendall's tau plus a 1-alpha confidence interval using the method recommended by Long and Cliff (1997).  The algorithm is based on Wilcox's code but was extended to return the consistent variance and the confidence intervals based on the t-distribution. Also added a Diagnostic parameter to output internal calculations.#
#' @author Rand Wilcox, Barbara Kitchenham and Lech Madeyski#
#' @export varStandardizedEffectSize#
#' @param x either a matrix with two columns containg two correlated variables or a vector of variables#
#' @param y if y=NULL, assume x is a matrix with two columns, otherwise y is a vector of variables with x[i] and x[i] being from the same experimental unit#
#' @param alpha=0.05, the Type 1 error level used for statistical tests#
#' @return list containing the estimate of Kendall's tau, it hypothesis testing variance, and the t-test value obtained from it, the significnce of the t-test, the consistent variance of tau and its confidence intervals based on both the normal dstribution and the t-test (recommended by Long and Cliff)#
#' @examples#
#' x=c(1.2,3,1.8,2,2,0.5,0.5,1,3,1)#
#' y=c(1,1,1,1,1,0,0,0,0,0)#
#
#' Kendalltaupb(x,y,alpha=.05)#
#' # $cor#
#' # [1] 0.3555556#
#' # $ci#
#' # [1] -0.04198026  0.55555556#
#' # $cit#
#' # [1] -0.1240567  0.5555556#
#' # $test#
#' # [1] 1.431084#
#' # $sqse#
#' # [1] 0.0617284#
#' # $consistentvar#
#' # [1] 0.04113925#
#' # $siglevel#
#' # [1] 0.1524063#
Kendalltaupb<-function(x,y=NULL,alpha=.05){#
if (length(x)<=3) stop("Too few data points")#
if (length(x)!=length(y)) stop("Invalid input vectors")#
if (length(x)!=length(x[!is.na(x)])) stop("Missing values not permitted")#
if (length(y)!=length(y[!is.na(y)])) stop("Missing values not permitted")#
#
# Needs a test to ensure one of the variables contains only 1 or 0 values#
#
m=cbind(x,y) #
#
x=m[,1]#
y=m[,2]#
xdif<-outer(x,x,FUN="-")#
ydif<-outer(y,y,FUN="-")#
tv<-sign(xdif)*sign(ydif)#
#
#Corrects error in Wilcox's algorithm#
n<-length(x)#
dbar<-apply(tv,1,sum)/(n-1)#
#
tau<-sum(tv)/(n*(n-1))#
#
A<-sum((dbar-tau)^2)/(n-1)#
B<-(n*(n-1)*(-1)*tau^2+sum(tv^2))/(n^2-n-1)#
C<-(4*(n-2)*A+2*B)/(n*(n-1)) # C is the consistent variance#
#
# Confidence interval based on normal distribution - not recommended by Long and Cliff#
crit<-qnorm(alpha/2)#
cilow<-tau+crit*sqrt(C)#
cihi<-tau-crit*sqrt(C)#
if (cilow < (-n/(2*(n-1)))) cilow=-n/(2*(n-1)) # Applies limits assuming a point bi-serial tau#
#
if (cihi > n/(2*(n-1))) cihi=n/(2*(n-1)) # Applies limits assuming a point bi-serial tau#
# Confidence interval based on t distribution - recommended by Long and Cliff#
 vv=qt(alpha/2,n-3)#
#
cilowt=tau+vv*sqrt(C)#
if (cilowt < (-n/(2*(n-1)))) cilowt=-n/(2*(n-1))#
cihit=tau-vv*sqrt(C)#
if (cihit>n/(2*(n-1))) cihit=n/(2*(n-1))#
#
# t-test based on hypothesis test variance - not recommended by Long and Cliff#
se=sqrt((2*(2*n+5))/(9*n*(n-1)))#
test<-tau/se#
sqse=se^2#
#
siglevel<-2*(1-pnorm(abs(test)))#
if (Diagnostics) list(cor=tau,ci=c(cilow,cihi),cit=c(cilowt,cihit),test=test,sqse=se^2,consistentvar=C,siglevel=siglevel, dbar=dbar,A=A,B=B)#
	else list(cor=tau,ci=c(cilow,cihi),cit=c(cilowt,cihit),test=test,sqse=se^2,consistentvar=C,siglevel=siglevel)#
#
}#
#' @title Cliffd #
#' @description This function implements finds Cliff's d and its confidence intervals. The null hypothesis is that for two independent group, P(X<Y)=P(X>Y). The function reports a 1-alpha confidence interval for P(X>Y)-P(X<Y)The algorith computes a confidence interval for Cliff's d using the method in Cliff, 1996, p. 140, eq 5.12. The function is based on code produce by Rand Wilcox but has been amended. The plotting function has been removed and the dependnecy on Wilcox's binomci function has been removed. Construction of confidence intervals if values in one group are all larger than values in the other group has been amended to use the smallest non-zero variance method. Upper and lower confidence interval bounds cannot assume invalid vaues, i.e. values <-1 or >1. #
#' @author Rand Wilcox, amendments Barbara Kitchenham and Lech Madeyski #
#' @export Cliffd#
#' @param  x  is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @param sigfig is the number of significant digit. If sigfig>0 the data in x and y is truncated to the specified value. #
#' @return list in cluding the value of Cliffs d its consistent variance and confidence intervals and the equivalent probability of superiority value and its confience intervals.#
# ' @examples#
#' x=c(1.2,3,2.2,4,2.5,3)#
#' y=c(3,4.2,4,6,7,5.9)#
#' # Cliffd(x,y)#
#' #  $n1#
#' # [1] 6#
#' # $n2#
#' # [1] 6#
#' # $cl#
#' # [1] -0.9772519#
#' # $cu#
#' # [1] -0.3476461#
#' # $d#
#' # [1] -0.8611111#
#' # $sqse.d#
#' # [1] 0.02017931#
#' # $phat#
#' # [1] 0.06944444#
#' # $summary.dvals#
#' #        P(X<Y)     P(X=Y)     P(X>Y)#
#' # [1,] 0.8888889 0.08333333 0.02777778#
#' # $p.cl#
#' # [1] 0.01137405#
#' # $p.cu#
#' # [1] 0.326177#
#' # #
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#
#'Cliffd(z,y)#
#
#' # $n1#
#' # [1] 4#
#' # $n2#
#' # [1] 4#
#' # $cl#
#' # [1] -1#
#' # $cu#
#' # [1] -0.4368172#
#' # $d#
#' # [1] -1#
#' # $sqse.d#
#' # [1] 0.009765625#
#' # $phat#
#' # [1] 0#
#' # $summary.dvals#
#' #      P(X<Y) P(X=Y) P(X>Y)#
#' # [1,]      1      0      0#
#' # $p.cl#
#' # [1] 0#
#' # $p.cu#
#' # [1] 0.2815914#
#
Cliffd<-function(x,y,alpha=.05,sigfig=-1){#
# Check that the data is valid#
if (length(x)<=1) stop("Too few data points")#
if (length(y)<=1) stop("Too few data points")#
if (length(x)!=length(x[!is.na(x)])) stop("Missing values not permitted")#
if (length(y)!=length(y[!is.na(y)])) stop("Missing values not permitted")#
#
# Truncate the data if necessary#
if (sigfig>0) {#
	x=signif(x,sigfig)#
	y=signif(y,sigfig)#
}#
#
m<-outer(x,y,FUN="-")#
msave<-m#
m<-sign(m)#
#
d<-mean(m)#
#
phat<-(1+d)/2#
#
flag=TRUE#
if(phat==0 || phat==1)flag=FALSE#
q0<-sum(msave==0)/length(msave)#
qxly<-sum(msave<0)/length(msave)#
qxgy<-sum(msave>0)/length(msave)#
c.sum<-matrix(c(qxly,q0,qxgy),nrow=1,ncol=3)#
dimnames(c.sum)<-list(NULL,c("P(X<Y)","P(X=Y)","P(X>Y)"))#
if(flag){#
# This is appropriate for the consistent variance of d#
sigdih<-sum((m-d)^2)/(length(x)*length(y)-1)#
#
di<-NA#
for (i in 1:length(x))di[i]<-sum(x[i]>y)/length(y)-sum(x[i]<y)/length(y)#
#
dh<-NA#
for (i in 1:length(y))dh[i]<-sum(y[i]<x)/length(x)-sum(y[i]>x)/length(x)#
sdi<-var(di)#
sdh<-var(dh)#
# sh is the consistent variance of d#
 sh<-((length(y)-1)*sdi+(length(x)-1)*sdh+sigdih)/(length(x)*length(y))#
zv<-qnorm(alpha/2)#
cu<-(d-d^3-zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)#
cl<-(d-d^3+zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)#
}#
if(!flag){#
	# phat=0 and d=-1 or phat=1 and d=1#
# Cannot calculate standard error of d, use alternative minimum change approach to calculate a "close" slightly too large estimate. Calculates the smallest non-zero consistent variance#
sh=0#
di=NA#
dh=NA#
vardi=NA#
vardh=NA#
nm=max(c(length(x),length(y)))#
nx=length(x)#
ny=length(y)#
#
tempn=nx+ny-1#
#
if(phat==1) {#
	cu<-1#
	tempn=nx+ny-1#
	disturbedx=c(nx:tempn)#
	disturbedy=c(1:nx)#
	disturbedanalysis=Cliffd(disturbedx,disturbedy)#
	cl=disturbedanalysis$cl#
}#
#
if(phat==0) {#
	cl=-1#
	disturbedy=c(nx:tempn)#
	disturbedx=c(1:nx)#
	disturbedanalysis=Cliffd(disturbedx,disturbedy)#
	cu=disturbedanalysis$cu#
}#
#
sh=disturbedanalysis$sqse.d#
}#
# Need to construct confidence intervals on phat equivalent to the d confidence intervals#
pci=c((1+cu)/2,(1+cl)/2)#
#
 cid.results=list(n1=length(x),n2=length(y),cl=cl,cu=cu,d=d,sqse.d=sh,phat=phat,summary.dvals=c.sum,p.cl=pci[2],p.cu=pci[1])#
return(cid.results)#
}#
#
#' @title   calculatePhat#
#' @description This function extract the probability of superiority (i.e., Phat) and its confidence interval based on Brunner and Munzel (2000) heteroscedastic analog of WMW test. It is based on Wilcox'x bmp function with some amendments. It does not include a plotit facility. It uses the smallest non-zero variance to idetify confidence intervals and statistical significance for values of Phat=0 and Phat=1. It truncates data if required.#
#' @author Rand Wilcox amendments by Barbara Kitchenham and Lech Madeyski#
#' @export calculatePhat #
#' @param x is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @sigfig If sigfig>0 the data in x and y is truncated to the specified number of significant digits. #
#' @return list including the value of the t-test for PHat, the estimate of PHat and Cliff's d, and the confidence intervals for PHat.#
#' @examples #
#'x=c(1.2, 3.0, 2.2, 4.0, 2.5, 3.0)#
#'y=c(3,4.2,4,6,7,5.9)#
#'calculatePhat(x,y)#
#' # $test.stat#
#' # [1] 6.381249#
#' # $phat#
#' # [1] 0.9305556#
#' # $dhat#
#' # [1] 0.8611111#
#' # $sig.level#
#' # [1] 0.0001191725#
#' # $s.e.#
#' # [1] 0.06747199#
#' # $ci.p#
#' # [1] 0.7783001 1.0000000#
#' # $df#
#' # [1] 9.148489#
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#' calculatePhat(z,y)#
#' # $test.stat#
#' # [1] 10.6066#
#' # $phat#
#' # [1] 1#
#' # $dhat#
#' # [1] 1#
#' # $sig.level#
#' # [1] 4.135921e-05#
#' # $s.e.#
#' # [1] 0.04419417#
#' # $ci.p#
#' # [1] 0.8918608 1.0000000#
#' # $df#
#' # [1] 6#
calculatePhat<-function(x,y,alpha=.05,sigfig=-1){	#
#
if (sigfig>0) {#
	x=signif(x,sigfig)#
	y=signif(y,sigfig)#
}#
#
x<-x[!is.na(x)]  # Remove any missing values#
y<-y[!is.na(y)]#
n1<-length(x)#
n2<-length(y)#
N<-n1+n2#
n1p1<-n1+1#
flag1<-c(1:n1)#
flag2<-c(n1p1:N)#
R<-rank(c(x,y))#
R1<-mean(R[flag1])#
R2<-mean(R[flag2])#
phat<-(R2-(n2+1)/2)/n1#
dhat<-2*phat-1#
#
flag=TRUE # TRUE if phat ne 0 and phat ne 1#
if (phat==0 | phat==1) flag=FALSE#
#
if (flag) {#
Rg1<-rank(x)#
Rg2<-rank(y)#
S1sq<-sum((R[flag1]-Rg1-R1+(n1+1)/2)^2)/(n1-1)#
S2sq<-sum((R[flag2]-Rg2-R2+(n2+1)/2)^2)/(n2-1)#
sig1<-S1sq/n2^2#
sig2<-S2sq/n1^2#
se<-sqrt(N)*sqrt(N*(sig1/n1+sig2/n2))#
bmtest<-(R2-R1)/se#
#
df<-(S1sq/n2 + S2sq/n1)^2/((S1sq/n2)^2/(n1-1)+(S2sq/n1)^2/(n2-1))#
sig<-2 * (1 - pt(abs(bmtest),df))#
vv<-qt(alpha/2,df)#
ci.p<-c(phat+vv*se/N,phat-vv*se/N)#
#
	} #
else {#
	# Calculate the smallest non-negative variance and use results to approximate variance and confidence intervals of phat. Gives a more realistic confidence interval than other methods#
	Nl1=N-1#
	newx=c(1:n1)#
	newy=c(n1:Nl1)#
	newres=calculatePhat(newx,newy)#
	se=newres$s.e.*N#
	df=newres$df#
	sig=newres$sig.level#
	vv<-qt(alpha/2,df)#
	ci.p=c(phat+vv*se/N,phat-vv*se/N)#
	bmtest=newres$test.stat#
	if (sum(x)>sum(y)) bmtest=bmtest*-1#
}#
# Ensure confidence intervals dont assume impossible values#
 if (ci.p[1]<0) ci.p[1]=0#
 if (ci.p[2]>1) ci.p[2]=1#
list(test.stat=bmtest,phat=phat,dhat=dhat,sig.level=sig,s.e.=se/N,ci.p=ci.p,df=df)#
}#
#
#' @title  Calc4GroupNPStats#
#' @description This function does a non-parametric analysis of a randomized blocks experiment assuming 2 blocks and 2 treatment conditions. #
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export Calc4GroupNPStats#
#' @param x1 is the data associated with  treatment A in one block 1#
#' @param x2 is the data associated with treatement B in block 1#
#' @param x3 is the data associated with treatment A in block 2#
#' @param x4  is the data associated with treatment B in block 2#
#' @param signif is the number of significant digits in the data. If >0 the datav will be appropriately truncated#
#' @param alpha is he significance level for all statistical tests#
#' @return The function returns the point biserial version of Kendall's tau and its variance, Cliff's d and its variance, the probability of superiority, phat, and its variance, for the overall experiment.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' Calc4GroupNPStats(x[[1]],x[[2]],x[[3]],x[[4]],sigfig=-1,alpha=0.05,Diagnostics=FALSE)#
#
Calc4GroupNPStats=function(x1,x2,x3,x4,sigfig=-1,alpha=0.05){#
#
#     Check the significant digits to ensure that equal values are properly detected#
 	if (sigfig>0) {#
 		x1=signif(x1,sigfig)#
		x2=signif(x2,sigfig)#
		x3=signif(x3,sigfig)#
 		x4=signif(x4,sigfig)#
 	}#
# Set up a dummy variable such that the observations using treatment A in block 1 are associated with the value 1 and observations using treatment B in block 1 are associated with the value 0#
 	dummy1=c(rep(1,length(x1)),rep(0,length(x2)))#
# Concatenate the observations in block 1#
 	xCO1=c(x1,x2)#
#	Use Wilcox's function to find tau and its two variances for block 1#
	tau1=Kendalltaupb(xCO1,dummy1,Diagnostics=Diagnostics)#
	n1=length(x1)+length(x2)#
# Set up a dummy variable such that the observations using treatment A in block 2 are associated with the value 1 and observations using treatment B in block 2 are associated with the value 0#
#
 	dummy2=c(rep(1,length(x3)),rep(0,length(x4)))#
# Concatenate the observations in block 12#
 	xCO2=c(x3,x4)#
#	Use Wilcox's function to find tau and its two variances for block 2#
 	tau2=Kendalltaupb(xCO2,dummy2,Diagnostics=Diagnostics)#
 	n2=length(x3)+length(x4)#
 	N=n1+n2#
#
	average.tau=(tau1$cor+tau2$cor)/2#
 	combinedsqse=(tau1$sqse+tau2$sqse)/4#
 	indctvar=c(tau1$consistentvar,tau2$consistentvar)#
 	ctvar=(tau1$consistentvar+tau2$consistentvar)/4#
# Find the confidence limits on the combined tau using t-distribution#
    vv=qt(alpha/2,N-6)   #
    ci.t<-c(average.tau+vv*sqrt(combinedsqse),average.tau-vv*sqrt(combinedsqse))#
    sigCVt=ci.t[1]>0|ci.t[2]<0#
 	 # Find the confidence limits on the combined tau using normal distribution#
   ci.n<-c(average.tau+qnorm(alpha/2)*sqrt(combinedsqse),average.tau-qnorm(alpha/2)*sqrt(combinedsqse))#
   sigCVn=ci.n[1]>0|ci.n[2]<0#
# Find the average d and the combined variances for the full experiment #
	d=(Cliffd(x1,x2)$d+Cliffd(x3,x4)$d)/2#
	vard=(Cliffd(x1,x2)$sqse.d+Cliffd(x3,x4)$sqse.d)/4#
	zv<-qnorm(alpha/2)#
	d.cu<-(d-d^3-zv*sqrt(vard)*sqrt((1-d^2)^2+zv^2*vard))/(1-d^2+zv^2*vard)#
	d.cl<-(d-d^3+zv*sqrt(vard)*sqrt((1-d^2)^2+zv^2*vard))/(1-d^2+zv^2*vard)#
	d.sig=d.cu<0|d.cl>0#
#
# Find the average phat and the combined variances for the full experiment   #
	B1.BMP=calculatePhat(x2,x1)#
	B2.BMP=calculatePhat(x4,x3)#
	phat1=B1.BMP$phat#
	phat2=B2.BMP$phat#
	phat= (phat1+phat2)/2#
	se1=B1.BMP$s.e.#
	se2=B2.BMP$s.e.#
	se=4*N*sqrt((se1^2+se2^2)/4)#
	phat.se=sqrt((B1.BMP$s.e^2 + B2.BMP$s.e.^2)/4)#
	phat.var=phat.se^2#
	phat.test=4*N*(phat-0.5)/se#
	phat.df=B1.BMP$df+B2.BMP$df#
	phat.pvalue=2*(1-pt(abs(phat.test),phat.df))#
	phat.sig=phat.pvalue<0.05#
	indcor=c(tau1$cor,tau2$cor)#
	indsqse=c(tau1$sqse,tau2$sqse)#
#
output=data.frame(N=N,phat=phat,phat.var=phat.var,phat.df=phat.df,phat.test=phat.test,phat.pvalue=phat.pvalue, phat.sig=phat.sig,d=d,vard=vard,d.sig=d.sig,cor=average.tau,sqse=combinedsqse,ctvar=ctvar,n1=n1,n2=n2,sigCVt=sigCVt,sigCVn=sigCVn)#
#
return(output)#
 }#
writeNamedTable=function(table,name,reps) {#
# This outputs a simulation table with a meaningful name and identifying the number of replications used for the simulation	#
# Parameters: #
#	table: is the data table to be printed#
#	name: is the name of the text file to hold the data table#
# 	reps: is the number of replications used for the simulation#
#
	name=paste(name,as.character(reps),sep="_")#
	fullname=paste(name,"txt",sep=".")#
	write.table(table,fullname,sep=" & ",row.names=FALSE,quote=FALSE)#
}#
#
#' @title LaplaceDist#
#' @description Returns a sample of N observations from a Laplace distribution with specified mean and spread.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export LaplaceDist#
#' @param N is the required sample size#
#' @param mean is the required mean#
#' @param spread is the spread of the function#
#' @return N values from a Laplace distribution#
#' @examples#
#' set.seed(123)#
#' LaplaceDist(10,0,1)#
#' # [1] -0.55311564  0.85946218 -0.20094937  1.45258293  2.12808209 -2.39565480  0.05785263  1.53636446  0.10855453 -0.09076809#
#
LaplaceDist=function(N,mean, spread, max=0.5,min=-0.5) {#
	y=runif(N,min,max) # Get data from a uniform distribution#
	x=mean-spread*sign(y) *log(1-2*abs(y))#
	return(x)#
}#
# Functions for parametric analysis#
#
#' @title ExtractMAStatistics#
#' @description This function extracts summary statistics from meta-analysis results obtained from the rma function of the metafor R package. If required the function transform back to standardized mean difference (effect size type "d" i.e. Hg) or point biserial correlations (effect size type "r").#
#' Warning: the `ExtractMAStatistics` function works with `metafor` version 2.0-0, but changes to metafor's method of providing access to its individual results may introduce errors into the function.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractMAStatistics#
#' @param maresults is the output from the rma function.#
#' @param Nc is the number of participants in the control condition group.#
#' @param Nt is the number of participants in the treatment condition group.#
#' @param Transform is a boolean value indicating whether the outcome values need to be transformed back to standardized mean difference ("d" i.e. Hg) or point biserial correlations ("r"). It is defaulted to TRUE. If this parameter is set to FALSE, no transformation will be applied.#
#' @param type this indicates the type of transformation required - it defaults to "d" which requests transformation from Zr to Hg, using "r" requests transformation from Zr to r.#
#' @param sig indicates the number of significant digits requested in the output, the default is 4; it rounds the values of mean, pvalue, upper and lower bound to the specified number of significant digits.#
#' @ returnse=FALSE if set to TRUE returns the standard error of the effect size#
#' @return data frame incl. summary statistics from meta-analysis results: overall mean value for the effect sizes, the p-value of the mean, the upper and lower confidence interval bounds (UB and LB), QE which is the heterogeneity test statistic and QEp which the the p-value of the heterogeneity statistic#
#' @examples#
#' ExpData=reproducer::KitchenhamMadeyskiBrereton.ExpData#
#' #Extract the experiment basic statics#
#' S1data=subset(ExpData,ExpData=="S1")#
#' #Use the descriptive data to construct effect size#
#' S1EffectSizes = reproducer::PrepareForMetaAnalysisGtoR(#
#' S1data$Mc,S1data$Mt,S1data$SDc,S1data$SDt,S1data$Nc,S1data$Nt)#
#' # Do a random effect meta-analysis of the transformed r_pbs effect size#
#' S1MA = metafor::rma(S1EffectSizes$zr, S1EffectSizes$vi)#
#' # Extract summary statistics from meta-analysis results and transform back to Hg scale#
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4)#
#' #     mean   pvalue    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 1.122 0.2384  4 0.41 #
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4,TRUE)#
#' #     mean   pvalue     se    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 0.2128 1.122 0.2384  4 0.41#
#
ExtractMAStatistics=function(maresults,Nc, Nt,Transform=TRUE,type="d",sig=4,returnse=FALSE){#
#
pvalue=as.numeric(maresults$pval)  #
#
se=as.numeric(maresults$se)#
#
QE=as.numeric(maresults$QE)#
QEp=as.numeric(maresults$QEp)#
mean=as.numeric(maresults$beta)   #
UB=as.numeric(maresults$ci.ub)  #
LB=as.numeric(maresults$ci.lb) #
#
if(Transform & type=="d"){#
mean=reproducer::transformZrtoHg(mean,Nc,Nt)#
se=reproducer::transformZrtoHg(se,Nc,Nt)#
UB=reproducer::transformZrtoHg(UB,Nc,Nt)#
LB=reproducer::transformZrtoHg(LB,Nc,Nt)#
}#
if(Transform & type=="r"){#
mean=reproducer::transformZrtoR(mean)#
se=reproducer::transformZrtoR(se)#
#
UB=reproducer::transformZrtoR(UB) #
LB=reproducer::transformZrtoR(LB)#
}#
mean=signif(mean,sig)#
pvalue=signif(pvalue,sig)#
se=signif(se,sig)#
#
UB=signif(UB,sig)#
LB=signif(LB,sig)#
QE=signif(QE,2)#
QEp=signif(QEp,2)#
if (returnse)#
metaanalysisresults=data.frame(mean,pvalue,se,UB,LB,QE,QEp)	#
else metaanalysisresults=data.frame(mean,pvalue,UB,LB,QE,QEp)#
return(metaanalysisresults)#
#
}#
################################################################################
# Simulation functions#
#' @title NPESRelationships#
#' @description This simulates one of four distributions, and finds the values of ktau, phat and Cliffs d and their variances. It assumes equal group sizes. It returns values of the effect sizes and their variance for a simulated randomised experiment with two treatments.  It returns whether to not each non-parametric effect size was significant. It also returns the parametric (unstandardized and unstandardized) Effect Size and the whether the t-test was signficiant.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export NPESRelationships#
#' @param mean: The mean used for one of the treatment groups#
#' @param sd: The spread used for both treatment groups. It mus be a real value greater than 0.#
#' @param diff: This is added to the parameter mean, to define the mean of the other treatment group. It can be a real value avd can take the value zero.#
#' @param N: this is the number of observations in each group. It must be an interger greater than 3.#
#' @param type: this specifies the underlying distribution used to generate the data. it takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param StdAdj: this specifes the extent of variance instability introduced by the treatment.#
#' @return data frame incl. the non-parametric and parametric effect sizes and whether the effect sizes are significant at the 0.05 level.#
#' @examples#
#' set.seed(123)#
#' NPESRelationships(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)#
#' NPESRelationships(mean=0,sd=1,diff=0.8,N=10,type="l",StdAdj=0)#
NPESRelationships= function(mean,sd,diff,N,type="n",StdAdj=0) {#
#
if (type=="n")#
{ x=rnorm(N,mean,sd)#
y=rnorm(N,mean+diff,sd+StdAdj)#
}#
if (type=="g")#
{ #
	rate=mean#
	shape=sd#
x=rgamma(N,shape,rate)#
y=rgamma(N,shape+StdAdj,rate+diff)#
}#
if (type=="l")#
{ x=rlnorm(N,mean,sd)#
y=rlnorm(N,mean+diff,sd+StdAdj)#
transx=log(x)#
transy=log(y)#
}#
#
if (type=="lap")#
{#
	# Changed to use built-in defaults for max and min#
	x=LaplaceDist(N,mean,sd)#
	y=LaplaceDist(N,mean+diff,sd+StdAdj)#
}#
#
# Calculate the Kendall's tau value for the data#
#
dummy=c(rep(0,N),rep(1,N))	#
xy=c(x,y)#
# Calculate ktau for the data set#
ktest=Kendalltaupb(dummy,xy)#
#
ktau=ktest$cor#
sigCVt=ktest$cit[1]>0|ktest$cit[2]<0#
#
# Calculate Cliff's d for the data set and determine whether the value is significant #
ctest=Cliffd(y,x)#
#
d=ctest$d#
dvar=ctest$sqse#
#
d.lb=ctest$cl#
d.ub=ctest$cu#
#
dsig=d.lb>0|d.ub<0#
#
# Calculate phat statistics for the data set#
#
ptest=calculatePhat(x,y)#
#
phat=ptest$phat#
phat.lb=ptest$ci.p[1]#
phat.ub=ptest$ci.p[2]#
#
phat.sig=ptest$sig.level<0.05#
phat.df=ptest$df#
phat.var=ptest$s.e.^2#
# Add the results of a t-test as a baseline#
#
res=t.test(x,y)#
#
UES=mean(y)-mean(x)#
Var=(var(x)+var(y))/2#
MedDiff=median(y)-median(x)#
EffectSize=UES/sqrt(Var)#
pval=res$p.value#
#
	if  (type=="l") #
			{#
			# Check that log-normal datra gives appropriate values after transformation			#
			trans.t=t.test(transx,transy)#
			pval.trans=trans.t$p.value#
			ES.trans=mean(transy)-mean(transx)#
			VarTrans=(var(transx) + var(transy))/2#
			StdES.trans=ES.trans/sqrt(VarTrans)#
		}#
#
if (type=="l")#
output=data.frame(phat=phat,varphat=phat.var,dfphat=phat.df,sigphat=phat.sig,d=d,vard=dvar,sigd=dsig,cor=ktau,varcor=ktest$consistentvar,sigCVt=sigCVt,ttestp=pval,ES=UES,Variance=Var,StdES=EffectSize,MedDiff=MedDiff,transttest=pval.trans,EStrans=ES.trans,StdEStrans=StdES.trans,VarTrans=VarTrans)#
else#
output=data.frame(phat=phat,varphat=phat.var,dfphat=phat.df,sigphat=phat.sig,d=d,vard=dvar,sigd=dsig,cor=ktau,varcor=ktest$consistentvar,sigCVt=sigCVt,ttestp=pval,ES=UES,Variance=Var,StdES=EffectSize,MedDiff=MedDiff)#
return(output)#
}
NPESRelationships(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)
#' @title calculateSmallSampleSizeAdjustment#
#' @description Function calculates the small sample size adjustment for standardized mean effect sizes#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export calculateSmallSampleSizeAdjustment#
#' @param df A vector of degrees of freedom#
#' @param exact Default value=TRUE, If exact==TRUE the function returns the exact value of the adjustment(s) which is suitable for small values of df, if exact==FALSE the function returns the approximate version of the adjustment(s). See Hedges and Olkin 'Statistical methods for Meta-Analysis' Academic Press 1985.#
#' @return small sample size adjustment value#
#' @examples#
#' df <- 2#
#' c <- calculateSmallSampleSizeAdjustment(df)#
#'#
#' df=c(5,10,17)#
#' adjexact=calculateSmallSampleSizeAdjustment(df)#
#' # adjexact=0.8407487 0.9227456 0.9551115#
#' # Hedges and Olkin values 0.8408, 0.9228,0.9551#
#' adjapprox=calculateSmallSampleSizeAdjustment(df,FALSE)#
#' # adjapprox=0.8421053 0.9230769 0.9552239#
#' # Another example:#
#' df=c(10,25,50)#
#' calculateSmallSampleSizeAdjustment(df,exact=TRUE)#
#' calculateSmallSampleSizeAdjustment(df,exact=FALSE)#
calculateSmallSampleSizeAdjustment=function(df,exact=TRUE) {#
	exactvec=c(rep(exact,length(df)))#
	# If exact is TRUE but the df is too large gamma cannot be calculated and the approximate value is used#
  c =ifelse(exactvec & df<340,sqrt(2/df)*gamma(df/2)/gamma((df-1)/2) , (1 - 3 / (4 * df - 1)))#
  	return(c)#
}#
#' @title varStandardizedEffectSize#
#' @description Function calculates the exact variance of a standardized effect size based on the relationship between t and the standardized effect size, see Morris and DeShon, Combining Effect Size Estimates in Meta-Analysis With Repeated Measures and Independent-Groups Designs, Psychological Methods, 7 (1), pp 105-125.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export varStandardizedEffectSize#
#' @param d An unadjusted standardized effect size #
#' @param A The squared constant linking t and d i.e. t*sqrt(A)=d#
#' @param f The degrees of freedom of the t value#
#' @param returnVarg if set to TRUE return the variance of the small sample size adjusted standardized effect size (g), otherwise returns var(d) where d is the input parameter#
#' @return if returnVarg if set to TRUE, return var(g) otherwise var(d)#
#' @examples#
#' d=0.5#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=FALSE)#
#' # [1] 0.2010239#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=TRUE)#
#' # [1] 0.1904167#
varStandardizedEffectSize=function(d,A,f,returnVarg=TRUE){#
#
	c=reproducer::calculateSmallSampleSizeAdjustment(f)#
	g=d*c # g is a better estimate of the population standardized effect size delta than d#
	var=(f/(f-2))*(A + g^2) - g^2/c^2 # best estimate of the variance of d#
	if (returnVarg) var=c^2*var # best estimate of the variance of g#
	return(var)#
}#
#' @title RandomizedBlocksAnalysis#
#' @description The function performs a heteroscedastic test of a two treatment by J blocks randomized blocks effect size. The data are assumed to be stored in $x$ in list mode, a matrix or a data frame. If in list mode, length(x) is assumed to correspond to the total number of groups. All groups are assumed to be independent. Missing values are automatically removed.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedBlocksAnalysis#
#' @param x the structure holding the data. In list format, for a 2 treatment by J block randomized blocks experiments, there are  2J list elements each one specifying the outcome for a specific block and a specific treatment.#
#' @param con is a 2J list containing the contrast coefficients that are used to calculate the mean effet size.#
#' @param alpha=0.05 is the Type 1 error level used for the test of significance#
#' @return The t-test and its associated metrics (i.e. critical value stansard error and degrees of freedom) and the estimate of the contrast with its upper and lower confidence interval bounds and p-value.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' vec=c(-1,1,-1,1)/2#
#' RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)#
#
#' # $n#
#' # [1] 10 10 10 10#
#' # $test#
#' #      test     crit        se       df#
#' # [1,] 4.432644 2.038622 0.2798104 31.33793#
#' # $psihat#
#' #      psihat  ci.lower ci.upper      p.value#
#' # [1,] 1.2403 0.6698721 1.810728 0.0001062952#
RandomizedBlocksAnalysis<-function(x,con=c(-0.5,0.5,-0.5,0.5),alpha=.05){#
#
if(is.data.frame(x))x=as.matrix(x)#
#
flag<-TRUE#
if(alpha!= .05 && alpha!=.01)flag<-FALSE#
if(is.matrix(x))x<-listm(x)#
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")#
con<-as.matrix(con)#
if (ncol(con)>1) stop("Only one linear contrast permitted for a standard randomized blocks experiment")#
J<-length(x)#
sam=NA#
h<-vector("numeric",J)#
w<-vector("numeric",J)#
xbar<-vector("numeric",J)#
for(j in 1:J){#
xx<-!is.na(x[[j]])#
val<-x[[j]]#
x[[j]]<-val[xx]  # Remove missing values#
sam[j]=length(x[[j]])#
h[j]<-length(x[[j]])#
   # h is the number of observations in the jth group.#
w[j]<-((length(x[[j]])-1)*var(x[[j]]))/(h[j]*(h[j]-1)) # The variance of the jth group#
xbar[j]<-mean(x[[j]]) # The mean of the jth group#
}#
#if(sum(con^2)>0){#
if(nrow(con)!=length(x)){#
stop("The number of groups does not match the number of contrast coefficients.")#
}#
psihat<-matrix(0,1,4)#
dimnames(psihat)<-list(NULL,c("psihat","ci.lower","ci.upper",#
"p.value"))#
test<-matrix(0,1,4)#
dimnames(test)<-list(NULL,c("test","crit","se","df"))#
df<-0#
psihat[1,1]<-sum(con[,1]*xbar)#
sejk<-sqrt(sum(con[,1]^2*w)) # The pooled standard error of the contrast#
#
test[1,1]<-sum(con[,1]*xbar)/sejk # The value of the t-test#
df<-(sum(con[,1]^2*w))^2/sum(con[,1]^4*w^2/(h-1)) # Degrees of freeedom allowing for heterogeneity#
vv=(1-alpha/2) #
crit<-qt(vv,df) #The critical value of the t-test for the degrees of freedom#
test[1,2]<-crit#
test[1,3]<-sejk#
test[1,4]<-df#
psihat[1,2]<-psihat[1,1]-crit*sejk#
psihat[1,3]<-psihat[1,1]+crit*sejk#
psihat[1,4]<-2*(1-pt(abs(test[1,1]),df))#
list(n=sam,test=test,psihat=psihat)#
}#
#'  @title Kendalltaupb#
#'  @description  Computes point bi-serial version of  Kendall's tau plus a 1-alpha confidence interval using the method recommended by Long and Cliff (1997).  The algorithm is based on Wilcox's code but was extended to return the consistent variance and the confidence intervals based on the t-distribution. Also added a Diagnostic parameter to output internal calculations.#
#' @author Rand Wilcox, Barbara Kitchenham and Lech Madeyski#
#' @export varStandardizedEffectSize#
#' @param x either a matrix with two columns containg two correlated variables or a vector of variables#
#' @param y if y=NULL, assume x is a matrix with two columns, otherwise y is a vector of variables with x[i] and x[i] being from the same experimental unit#
#' @param alpha=0.05, the Type 1 error level used for statistical tests#
#' @return list containing the estimate of Kendall's tau, it hypothesis testing variance, and the t-test value obtained from it, the significnce of the t-test, the consistent variance of tau and its confidence intervals based on both the normal dstribution and the t-test (recommended by Long and Cliff)#
#' @examples#
#' x=c(1.2,3,1.8,2,2,0.5,0.5,1,3,1)#
#' y=c(1,1,1,1,1,0,0,0,0,0)#
#
#' Kendalltaupb(x,y,alpha=.05)#
#' # $cor#
#' # [1] 0.3555556#
#' # $ci#
#' # [1] -0.04198026  0.55555556#
#' # $cit#
#' # [1] -0.1240567  0.5555556#
#' # $test#
#' # [1] 1.431084#
#' # $sqse#
#' # [1] 0.0617284#
#' # $consistentvar#
#' # [1] 0.04113925#
#' # $siglevel#
#' # [1] 0.1524063#
Kendalltaupb<-function(x,y=NULL,alpha=.05){#
if (length(x)<=3) stop("Too few data points")#
if (length(x)!=length(y)) stop("Invalid input vectors")#
if (length(x)!=length(x[!is.na(x)])) stop("Missing values not permitted")#
if (length(y)!=length(y[!is.na(y)])) stop("Missing values not permitted")#
#
# Needs a test to ensure one of the variables contains only 1 or 0 values#
#
m=cbind(x,y) #
#
x=m[,1]#
y=m[,2]#
xdif<-outer(x,x,FUN="-")#
ydif<-outer(y,y,FUN="-")#
tv<-sign(xdif)*sign(ydif)#
#
#Corrects error in Wilcox's algorithm#
n<-length(x)#
dbar<-apply(tv,1,sum)/(n-1)#
#
tau<-sum(tv)/(n*(n-1))#
#
A<-sum((dbar-tau)^2)/(n-1)#
B<-(n*(n-1)*(-1)*tau^2+sum(tv^2))/(n^2-n-1)#
C<-(4*(n-2)*A+2*B)/(n*(n-1)) # C is the consistent variance#
#
# Confidence interval based on normal distribution - not recommended by Long and Cliff#
crit<-qnorm(alpha/2)#
cilow<-tau+crit*sqrt(C)#
cihi<-tau-crit*sqrt(C)#
if (cilow < (-n/(2*(n-1)))) cilow=-n/(2*(n-1)) # Applies limits assuming a point bi-serial tau#
#
if (cihi > n/(2*(n-1))) cihi=n/(2*(n-1)) # Applies limits assuming a point bi-serial tau#
# Confidence interval based on t distribution - recommended by Long and Cliff#
 vv=qt(alpha/2,n-3)#
#
cilowt=tau+vv*sqrt(C)#
if (cilowt < (-n/(2*(n-1)))) cilowt=-n/(2*(n-1))#
cihit=tau-vv*sqrt(C)#
if (cihit>n/(2*(n-1))) cihit=n/(2*(n-1))#
#
# t-test based on hypothesis test variance - not recommended by Long and Cliff#
se=sqrt((2*(2*n+5))/(9*n*(n-1)))#
test<-tau/se#
sqse=se^2#
#
siglevel<-2*(1-pnorm(abs(test)))#
list(cor=tau,ci=c(cilow,cihi),cit=c(cilowt,cihit),test=test,sqse=se^2,consistentvar=C,siglevel=siglevel)#
#
}#
#' @title Cliffd #
#' @description This function implements finds Cliff's d and its confidence intervals. The null hypothesis is that for two independent group, P(X<Y)=P(X>Y). The function reports a 1-alpha confidence interval for P(X>Y)-P(X<Y)The algorith computes a confidence interval for Cliff's d using the method in Cliff, 1996, p. 140, eq 5.12. The function is based on code produce by Rand Wilcox but has been amended. The plotting function has been removed and the dependnecy on Wilcox's binomci function has been removed. Construction of confidence intervals if values in one group are all larger than values in the other group has been amended to use the smallest non-zero variance method. Upper and lower confidence interval bounds cannot assume invalid vaues, i.e. values <-1 or >1. #
#' @author Rand Wilcox, amendments Barbara Kitchenham and Lech Madeyski #
#' @export Cliffd#
#' @param  x  is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @param sigfig is the number of significant digit. If sigfig>0 the data in x and y is truncated to the specified value. #
#' @return list in cluding the value of Cliffs d its consistent variance and confidence intervals and the equivalent probability of superiority value and its confience intervals.#
# ' @examples#
#' x=c(1.2,3,2.2,4,2.5,3)#
#' y=c(3,4.2,4,6,7,5.9)#
#' # Cliffd(x,y)#
#' #  $n1#
#' # [1] 6#
#' # $n2#
#' # [1] 6#
#' # $cl#
#' # [1] -0.9772519#
#' # $cu#
#' # [1] -0.3476461#
#' # $d#
#' # [1] -0.8611111#
#' # $sqse.d#
#' # [1] 0.02017931#
#' # $phat#
#' # [1] 0.06944444#
#' # $summary.dvals#
#' #        P(X<Y)     P(X=Y)     P(X>Y)#
#' # [1,] 0.8888889 0.08333333 0.02777778#
#' # $p.cl#
#' # [1] 0.01137405#
#' # $p.cu#
#' # [1] 0.326177#
#' # #
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#
#'Cliffd(z,y)#
#
#' # $n1#
#' # [1] 4#
#' # $n2#
#' # [1] 4#
#' # $cl#
#' # [1] -1#
#' # $cu#
#' # [1] -0.4368172#
#' # $d#
#' # [1] -1#
#' # $sqse.d#
#' # [1] 0.009765625#
#' # $phat#
#' # [1] 0#
#' # $summary.dvals#
#' #      P(X<Y) P(X=Y) P(X>Y)#
#' # [1,]      1      0      0#
#' # $p.cl#
#' # [1] 0#
#' # $p.cu#
#' # [1] 0.2815914#
#
Cliffd<-function(x,y,alpha=.05,sigfig=-1){#
# Check that the data is valid#
if (length(x)<=1) stop("Too few data points")#
if (length(y)<=1) stop("Too few data points")#
if (length(x)!=length(x[!is.na(x)])) stop("Missing values not permitted")#
if (length(y)!=length(y[!is.na(y)])) stop("Missing values not permitted")#
#
# Truncate the data if necessary#
if (sigfig>0) {#
	x=signif(x,sigfig)#
	y=signif(y,sigfig)#
}#
#
m<-outer(x,y,FUN="-")#
msave<-m#
m<-sign(m)#
#
d<-mean(m)#
#
phat<-(1+d)/2#
#
flag=TRUE#
if(phat==0 || phat==1)flag=FALSE#
q0<-sum(msave==0)/length(msave)#
qxly<-sum(msave<0)/length(msave)#
qxgy<-sum(msave>0)/length(msave)#
c.sum<-matrix(c(qxly,q0,qxgy),nrow=1,ncol=3)#
dimnames(c.sum)<-list(NULL,c("P(X<Y)","P(X=Y)","P(X>Y)"))#
if(flag){#
# This is appropriate for the consistent variance of d#
sigdih<-sum((m-d)^2)/(length(x)*length(y)-1)#
#
di<-NA#
for (i in 1:length(x))di[i]<-sum(x[i]>y)/length(y)-sum(x[i]<y)/length(y)#
#
dh<-NA#
for (i in 1:length(y))dh[i]<-sum(y[i]<x)/length(x)-sum(y[i]>x)/length(x)#
sdi<-var(di)#
sdh<-var(dh)#
# sh is the consistent variance of d#
 sh<-((length(y)-1)*sdi+(length(x)-1)*sdh+sigdih)/(length(x)*length(y))#
zv<-qnorm(alpha/2)#
cu<-(d-d^3-zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)#
cl<-(d-d^3+zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)#
}#
if(!flag){#
	# phat=0 and d=-1 or phat=1 and d=1#
# Cannot calculate standard error of d, use alternative minimum change approach to calculate a "close" slightly too large estimate. Calculates the smallest non-zero consistent variance#
sh=0#
di=NA#
dh=NA#
vardi=NA#
vardh=NA#
nm=max(c(length(x),length(y)))#
nx=length(x)#
ny=length(y)#
#
tempn=nx+ny-1#
#
if(phat==1) {#
	cu<-1#
	tempn=nx+ny-1#
	disturbedx=c(nx:tempn)#
	disturbedy=c(1:nx)#
	disturbedanalysis=Cliffd(disturbedx,disturbedy)#
	cl=disturbedanalysis$cl#
}#
#
if(phat==0) {#
	cl=-1#
	disturbedy=c(nx:tempn)#
	disturbedx=c(1:nx)#
	disturbedanalysis=Cliffd(disturbedx,disturbedy)#
	cu=disturbedanalysis$cu#
}#
#
sh=disturbedanalysis$sqse.d#
}#
# Need to construct confidence intervals on phat equivalent to the d confidence intervals#
pci=c((1+cu)/2,(1+cl)/2)#
#
 cid.results=list(n1=length(x),n2=length(y),cl=cl,cu=cu,d=d,sqse.d=sh,phat=phat,summary.dvals=c.sum,p.cl=pci[2],p.cu=pci[1])#
return(cid.results)#
}#
#
#' @title   calculatePhat#
#' @description This function extract the probability of superiority (i.e., Phat) and its confidence interval based on Brunner and Munzel (2000) heteroscedastic analog of WMW test. It is based on Wilcox'x bmp function with some amendments. It does not include a plotit facility. It uses the smallest non-zero variance to idetify confidence intervals and statistical significance for values of Phat=0 and Phat=1. It truncates data if required.#
#' @author Rand Wilcox amendments by Barbara Kitchenham and Lech Madeyski#
#' @export calculatePhat #
#' @param x is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @sigfig If sigfig>0 the data in x and y is truncated to the specified number of significant digits. #
#' @return list including the value of the t-test for PHat, the estimate of PHat and Cliff's d, and the confidence intervals for PHat.#
#' @examples #
#'x=c(1.2, 3.0, 2.2, 4.0, 2.5, 3.0)#
#'y=c(3,4.2,4,6,7,5.9)#
#'calculatePhat(x,y)#
#' # $test.stat#
#' # [1] 6.381249#
#' # $phat#
#' # [1] 0.9305556#
#' # $dhat#
#' # [1] 0.8611111#
#' # $sig.level#
#' # [1] 0.0001191725#
#' # $s.e.#
#' # [1] 0.06747199#
#' # $ci.p#
#' # [1] 0.7783001 1.0000000#
#' # $df#
#' # [1] 9.148489#
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#' calculatePhat(z,y)#
#' # $test.stat#
#' # [1] 10.6066#
#' # $phat#
#' # [1] 1#
#' # $dhat#
#' # [1] 1#
#' # $sig.level#
#' # [1] 4.135921e-05#
#' # $s.e.#
#' # [1] 0.04419417#
#' # $ci.p#
#' # [1] 0.8918608 1.0000000#
#' # $df#
#' # [1] 6#
calculatePhat<-function(x,y,alpha=.05,sigfig=-1){	#
#
if (sigfig>0) {#
	x=signif(x,sigfig)#
	y=signif(y,sigfig)#
}#
#
x<-x[!is.na(x)]  # Remove any missing values#
y<-y[!is.na(y)]#
n1<-length(x)#
n2<-length(y)#
N<-n1+n2#
n1p1<-n1+1#
flag1<-c(1:n1)#
flag2<-c(n1p1:N)#
R<-rank(c(x,y))#
R1<-mean(R[flag1])#
R2<-mean(R[flag2])#
phat<-(R2-(n2+1)/2)/n1#
dhat<-2*phat-1#
#
flag=TRUE # TRUE if phat ne 0 and phat ne 1#
if (phat==0 | phat==1) flag=FALSE#
#
if (flag) {#
Rg1<-rank(x)#
Rg2<-rank(y)#
S1sq<-sum((R[flag1]-Rg1-R1+(n1+1)/2)^2)/(n1-1)#
S2sq<-sum((R[flag2]-Rg2-R2+(n2+1)/2)^2)/(n2-1)#
sig1<-S1sq/n2^2#
sig2<-S2sq/n1^2#
se<-sqrt(N)*sqrt(N*(sig1/n1+sig2/n2))#
bmtest<-(R2-R1)/se#
#
df<-(S1sq/n2 + S2sq/n1)^2/((S1sq/n2)^2/(n1-1)+(S2sq/n1)^2/(n2-1))#
sig<-2 * (1 - pt(abs(bmtest),df))#
vv<-qt(alpha/2,df)#
ci.p<-c(phat+vv*se/N,phat-vv*se/N)#
#
	} #
else {#
	# Calculate the smallest non-negative variance and use results to approximate variance and confidence intervals of phat. Gives a more realistic confidence interval than other methods#
	Nl1=N-1#
	newx=c(1:n1)#
	newy=c(n1:Nl1)#
	newres=calculatePhat(newx,newy)#
	se=newres$s.e.*N#
	df=newres$df#
	sig=newres$sig.level#
	vv<-qt(alpha/2,df)#
	ci.p=c(phat+vv*se/N,phat-vv*se/N)#
	bmtest=newres$test.stat#
	if (sum(x)>sum(y)) bmtest=bmtest*-1#
}#
# Ensure confidence intervals dont assume impossible values#
 if (ci.p[1]<0) ci.p[1]=0#
 if (ci.p[2]>1) ci.p[2]=1#
list(test.stat=bmtest,phat=phat,dhat=dhat,sig.level=sig,s.e.=se/N,ci.p=ci.p,df=df)#
}#
#
#' @title  Calc4GroupNPStats#
#' @description This function does a non-parametric analysis of a randomized blocks experiment assuming 2 blocks and 2 treatment conditions. #
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export Calc4GroupNPStats#
#' @param x1 is the data associated with  treatment A in one block 1#
#' @param x2 is the data associated with treatement B in block 1#
#' @param x3 is the data associated with treatment A in block 2#
#' @param x4  is the data associated with treatment B in block 2#
#' @param signif is the number of significant digits in the data. If >0 the datav will be appropriately truncated#
#' @param alpha is he significance level for all statistical tests#
#' @return The function returns the point biserial version of Kendall's tau and its variance, Cliff's d and its variance, the probability of superiority, phat, and its variance, for the overall experiment.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' Calc4GroupNPStats(x[[1]],x[[2]],x[[3]],x[[4]],sigfig=-1,alpha=0.05)#
#
Calc4GroupNPStats=function(x1,x2,x3,x4,sigfig=-1,alpha=0.05){#
#
#     Check the significant digits to ensure that equal values are properly detected#
 	if (sigfig>0) {#
 		x1=signif(x1,sigfig)#
		x2=signif(x2,sigfig)#
		x3=signif(x3,sigfig)#
 		x4=signif(x4,sigfig)#
 	}#
# Set up a dummy variable such that the observations using treatment A in block 1 are associated with the value 1 and observations using treatment B in block 1 are associated with the value 0#
 	dummy1=c(rep(1,length(x1)),rep(0,length(x2)))#
# Concatenate the observations in block 1#
 	xCO1=c(x1,x2)#
#	Use Wilcox's function to find tau and its two variances for block 1#
	tau1=Kendalltaupb(xCO1,dummy1)#
	n1=length(x1)+length(x2)#
# Set up a dummy variable such that the observations using treatment A in block 2 are associated with the value 1 and observations using treatment B in block 2 are associated with the value 0#
#
 	dummy2=c(rep(1,length(x3)),rep(0,length(x4)))#
# Concatenate the observations in block 12#
 	xCO2=c(x3,x4)#
#	Use Wilcox's function to find tau and its two variances for block 2#
 	tau2=Kendalltaupb(xCO2,dummy2)#
 	n2=length(x3)+length(x4)#
 	N=n1+n2#
#
	average.tau=(tau1$cor+tau2$cor)/2#
 	combinedsqse=(tau1$sqse+tau2$sqse)/4#
 	indctvar=c(tau1$consistentvar,tau2$consistentvar)#
 	ctvar=(tau1$consistentvar+tau2$consistentvar)/4#
# Find the confidence limits on the combined tau using t-distribution#
    vv=qt(alpha/2,N-6)   #
    ci.t<-c(average.tau+vv*sqrt(combinedsqse),average.tau-vv*sqrt(combinedsqse))#
    sigCVt=ci.t[1]>0|ci.t[2]<0#
 	 # Find the confidence limits on the combined tau using normal distribution#
   ci.n<-c(average.tau+qnorm(alpha/2)*sqrt(combinedsqse),average.tau-qnorm(alpha/2)*sqrt(combinedsqse))#
   sigCVn=ci.n[1]>0|ci.n[2]<0#
# Find the average d and the combined variances for the full experiment #
	d=(Cliffd(x1,x2)$d+Cliffd(x3,x4)$d)/2#
	vard=(Cliffd(x1,x2)$sqse.d+Cliffd(x3,x4)$sqse.d)/4#
	zv<-qnorm(alpha/2)#
	d.cu<-(d-d^3-zv*sqrt(vard)*sqrt((1-d^2)^2+zv^2*vard))/(1-d^2+zv^2*vard)#
	d.cl<-(d-d^3+zv*sqrt(vard)*sqrt((1-d^2)^2+zv^2*vard))/(1-d^2+zv^2*vard)#
	d.sig=d.cu<0|d.cl>0#
#
# Find the average phat and the combined variances for the full experiment   #
	B1.BMP=calculatePhat(x2,x1)#
	B2.BMP=calculatePhat(x4,x3)#
	phat1=B1.BMP$phat#
	phat2=B2.BMP$phat#
	phat= (phat1+phat2)/2#
	se1=B1.BMP$s.e.#
	se2=B2.BMP$s.e.#
	se=4*N*sqrt((se1^2+se2^2)/4)#
	phat.se=sqrt((B1.BMP$s.e^2 + B2.BMP$s.e.^2)/4)#
	phat.var=phat.se^2#
	phat.test=4*N*(phat-0.5)/se#
	phat.df=B1.BMP$df+B2.BMP$df#
	phat.pvalue=2*(1-pt(abs(phat.test),phat.df))#
	phat.sig=phat.pvalue<0.05#
	indcor=c(tau1$cor,tau2$cor)#
	indsqse=c(tau1$sqse,tau2$sqse)#
#
output=data.frame(N=N,phat=phat,phat.var=phat.var,phat.df=phat.df,phat.test=phat.test,phat.pvalue=phat.pvalue, phat.sig=phat.sig,d=d,vard=vard,d.sig=d.sig,cor=average.tau,sqse=combinedsqse,ctvar=ctvar,n1=n1,n2=n2,sigCVt=sigCVt,sigCVn=sigCVn)#
#
return(output)#
 }#
writeNamedTable=function(table,name,reps) {#
# This outputs a simulation table with a meaningful name and identifying the number of replications used for the simulation	#
# Parameters: #
#	table: is the data table to be printed#
#	name: is the name of the text file to hold the data table#
# 	reps: is the number of replications used for the simulation#
#
	name=paste(name,as.character(reps),sep="_")#
	fullname=paste(name,"txt",sep=".")#
	write.table(table,fullname,sep=" & ",row.names=FALSE,quote=FALSE)#
}#
#
#' @title LaplaceDist#
#' @description Returns a sample of N observations from a Laplace distribution with specified mean and spread.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export LaplaceDist#
#' @param N is the required sample size#
#' @param mean is the required mean#
#' @param spread is the spread of the function#
#' @return N values from a Laplace distribution#
#' @examples#
#' set.seed(123)#
#' LaplaceDist(10,0,1)#
#' # [1] -0.55311564  0.85946218 -0.20094937  1.45258293  2.12808209 -2.39565480  0.05785263  1.53636446  0.10855453 -0.09076809#
#
LaplaceDist=function(N,mean, spread, max=0.5,min=-0.5) {#
	y=runif(N,min,max) # Get data from a uniform distribution#
	x=mean-spread*sign(y) *log(1-2*abs(y))#
	return(x)#
}#
# Functions for parametric analysis#
#
#' @title ExtractMAStatistics#
#' @description This function extracts summary statistics from meta-analysis results obtained from the rma function of the metafor R package. If required the function transform back to standardized mean difference (effect size type "d" i.e. Hg) or point biserial correlations (effect size type "r").#
#' Warning: the `ExtractMAStatistics` function works with `metafor` version 2.0-0, but changes to metafor's method of providing access to its individual results may introduce errors into the function.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractMAStatistics#
#' @param maresults is the output from the rma function.#
#' @param Nc is the number of participants in the control condition group.#
#' @param Nt is the number of participants in the treatment condition group.#
#' @param Transform is a boolean value indicating whether the outcome values need to be transformed back to standardized mean difference ("d" i.e. Hg) or point biserial correlations ("r"). It is defaulted to TRUE. If this parameter is set to FALSE, no transformation will be applied.#
#' @param type this indicates the type of transformation required - it defaults to "d" which requests transformation from Zr to Hg, using "r" requests transformation from Zr to r.#
#' @param sig indicates the number of significant digits requested in the output, the default is 4; it rounds the values of mean, pvalue, upper and lower bound to the specified number of significant digits.#
#' @ returnse=FALSE if set to TRUE returns the standard error of the effect size#
#' @return data frame incl. summary statistics from meta-analysis results: overall mean value for the effect sizes, the p-value of the mean, the upper and lower confidence interval bounds (UB and LB), QE which is the heterogeneity test statistic and QEp which the the p-value of the heterogeneity statistic#
#' @examples#
#' ExpData=reproducer::KitchenhamMadeyskiBrereton.ExpData#
#' #Extract the experiment basic statics#
#' S1data=subset(ExpData,ExpData=="S1")#
#' #Use the descriptive data to construct effect size#
#' S1EffectSizes = reproducer::PrepareForMetaAnalysisGtoR(#
#' S1data$Mc,S1data$Mt,S1data$SDc,S1data$SDt,S1data$Nc,S1data$Nt)#
#' # Do a random effect meta-analysis of the transformed r_pbs effect size#
#' S1MA = metafor::rma(S1EffectSizes$zr, S1EffectSizes$vi)#
#' # Extract summary statistics from meta-analysis results and transform back to Hg scale#
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4)#
#' #     mean   pvalue    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 1.122 0.2384  4 0.41 #
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4,TRUE)#
#' #     mean   pvalue     se    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 0.2128 1.122 0.2384  4 0.41#
#
ExtractMAStatistics=function(maresults,Nc, Nt,Transform=TRUE,type="d",sig=4,returnse=FALSE){#
#
pvalue=as.numeric(maresults$pval)  #
#
se=as.numeric(maresults$se)#
#
QE=as.numeric(maresults$QE)#
QEp=as.numeric(maresults$QEp)#
mean=as.numeric(maresults$beta)   #
UB=as.numeric(maresults$ci.ub)  #
LB=as.numeric(maresults$ci.lb) #
#
if(Transform & type=="d"){#
mean=reproducer::transformZrtoHg(mean,Nc,Nt)#
se=reproducer::transformZrtoHg(se,Nc,Nt)#
UB=reproducer::transformZrtoHg(UB,Nc,Nt)#
LB=reproducer::transformZrtoHg(LB,Nc,Nt)#
}#
if(Transform & type=="r"){#
mean=reproducer::transformZrtoR(mean)#
se=reproducer::transformZrtoR(se)#
#
UB=reproducer::transformZrtoR(UB) #
LB=reproducer::transformZrtoR(LB)#
}#
mean=signif(mean,sig)#
pvalue=signif(pvalue,sig)#
se=signif(se,sig)#
#
UB=signif(UB,sig)#
LB=signif(LB,sig)#
QE=signif(QE,2)#
QEp=signif(QEp,2)#
if (returnse)#
metaanalysisresults=data.frame(mean,pvalue,se,UB,LB,QE,QEp)	#
else metaanalysisresults=data.frame(mean,pvalue,UB,LB,QE,QEp)#
return(metaanalysisresults)#
#
}
NPESRelationships(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)
#' @title RandomExperimentSimulations#
#' @description This function performs multiple simulations of two-group balanced experiments for one of four distributions and a specific group size. It identifies the average value of phat, Cliff' d and Kendall's point biserial tau and their variances. It either returns the effect sizes for each non-parametric effect size or it reports the number of times the each non-parametric effect size is assessed to be significantly different from zero. We also present the values for the t-test as a comparison. For log-normal data the results of analysing the transformed data are also reported.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export NPESRelationships#
#' @param mean: The mean used for one of the treatment groups#
#' @param sd: The spread used for both treatment groups. It mus be a real value greater than 0.#
#' @param diff: This is added to the parameter mean, to define the mean of the other treatment group. It can be a real value avd can take the value zero.#
#' @param N: this is the number of observations in each group. It must be an interger greater than 3.#
#' @param reps: this identifies the number of times each experiment simulation is replicated.#
#' @param type: this specifies the underlying distribution used to generate the data. it takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param seed=123 This specifies the initial seed for the set of replications#
#' @param StdAdj: this specifes the extent of variance instability introduced by the treatment.#
#' @param returnData=FALSE: If TRUE, the function returns the individual effect sizes and their varaiances, otherwise it returns summary statistics.#
#' @examples#
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(rbind(DataTable+cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		PercentageZeros=100*Zero/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,PercentageZeros,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
	else#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff,PercentageZeros)#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(rbind(DataTable+cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		PercentageZeros=100*Zero/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,PercentageZeros,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff,PercentageZeros)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(rbind(DataTable+cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		PercentageZeros=100*Zero/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,PercentageZeros,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff,PercentageZeros)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(rbind(DataTable+cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,PercentageZeros,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(rbind(DataTable+cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}#
#
# Example#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)
Data=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0,returnData=TRUE)
head(Data)
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=as.data.frame(cbind(DataTable+rbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}#
#
# Example#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0,returnData=TRUE)
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=data.frame(cbind(DataTable,rbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}#
#
# Example#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0,returnData=TRUE)
RandomExperimentSimulations=function(mean,sd,diff,N, reps,type="n",seed=123,StdAdj=0,returnData=FALSE) {#
#
	phatsum=0 # This is used to sum the value of phat across the replications#
	phatvarsum=0  # This is used to sum the value of the variance of phat across the replications#
	sig.phat=0 # This is used to sum the number of times pat is significant across the replications#
	phatsquare=0 # This  is used to sum phat^2 and construct an empirical variance of phat#
	dsum=0 # This is used to sum the value of Cliff's d across the replications#
	dvarsum=0  # This is used to sum the value of the variance of Cliff's d across the replications#
	sig.d=0 # This is used to sum the number of times d is significant across the replications#
	dsquare=0 # This  is used to sum d^2 and construct an empirical variance of d.#
	ksum=0 # This is used to sum the value of the point biserial tau across the replications#
	kvarsum=0 # This is used to sum the value of the variance of the point biserial tau across the replications#
	ksquare=0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
	ksigCVt=0#
	tsig=0 # This is used to count the number of significant t values across the replications#
	ES=0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
	StdES=0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
	Var=0 # This is used to sum the value of the variance across replications#
	ES.l.trans=0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
	StdES.l.trans=0 # This is used to sum the transformed standardized effect size for lognormal data sets#
	Var.l.trans=0 # This is used to sum the transformed variance for lognormal data sets#
	MedDiff=0 # Used to hold the median difference#
	DataTable=NULL#
	set.seed(seed)#
for (i in 1:reps) {#
	# Call the program that generates the random data sets and calculates the sample statistics.#
		res=NPESRelationships(mean,sd,diff,N,type,StdAdj)#
#
	if 	(returnData==FALSE) {#
		# Aggregate data to provide counts of significance and overall effect size averages#
		# Cliff's d#
			dsum=dsum+res$d#
			if (res$sigd) sig.d=sig.d+1#
			dsquare=dsquare+res$d^2#
		# Probability of superiority#
			phatsum=phatsum+res$phat#
			phatvarsum=phatvarsum+res$varphat#
			if (res$sigphat) sig.phat=sig.phat+1#
			phatsquare=phatsquare+res$phat^2#
#
		# Point biserial Kendall's tau#
			ksum=ksum+res$cor#
			kvarsum=kvarsum+res$varcor#
			ksquare=ksquare+res$cor^2#
			ksigCVt=ksigCVt + if (res$sigCVt) 1 else 0#
		# Parametric statistics#
			ES=ES+res$ES#
			StdES=StdES+res$StdES#
			MedDiff=MedDiff+res$MedDiff#
			tsig=tsig+if (res$ttestp<.05) 1 else 0#
			Var=Var+res$Variance#
			if (type=="l") {#
				ES.l.trans=ES.l.trans+res$EStrans#
				StdES.l.trans=StdES.l.trans+res$StdEStrans#
				Var.l.trans=Var.l.trans+res$VarTrans#
				}#
			}#
		else {#
			# Store the outcome from each replication#
			DataTable=data.frame(rbind(DataTable,cbind(Cliffd=res$d,PHat=res$phat,StdES=res$StdES)))#
			}#
		}#
	if 	(returnData==FALSE) {#
	#Calculate averages of the statistics across the replications#
		d=dsum/reps#
		dvar=dvarsum/(reps)#
		sigd=sig.d/(reps)#
		emp.d.var=(dsquare-reps*d^2)/(reps-1)#
		phat=phatsum/reps#
		phatvar=phatvarsum/(reps)#
		sigphat=sig.phat/(reps)#
		emp.phat.var=(phatsquare-reps*phat^2)/(reps-1)#
		ktau=ksum/reps#
		ktauvar=kvarsum/reps#
		emp.tau.var=(ksquare-reps*ktau^2)/(reps-1)#
		kpowerCVt=ksigCVt/reps#
		ES=ES/reps#
		StdES=StdES/reps	#
		MedDiff=MedDiff/reps#
		Variance=Var/reps#
		tpower=tsig/reps#
		if (type=="l")#
			{#
				# This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
				ESLog=ES.l.trans/reps#
				StdESLog=StdES.l.trans/reps#
				VarLog=Var.l.trans/reps#
			}#
#
	if (type=="l")	{#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance,StdES,MedDiff,ESLog=ESLog,StdESLog=StdESLog,VarLog=VarLog)#
		}#
	else {#
		outcome=data.frame(phat,phatvar,sigphat,emp.phat.var,d,dvar,sigd, emp.d.var,ktau,ktauvar,emp.tau.var,kpowerCVt,tpower,ES,Variance, StdES,MedDiff)#
		}#
	}#
	else {#
		outcome=DataTable#
	}#
	return(outcome)	#
	}
Data=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=5,type="n",seed=123,StdAdj=0,returnData=TRUE)
Data
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=10,type="n",seed=123,StdAdj=0,returnData=TRUE)
Data=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0,returnData=TRUE)
hist(Data$Cliffd)
bocplot(Data$Cliffd)
boxplot(Data$Cliffd)
boxplot(Data)
Data=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0,returnData=TRUE)
boxplot(Data)
NormData=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0,returnData=TRUE)
LogData=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0,returnData=TRUE)
boxplot(NormData$Cliffd,LogData$Cliffd)
boxplot(NormData$StdES,LogData$StdES)
boxplot(NormData$PHat,LogData$SPHat)
boxplot(NormData$PHat,LogData$PHat)
LogData2=RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=200, reps=500,type="l",seed=123,StdAdj=0,returnData=TRUE)
boxplot(LogData$StdES,LogData2$StdES)
set.seed()
set.seed
rbind
boxplot(LogData$Cliffd,LogData2$Cliffd)
as.mtrix
as.matrix
is.data.frame
data.frame
listm
vector
x=c(1,2,3)
is.kist(x)
is.list(x)
base::is.matrix(x)
!base::is.list(x)
length
is.na
var
mean
sum
matrix
list
abs
RandomizedBlocksAnalysis<-function(x,con=c(-0.5,0.5,-0.5,0.5),alpha=.05){#
#
if(base::is.data.frame(x))x=base::as.matrix(x)#
#
flag<-TRUE#
if(alpha!= .05 && alpha!=.01)flag<-FALSE#
if(base::is.matrix(x))x<-listm(x)#
if(!base::is.list(x))stop("Data must be stored in a matrix or in list mode.")#
con<-base::as.matrix(con)#
if (ncol(con)>1) stop("Only one linear contrast permitted for a standard randomized blocks experiment")#
J<-length(x)#
sam=NA#
h<-base::vector("numeric",J)#
w<-base::vector("numeric",J)#
xbar<-base::vector("numeric",J)#
for(j in 1:J){#
xx<-!is.na(x[[j]])#
val<-x[[j]]#
x[[j]]<-val[xx]  # Remove missing values#
sam[j]=length(x[[j]])#
h[j]<-length(x[[j]])#
   # h is the number of observations in the jth group.#
w[j]<-((length(x[[j]])-1)*stats::var(x[[j]]))/(h[j]*(h[j]-1)) # The variance of the jth group#
xbar[j]<-base::mean(x[[j]]) # The mean of the jth group#
}#
#if(sum(con^2)>0){#
if(nrow(con)!=length(x)){#
stop("The number of groups does not match the number of contrast coefficients.")#
}#
psihat<-base::matrix(0,1,4)#
dimnames(psihat)<-list(NULL,c("psihat","ci.lower","ci.upper",#
"p.value"))#
test<-matrix(0,1,4)#
dimnames(test)<-list(NULL,c("test","crit","se","df"))#
df<-0#
psihat[1,1]<-sum(con[,1]*xbar)#
sejk<-sqrt(sum(con[,1]^2*w)) # The pooled standard error of the contrast#
#
test[1,1]<-sum(con[,1]*xbar)/sejk # The value of the t-test#
df<-(sum(con[,1]^2*w))^2/sum(con[,1]^4*w^2/(h-1)) # Degrees of freeedom allowing for heterogeneity#
vv=(1-alpha/2) #
crit<-qt(vv,df) #The critical value of the t-test for the degrees of freedom#
test[1,2]<-crit#
test[1,3]<-sejk#
test[1,4]<-df#
psihat[1,2]<-psihat[1,1]-crit*sejk#
psihat[1,3]<-psihat[1,1]+crit*sejk#
psihat[1,4]<-2*(1-pt(abs(test[1,1]),df))#
list(n=sam,test=test,psihat=psihat)#
}
RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)
x=list()
x[[1]]=rnorm(10,0,1)
x[[2]]=rnorm(10,0.8,1)
x[[3]]=rnorm(10,0.5,1)
x[[4]]=rnorm(10,1.3,1)
vec=c(-1,1,-1,1)/2
RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)
set.seed(123)
x=list()
x[[1]]=rnorm(10,0,1)
x[[2]]=rnorm(10,0.8,1)
x[[3]]=rnorm(10,0.5,1)
x[[4]]=rnorm(10,1.3,1)
RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)
list
outer
sign
apply
qnorm
pnorm
qt
is.na
signif
outer
max
is.na
log
abs
runif
data.frame
as.data.frame
cbind
q()
set/
set.wd("/Users/barbarakitchenham/Dropbox/R")
setwd("/Users/barbarakitchenham/Dropbox/R")
getwd()
#' script to obtain correlation coefficients#
#
#' @title CalculateRLevel1#
#' @description This function calculates the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for each sequence group and each outcome metric. The function returns both the exact r value and the r value based on pooled variances for each sequnce group and outcome metric#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export CalculateRLevel1#
#' @param Dataset This holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data set should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Groups This is a list that defined the sequence group identifiers used in the dataset.#
#' @param Metrics This is a list of metrics, e.g., ("Correctness","Time","Efficiency").#
#' @param ExperimentName This an identifiers used to define the specific experiment in the output from this function.#
#' @param Type this is a character string specifying whether the experiment is a two sequence group of four sequence group experiment.#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return table this is a tibble holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' # importFrom stats#
#' # importFrom var#
#' # importFrom tibble#
#' @examples#
#' ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' Metrics=c("Comprehension","Modification")#
#' Type=c("4G", "4G", "4G", "4G")#
#' Groups=c("A","B","C","D")#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from a file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric for the first experiment.#
#'  CalculateRLevel1(Dataset=ReshapedData[[1]],StudyID,Groups=c("A","B","C","D"),ExperimentName=ShortExperimentNames[1],Metrics,Type=Type[1],Control)#
#' # A tibble: 8 x 15#
#' #  Study Exp   Group Metric        Id        n ControlFirst    var1   var2    varp ControlVarProp VarProp vardiff      r    r.p#
#' #  <chr> <chr> <chr> <chr>         <chr> <int> <lgl>          <dbl>  <dbl>   <dbl>          <dbl>     <dbl>   <dbl>  <dbl>  <dbl>#
#' # 1 S2    E1    A     Comprehension S2E1A     6 FALSE        0.0183  0.0163 0.0173           0.472     0.528  0.0449 -0.301 -0.300#
#' # 2 S2    E1    B     Comprehension S2E1B     6 TRUE         0.0201  0.0326 0.0263           0.381     0.381  0.0180  0.679  0.659#
#' # 3 S2    E1    C     Comprehension S2E1C     6 FALSE        0.00370 0.0155 0.00962          0.808     0.192  0.0214 -0.142 -0.112#
#' # 4 S2    E1    D     Comprehension S2E1D     6 TRUE         0.0173  0.0201 0.0187           0.462     0.462  0.0148  0.606  0.605#
#' # 5 S2    E1    A     Modification  S2E1A     6 FALSE        0.0528  0.0384 0.0455           0.421     0.579  0.0735  0.196  0.193#
#' # 6 S2    E1    B     Modification  S2E1B     6 TRUE         0.0185  0.0482 0.0333           0.277     0.277  0.0508  0.265  0.237#
#' # 7 S2    E1    C     Modification  S2E1C     6 FALSE        0.00655 0.0244 0.0155           0.788     0.212  0.0447 -0.544 -0.444#
#' # 8 S2    E1    D     Modification  S2E1D     6 TRUE         0.0222  0.0266 0.0244           0.455     0.455  0.0695 -0.425 -0.424#
CalculateRLevel1 = function(Dataset,#
                            StudyID,#
                            Groups = c("A", "B", "C", "D"),#
                            ExperimentName,#
                            Metrics,#
                            Type,#
                            Control) {#
  if (Type == "2G")#
    Groups = Groups[1:2]#
  else#
    Groups = Groups[1:4]#
#
  NumReps = length(Groups)#
  table = NULL#
#
  r.table = NULL#
  NumMets = length(Metrics)#
  N = length(Dataset$SequenceGroup.1)#
  rvalidreps = 0#
  for (j in 1:NumMets) {#
    for (i in 1:NumReps) {#
      # Analyse the wide data for each Sequence Group in a specific experiment for a specific metric#
      ss = base::subset(Dataset, SequenceGroup.1 == Groups[i])#
      # Select the data for the specific metric corresponding to each time period#
      name1 = base::paste(Metrics[j], "1", sep = ".")#
      name2 = base::paste(Metrics[j], "2", sep = ".")#
      res1 = base::subset(ss, select = name1)#
      n1 = length(ss$SequenceGroup.1)#
      res2 = base::subset(ss, select = name2)#
      n2 = length(ss$SequenceGroup.2) # n1 should equal n2#
      if (n1 != n2)#
        return("Incorrect subset")#
      # Analyse the difference data for each Metric#
      diff = res2 - res1#
#
      if (n1 > 1) {#
        vardiff = as.numeric(stats::var(diff))#
        var1 = as.numeric(stats::var(res1))#
        var2 = as.numeric(stats::var(res2))#
      }#
      else {#
        # Cannot construct a sensible variance#
        vardiff = 0#
        var1 = 0#
        var2 = 0#
      }#
#
	 if (vardiff > 0 & var1 > 0 & var2 > 0) {#
#
       # Ensure all variances are non-zero otherwise do not add to the row#
#
      # Identify which variance corresponds to the control condition#
      ControlFirst = FALSE#
      if (ss$Treatment.1[1] == Control)#
        ControlFirst = TRUE#
      if (ControlFirst)#
        controlvar = var1#
      else#
        controlvar = var2#
#
     	ControlVarProp = controlvar / (var1 + var2)#
      	TimePeriodVarProp = var1 / (var1 + var2)#
#
        # Calculate r without assuming variance homogeneity#
        r = as.numeric((var1 + var2 - vardiff) / (2 * sqrt(var1 * var2)))#
        r = signif(r, 4)#
        #  Calculate r assuming variance homogeneity#
        pooled.var = (var1 + var2) / 2#
        r.p = as.numeric((2 * pooled.var - vardiff) / (2 * pooled.var))#
        r.p = signif(r.p, 4)#
        var1 = signif(var1, 4)#
        var2 = signif(var2, 4)#
        vardiff = signif(vardiff, 4)#
        ControlVarProp = signif(ControlVarProp, 3)#
        TimePeriodVarProp = signif(TimePeriodVarProp, 3)#
    # Construct an Id that is unique for a specific study, experiment and sequence group but treats r-values for different metrics as repeated measures.#
        Id = base::paste(StudyID, ExperimentName, sep = "")#
        Id = base::paste(Id, Groups[i], sep = "")#
        row = base::cbind(#
          Study = StudyID,#
          Exp = ExperimentName,#
          Group = Groups[i],#
          Metric = Metrics[j],#
          Id = Id,#
          n = n1,#
          ControlFirst = ControlFirst,#
          var1 = var1,#
          var2 = var2,#
          varp = pooled.var,#
          ControlVarProp = ControlVarProp,#
          VarProp = TimePeriodVarProp,#
          vardiff = vardiff,#
          r = r,#
          r.p = r.p#
        )#
        table = tibble::as_tibble(rbind(table, row))#
      }#
    }#
#
  }#
#
  	#Coerce table columns to correct format#
	table$n=as.integer(table$n)#
	table$ControlFirst=as.logical(table$ControlFirst)#
	table$var1=as.numeric(table$var1)#
	table$var2=as.numeric(table$var2)#
	table$varp=as.numeric(table$varp)#
	table$ControlVarProp=as.numeric(table$ControlVarProp)#
	table$VarProp=as.numeric(table$VarProp)#
	table$vardiff=as.numeric(table$vardiff)#
	table$r=as.numeric(table$r)#
	table$r.p=as.numeric(table$r.p)#
  return(table)#
#
}#
############################################################################################################
#' @title ExtractGroupSizeData#
#' @description This function constructs a table identifying the number of participants in each sequence group for a set of experiments each of which used a crossover design.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @param Table this contains information about each experiment in a set of related experiments in the format of data returned by ConstructLevel1ExperimentRData#
#' @param StudyID an identifer for the group of related experiment.#
#' @param ExperimentNames a list of character strings identifying each experiment.#
#' @param Groups a list of the terms used to specify sequence groups in the experiments.#
#' @param Metrics a list of the terms used to identify the output metrics used in the experiment.#
#' @return A table containing the number of participants in each sequence group in rach experiment.#
#' # @example GroupSize = ExtractGroupSizeData(Table=NewTable, StudyID, ExperimentNames, Groups=Groups, Metrics=Metrics)#
####################### Needs correct example #######################################
#
ExtractGroupSizeData = function(Table,#
                                StudyID,#
                                ExperimentNames,#
                                Groups,#
                                Metrics)#
{#
#
################# CDoesn't cope with zero variances for some metrics ###########################################
  NumExp = length(ExperimentNames)#
  FirstMetric = Metrics[1]#
  GroupDataTable = NULL#
  for (i in 1:NumExp) {#
    ss = base::subset(#
      Table,#
      (Exp == ExperimentNames[i] &#
         Metric == FirstMetric),#
      select = c("Study", "Exp", "Group", "n")#
    )#
    GroupDataTable = as.data.frame(rbind(GroupDataTable, ss))#
  }#
#
 ################# Change to tibble? ###########################################
  return(GroupDataTable)#
}#
#
############################################################################################
#
#' @title ConstructLevel1ExperimentRData#
#' @description This function returns the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for a group of 1 or more experiments for each sequence group and each outcome metric. For sets of 2 or more experiments, the experiments are assumed to be replicates and to report the same sets of Metrics and have the same Control treatment and use the same squence Group identifiers, but are not necessarily the same Type. We return both the exact r value and the r value based on pooled variances for each sequence group and outcome metric.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ConstructLevel1ExperimentRData#
#' @param Data This is a list parameter each entry in the list holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Group This is a list that defined the sequence group identifiers used in the dataset.#
#' @param ExperimentNames This a list of identifiers used to define each experiment in the output from this function.#
#' @param Metrics This is a list of of character strings identifying each outcome metric reported in each of the experiments in the set of replicated experiments.#
#' @param Type this is a list of character strings specifying for each experiment whether the experiment is a 2-group or 4-group experiment#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return R.Data.Table this is a tibble holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' @examples#
#' ##
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from each file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric in each experiment#
#' ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#' # A tibble: 32 x 15#
#' #   Study Exp   Group Metric        Id        n ControlFirst    var1   var2    varp ControlVarProp TPVarProp vardiff      r    r.p#
#' #   <chr> <chr> <chr> <chr>         <chr> <int> <lgl>          <dbl>  <dbl>   <dbl>          <dbl>     <dbl>   <dbl>  <dbl>  <dbl>#
#' #  1 S2    E1    A     Comprehension S2E1A     6 FALSE        0.0183  0.0163 0.0173           0.472     0.528  0.0449 -0.301 -0.300#
#' #  2 S2    E1    B     Comprehension S2E1B     6 TRUE         0.0201  0.0326 0.0263           0.381     0.381  0.0180  0.679  0.659#
#' #  3 S2    E1    C     Comprehension S2E1C     6 FALSE        0.00370 0.0155 0.00962          0.808     0.192  0.0214 -0.142 -0.112#
#' #  4 S2    E1    D     Comprehension S2E1D     6 TRUE         0.0173  0.0201 0.0187           0.462     0.462  0.0148  0.606  0.605#
#' #  5 S2    E1    A     Modification  S2E1A     6 FALSE        0.0528  0.0384 0.0455           0.421     0.579  0.0735  0.196  0.193#
#' #  6 S2    E1    B     Modification  S2E1B     6 TRUE         0.0185  0.0482 0.0333           0.277     0.277  0.0508  0.265  0.237#
#' #  7 S2    E1    C     Modification  S2E1C     6 FALSE        0.00655 0.0244 0.0155           0.788     0.212  0.0447 -0.544 -0.444#
#' #  8 S2    E1    D     Modification  S2E1D     6 TRUE         0.0222  0.0266 0.0244           0.455     0.455  0.0695 -0.425 -0.424#
#' #  9 S2    E2    A     Comprehension S2E2A     6 FALSE        0.0194  0.0425 0.0309           0.687     0.313  0.0489  0.227  0.211#
#' # 10 S2    E2    B     Comprehension S2E2B     6 TRUE         0.0198  0.0192 0.0195           0.508     0.508  0.0670 -0.717 -0.717#
#' # #  … with 22 more rows#
#
ConstructLevel1ExperimentRData = function(Data,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Groups,#
                                          Metrics,#
                                          Type,#
                                          Control) {#
  # This calls the algorithm that constructs the correlation values for each metric and each independent sequence group in an experiment. It identifies the data for each sequnce group in each experiment for each experiment and collates the reurned r-values for each sequence group#
  NumExp = length(ExperimentNames)#
#
  R.Data.Table = NULL#
#
  for (i in 1:NumExp) {#
    r.data = CalculateRLevel1(#
      Data[[i]],#
      Groups = Groups,#
      StudyID = StudyID,#
      ExperimentName = ExperimentNames[i],#
      Metrics,#
      Type = Type[i],#
      Control = Control#
    )#
#
    R.Data.Table = tibble::as_tibble(rbind(R.Data.Table, r.data))#
  }#
#
  return(R.Data.Table)#
}#
#
##############################################################################################################
#
#' @title ExtractExperimentData#
#' @description This function reads datasets from a defined directory in the reproducer package that hold the results of a fmaily crossover experiments in the long format. It converts the data to the wide format if required.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractExperimentData#
#' @param DataSet This is a tibble holding the data for each crossover experiment in a family (a family can include only one experiment).#
#' @param ExperimentNames This is a list with the full names of each experiment.#
#' @param idvar="ParticipantID" This is the name of the column that contains the data for specific participants. It is only assumed to be unique within an experiment.#
#' @param timevar="Period" This is the name of the table column that defines which data was collected in a specific time period. This function assumes that there are only two time periods.#
#' @param ConvertToWide=TRUE This determine whether the fucntion converts the data to the wide format.#
#' @return A list with an entry for the data for each experiment. If ConvertToWide is TRUE, it returns the data in the wide format otherwise it returns the data as it was read. Within each list item the data is returned as a tibble#
#' #importFrom stats#
#' # importFrom tibble#
#' # importFrom base#
#' @examples#
#' ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' # Obtain experimental data from each file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData = ExtractExperimentData(dataset2,ExperimentNames=ExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' ReshapedData[[1]]#
#' # A tibble: 24 x 15#
#' #    ParticipantID ExperimentID.1 SequenceGroup.1 System.1 Treatment.1 Comprehension.1 Modification.1 CrossOverID.1 ExperimentID.2 SequenceGroup.2#
#' #    <fct>         <fct>          <fct>           <fct>    <fct>                 <dbl>          <dbl> <fct>         <fct>          <fct>#
#' #  1 1             EUBAS          A               S1       AM                     0.77          0.88  CO1           EUBAS          A#
#' #  2 5             EUBAS          A               S1       AM                     0.61          0.75  CO1           EUBAS          A#
#' #  3 9             EUBAS          A               S1       AM                     0.61          0.79  CO1           EUBAS          A#
#' #  4 13            EUBAS          A               S1       AM                     0.52          0.5   CO1           EUBAS          A#
#' #  5 17            EUBAS          A               S1       AM                     0.43          0.290 CO1           EUBAS          A#
#' #  6 21            EUBAS          A               S1       AM                     0.77          0.83  CO1           EUBAS          A#
#' #  7 2             EUBAS          B               S1       SC                     0.92          0.38  CO1           EUBAS          B#
#' #  8 6             EUBAS          B               S1       SC                     0.63          0.5   CO1           EUBAS          B#
#' #  9 10            EUBAS          B               S1       SC                     0.51          0.580 CO1           EUBAS          B#
#' # 10 14            EUBAS          B               S1       SC                     0.64          0.75  CO1           EUBAS          B#
#' # # … with 14 more rows, and 5 more variables: System.2 <fct>, Treatment.2 <fct>, Comprehension.2 <dbl>, Modification.2 <dbl>, CrossOverID.2 <fct>#
#
ExtractExperimentData = function(DataSet,#
                                 ExperimentNames,#
                                 idvar = "ParticipantID",#
                                 timevar = "Period",#
                                 ConvertToWide = TRUE) {#
  # This algorithm reads the data for each experiment in a family (or a study)  and converts to the "wide" data format#
#
  NumExp = length(ExperimentNames)#
  ExpData = list()#
  for (i in 1:NumExp) {#
    # Read each of the data sets into a separate list entry#
#
		tempdata =as.data.frame(base::subset(DataSet,ExperimentID==ExperimentNames[i]))#
#
# Find the wide version of the dataset based on the Period variable if required#
		if (ConvertToWide) {#
			tempdata=stats::reshape(tempdata,idvar=idvar,timevar=timevar,direction="wide")}#
# Return the data in the tibble format#
		ExpData[[i]] =tibble::as_tibble(tempdata)#
#
	}#
#
	return(ExpData)#
}#
#
########################################################################################################
#' @title CalculateLevel2ExperimentRData#
#' @description This function analyses data on r values obtained in the format obtained from  the ConstructLevel1ExperimentRData function and finds the r-value for each metric for each experiment.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export CalculateLevel2ExperimentRData#
#' @param Level1Data a tibble in the format produced by the ConstructLevel1ExperimentRData function which has r-values for each sequence group in a crossover experiment#
#' @param Group This is a list that defines the sequence group labels used in the dataset.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param ExperimentNames This a list of identifiers used to define each experiment in the output from this function.#
#' @param Metrics This is a list of of character strings identifying each outcome metric reported in each of the experiments in the set of replicated experiments.#
#' @param Type this is a list of character strings specifying for each experiment whether the experiment is a two sequence group "2G" or four sequence group "4G" experiment.#
#' return RExp.Table This is a table containing the pooled data variance and the pooled difference variance for the experiment and the value r for the eperiment for eachm metric#
#' @examples#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' # Obtain experimental data from each file and put in wide format#
#' ReshapedData = ExtractExperimentData(KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM,ExperimentNames=FullExperimentNames, idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' Lev1Data= ConstructLevel1ExperimentRData(ReshapedData,StudyID,ShortExperimentNames,Groups,Metrics,Type,Control)#
#' #Lev1Data= ConstructLevel1ExperimentRData(Data=ReshapedData,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#' CalculateLevel2ExperimentRData(Lev1Data,Groups=Groups,StudyID=StudyID,ExperimentNames=ShortExperimentNames,Metrics=Metrics,Type=Type)#
CalculateLevel2ExperimentRData = function(Level1Data,#
                                          Groups,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Metrics,#
                                          Type) {#
  NumExp = length(ExperimentNames)#
#
  RExp.Table = NULL#
  NumMets = length(Metrics)#
  for (i in 1:NumExp) {#
    # Extract the variance data for a specific experiment#
    ExpData = base::subset(Level1Data, Exp == ExperimentNames[i])#
    if (Type[i] == "2G")#
      SequenceGroups = Groups[1:2]#
    else#
      SequenceGroups = Groups[1:4]#
#
    NumGroups = length(SequenceGroups)#
    ExpID = ExpData$Exp[1]#
    ID = base::paste(StudyID, ExpID, sep = "")#
    for (j in 1:NumMets) {#
      # Find the sequence group statistics for a specific Metric#
      MetData = base::subset(ExpData, Metric == Metrics[j])#
      # Find the number of participants in the experiment#
      NumParticipants = sum(MetData$n)#
#
      # Calculate the pooled sequence group variance and difference variance#
      PooledVar1 = sum(MetData$var1 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      PooledVar2 = sum(MetData$var2 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      VarProp = PooledVar1 / (PooledVar1 + PooledVar2)#
      PooledVar = sum(MetData$varp * (MetData$n - 1)) / (NumParticipants -#
                                                           NumGroups)#
      PooledDiffVar = sum(MetData$vardiff * (MetData$n - 1)) / (NumParticipants -#
                                                                  NumGroups)#
      # Calculate r#
      r = (2 * PooledVar - PooledDiffVar) / (2 * PooledVar)#
      # Tidy up calculated values#
      r = signif(r, 4)#
      PooledVar1 = signif(PooledVar1, 4)#
      PooledVar2 = signif(PooledVar2, 4)#
      PooledVar = signif(PooledVar, 4)#
      VarProp = signif(VarProp, 4)#
      PooledDiffVar = signif(PooledDiffVar, 4)#
      # Put the output into a format that identifes the study, experiment, experiment size and metric and the average r for the experiment#
      row =#
        cbind(#
          PooledVar1 = PooledVar1,#
          PooledVar2 = PooledVar2,#
          VarProp = VarProp,#
          PooledVar = PooledVar,#
          PooledDiffVar = PooledDiffVar,#
          r.Exp = r#
        )#
      RExp.Table = rbind(#
        RExp.Table,#
        cbind(#
          StudyID = StudyID,#
          ExpID = ID,#
          N = NumParticipants,#
          Metric = Metrics[j],#
          row#
        )#
      )#
#
    }#
#
  }#
#
	RExp.Table=tibble::as_tibble(RExp.Table)#
#
	# Coerce the data items to the correct formats#
	RExp.Table$N=as.integer(RExp.Table$N)#
	RExp.Table$PooledVar1=as.numeric(RExp.Table$PooledVar1)#
	RExp.Table$PooledVar2=as.numeric(RExp.Table$PooledVar2)#
	RExp.Table$PooledVar=as.numeric(RExp.Table$PooledVar)#
	RExp.Table$VarProp=as.numeric(RExp.Table$VarProp)#
	RExp.Table$PooledDiffVar=as.numeric(RExp.Table$PooledDiffVar)#
	RExp.Table$r.Exp=as.numeric(RExp.Table$r.Exp)#
	return(RExp.Table)#
#
}#
#
#############################################################################################################
#
# Functions to help produce tables for the paper#
#
#' @title ExtractSummaryStatisticsRandomizedExp#
#' @description This function extracts data obtained from the lme4 package lmer function. It assumes a simple randomized experiment with each element having one or more repeated measures. It outputs the mean together with its standard error and confidence interval bounds.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @param lmeRA The output from the lmer function#
#' @param N The total number of observations#
#' @param alpha the probability level to be used when constructing the confidence interval bounds.#
#' @return REA.Summary A dataframe holding the number of observations N, the overall mean value as its standard errorreported as by the lmer function, and its confidence interval bounds.#
#' @examples#
#' # Directory="/Users/barbarakitchenham/Dropbox/Scanniello2014TOSEM"#
#' # Filenames=c("E-UBAS.csv","R1-UCLM.csv","R2-UCLM.csv","R3-UCLM.csv")#
#' # Metrics=c("Comprehension","Modification")#
#' # Groups=c("1","2","3","4")#
#' # Type=c(rep("4G",4))#
#' # StudyID="S2"#
#' # Control="SC"#
#' # ExpData= ExtractExperimentData(Directory,FileNames=Filenames,idvar="ID",timevar="Period",seperator=",",ConvertToWide=TRUE)#
#' # NewTable= ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
#' # resRe=lme4::lmer(r~(1|Id),data=NewTable)#
#' # summary(resRe)#
#' # Linear mixed model fit by REML ['lmerMod']#
#' # Formula: r ~ (1 | Id)#
#' # REML criterion at convergence: 47.8#
#' # Scaled residuals:#
#' #     Min      1Q  Median      3Q     Max#
#' # -1.4382 -0.9691  0.2190  0.8649  1.4761#
#' ##
#' # Random effects:#
#' #  Groups   Name        Variance Std.Dev.#
#' #  Id       (Intercept) 0.03978  0.1994#
#' #  Residual             0.20974  0.4580#
#' # Number of obs: 32, groups:  Id, 16#
#' ##
#' # Fixed effects:#
#' #            Estimate Std. Error t value#
#' # (Intercept)  0.06175    0.09508   0.649#
#' # N=length(NewTable$r)#
#' # ExtractSummaryStatisticsRandomizedExp(lmeRA=resRe,N=32,alpha=0.05)#
#' #     N    Mean      SE LowerBound UpperBound#
#' #  1 32 0.06175 0.09508    -0.1319     0.2554#
ExtractSummaryStatisticsRandomizedExp = function(lmeRA, N, alpha = 0.05) {#
  REA.fe.t = stats::coef(summary(lmeRA))#
  REA.Mean = REA.fe.t[1, "Estimate"]#
  REA.SE = REA.fe.t[1, "Std. Error"]#
  # Calculate confidence interval for mean r#
  vv = stats::qt(alpha / 2, N)#
  REA.LowerBound = REA.Mean + vv * REA.SE#
  REA.UpperBound = REA.Mean - vv * REA.SE#
#
  # Truncate the values for readability#
  REA.Mean = signif(REA.Mean, 4)#
  REA.SE = signif(REA.SE, 4)#
  REA.LowerBound = signif(REA.LowerBound, 4)#
  REA.UpperBound = signif(REA.UpperBound, 4)#
  REA.Summary = data.frame(#
    cbind(#
      N = N,#
      Mean = REA.Mean,#
      SE = REA.SE,#
      LowerBound = REA.LowerBound,#
      UpperBound = REA.UpperBound#
    )#
  )#
#
  return(REA.Summary)#
}#
#########################################################################################################################
#' @title calculateBasicStatistics#
#' @description This function calculates the following statistcs for a set of data: length, mean, median, variance, standard error of the mean, and confidence interval bounds. The input data imust be a vector of 2 or more numerical values.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @param x The data to be summarized#
#' @param alpha The probability level to be used when constructing the confidence interval bounds.#
#' @return A dataframe comprising the length, mean, variance, standard error and confidence limit bounds of the inout data x.#
#' @examples#
#' # Directory="/Users/barbarakitchenham/Dropbox/Scanniello2014TOSEM"#
#' # Filenames=c("E-UBAS.csv","R1-UCLM.csv","R2-UCLM.csv","R3-UCLM.csv")#
#' # Metrics=c("Comprehension","Modification")#
#' # Groups=c("1","2","3","4")#
#' # Type=c(rep("4G",4))#
#' # StudyID="S2"#
#' # Control="SC"#
#' # ExpData= ExtractExperimentData(Directory,FileNames=Filenames,idvar="ID",timevar="Period",seperator=",",ConvertToWide=TRUE)#
#' # NewTable= ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#' # calculateBasicStatistics(NewTable$r)#
#' # #  N    Mean Median Variance      SE LowerBound UpperBound#
#' # #  1 32 0.06175 0.1688   0.2482 0.08808    -0.1109     0.2344#
#
calculateBasicStatistics = function(x, alpha = 0.05) {#
  # Finds the basic statistics for a set of data#
  N = length(x)#
  mean_x = mean(x)#
  var_x = stats::var(x)#
  median_x = stats::median(x)#
  se_x = sqrt(var_x / N)#
  crit <- stats::qnorm(alpha / 2)#
  lowerbound_x = mean_x + crit * se_x#
  upperbound_x = mean_x - crit * se_x#
  summary_x = data.frame(#
    cbind(#
      N = N,#
      Mean = mean_x,#
      Median = median_x,#
      Variance = var_x,#
      SE = se_x,#
      LowerBound = lowerbound_x,#
      UpperBound = upperbound_x#
    )#
  )#
  summary_x = signif(summary_x, 4)#
  return(summary_x)#
}#
#############################################################################################################
#' @title calculateGroupSummaryStatistics#
#' @description This function calculates the following statistcs data within groups: length, mean, median, variance, standard error of the mean, and confidence interval bounds.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @param x The data to be summarized. This must be a vector of 2 or more numerical values#
#' @param Group The catagorical data data defining the groups. This must vector of the same lenth as x containing factors specifying the data groups#
#' @param alpha The probability level to be used when constructing the confidence interval bounds.#
#' @return A dataframe comprising the number, mean, variance, standard error and confidence limit bounds of the data in each category#
#' @examples#
#' # Directory="/Users/barbarakitchenham/Dropbox/Scanniello2014TOSEM"#
#' # Filenames=c("E-UBAS.csv","R1-UCLM.csv","R2-UCLM.csv","R3-UCLM.csv")#
#' # Metrics=c("Comprehension","Modification")#
#' # Groups=c("1","2","3","4")#
#' # Type=c(rep("4G",4))#
#' # StudyID="S2"#
#' # Control="SC"#
#' # ExpData= ExtractExperimentData(Directory,FileNames=Filenames,idvar="ID",timevar="Period",seperator=",",ConvertToWide=TRUE)#
#' # NewTable= ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#' # SeqGroupLev=NULL#
#' # N.NT=length(NewTable$r)#
#' # for (i in 1:N.NT) {#
#' # 	if (NewTable$n[i]<=8) SeqGroupLev[i]=as.character(NewTable$n[i])#
#' # 	if (NewTable$n[i]>8) SeqGroupLev[i]=as.character(9)#
#' # }#
#' # calculateGroupSummaryStatistics(NewTable$r,Group=SeqGroupLev)#
#' # #    N    Mean  Median Variance  StDev     SE#
#' # # 1  4 -0.0833 -0.1699   0.2314 0.4810 0.2405#
#' # # 2 12  0.3658  0.4477   0.2109 0.4592 0.1326#
#' # # 3 16 -0.1300 -0.2214   0.1933 0.4397 0.1099#
calculateGroupSummaryStatistics = function(x, Group) {#
  # Calculates summary statistics for grouped data#
  agg.mean = stats::aggregate(x, by = list(Group), FUN = mean)#
  agg.mean = reshape::rename(agg.mean, c(x = "Mean"))#
  agg.median = stats::aggregate(x, by = list(Group), FUN = median)#
  agg.median = reshape::rename(agg.median, c(x = "Median"))#
  agg.length = stats::aggregate(x, by = list(Group), FUN = length)#
  agg.length = reshape::rename(agg.length, c(x = "N"))#
  agg.var = stats::aggregate(x, by = list(Group), FUN = var)#
  agg.var = reshape::rename(agg.var, c(x = "Variance"))#
  agg.StDev = sqrt(agg.var$Variance)#
  agg.SE = sqrt(agg.var$Variance / agg.length$N)#
#
  summaryTable = NULL#
  summaryTable = data.frame(#
    cbind(#
      N = agg.length$N,#
      Mean = agg.mean$Mean,#
      Median = agg.median$Median,#
      Variance = agg.var$Variance,#
      StDev = agg.StDev,#
      SE = agg.SE#
    )#
  )#
  summaryTable = signif(summaryTable, 4)#
#
  return(summaryTable)#
}#
#
#############################################################################################################################
#
#' @title rSimulations#
#' @description This function simulates many datasest from the same bivariate distibution to investigate the distribution of corrleations for specific sample sizes.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export rSimulations#
#' @param mean The mean used for one of bivariate distruibutions - assumed to be the control condition in an experiment.#
#' @param var The variance used for both treatment groups. It must be a real value greater than 0.#
#' @param diff This value is added to the parameter mean to specify the mean for the other bivariate distribution - assumed to be the treatment condition in an experiment.#
#' @param r This specifies the correlation coefficient to be used for the bivariate normal distribution it must be a value in the range [-1,1].#
#' @param N The number of observations in each simulated bivariate normal data set.#
#' @param VarAdj This value will be added to the variance of the treatment condition.#
#' @param reps The number of bivariate data sets that will be simulated.#
#' @param seed This specifies the seed value for the simulations and allows the experiment to be repeated.#
#' @param returntSignificant=F. If set to true the percentage of times the t-test delivered a value signiicant at the 0.05 level is reported.#
#' @param returndata=F If set to FALSE, the function returns the summary information across all the replications. If set to TRUE the function outputs the r and variance ratio, and variance accuracy values generated in each replication.#
#' @param plothist=F. If set to T, the function outputs a histogram of the r-values, the varprop values and the accuracy values.#
#' @return output If returndata=F, the output returns summary information about the average of r and the variance properties across the replicated data sets.If returndata=T, the function returns the r-values obtained for each of the simulated data sets to gether with the varaiance ratio, the variance accuracy measure and a dummy variable indicating whether a test of significance between the mean values was significant (which is indicated by the dummy variable being set to 1) or not (which is indicated by the dummy variable being set to 0)#
#' @examples#
#' output=rSimulations(mean=0,var=1,diff=0,r=0.25,N=4,reps=10000)#
#' output=signif(output,4)#
#' output#
#' #  r.Mean r.Median  Var.r PercentNegative Mean.VarProp Variance.VarProp Percent.VarProp.Anomalies Mean.Varaccuracy Var.varaccuracy Percent.VarAccuracy.Anomalies#
#' # 1 0.2132   0.3128 0.3126           34.21       0.5036          0.06046                     18.42            1.004          0.3554                         37.09#
#' #   Mean.VarDiffAccuracy Var.VarDiffAccuracy Percent.DiffVarAccuracy.Anomalies#
#' # 1                1.003              0.6618                             52.52#
#' output=rSimulations(mean=0,var=1,diff=0.8,r=0.25,N=60,reps=10000,returntSignificant=TRUE)#
#' output=signif(output,4)#
#' output#
#' #   r.Mean r.Median   Var.r PercentNegative Mean.VarProp Variance.VarProp Percent.VarProp.Anomalies Mean.Varaccuracy Var.varaccuracy Percent.VarAccuracy.Anomalies#
#' # 1 0.2492   0.2534 0.01529            2.62       0.5009         0.003897                         0                1         0.01786                          0.11#
#' #   Mean.VarDiffAccuracy Var.VarDiffAccuracy Percent.DiffVarAccuracy.Anomalies PercentSig#
#' # 1               0.9989             0.03469                              0.92      99.67#
#' output=rSimulations(mean=0,var=1,diff=0,r=0.25,N=30,reps=10,returndata=TRUE)#
#' output#
#' #     rvalues   VarProp VarAccuracy VarDiffAccuracy tSig#
#' #1  0.3981111 0.4276398   0.8630528       0.6974386    0#
#' #2  0.2104742 0.4994285   0.7812448       0.8224174    0#
#' #3  0.4252424 0.4933579   1.1568545       0.8866058    0#
#' #4  0.3502651 0.6004373   0.8710482       0.7628923    0#
#' #5  0.3845145 0.6029086   0.9618363       0.7998859    0#
#' #6  0.1397217 0.4201069   1.1817022       1.3582855    0#
#' #7  0.2311455 0.3894894   0.8322239       0.8594886    0#
#' #8  0.3725047 0.5985897   1.1742117       0.9938662    0#
#' #9  0.4881618 0.2712268   0.7585261       0.5723671    0#
#' #10 0.1568071 0.3936400   0.9869924       1.1143561    0#
rSimulations = function(mean,#
                        var,#
                        diff,#
                        r,#
                        N,#
                        reps,#
                        VarAdj = 0,#
                        seed = 123,#
                        returntSignificant = F,#
                        returndata = F,#
                        plothist = F) {#
  # This function simulates bivariate normal distributions in order to investigate the distribution of given r-values for different sample sizes.#
  set.seed(seed)#
#
  # Set up variables to hold the data from each replication#
#
  rvalues = c(rep(NA, reps))#
  varratio = c(rep(NA, reps))#
  varaccuracy = c(rep(NA, reps))#
  vardiffaccuracy=c(rep(NA,reps))#
  tSig = c(rep(NA, reps))#
#
  rnegative = 0#
#
  # Set up the parameters for the bivariate normal distribution#
  meanvec = c(mean + diff, mean)#
  covar = r * sqrt(var) * sqrt(var + VarAdj)#
  sigma = matrix(c(var + VarAdj, covar, covar, var),#
                 nrow = 2,#
                 ncol = 2)#
  for (i in 1:reps) {#
    # Generate and analyse each simulation#
    Mydataraw = MASS::mvrnorm(N, meanvec, sigma)#
    Mydataraw = as.data.frame(Mydataraw)#
    names(Mydataraw) = c("y", "x")#
#
    # Find the variance and r values#
    varx = stats::var(Mydataraw$x)#
    vary = stats::var(Mydataraw$y)#
    Mydatadiff = Mydataraw$y - Mydataraw$x#
#
    vardiff=stats::var(Mydatadiff)#
#
# Measure the heterogeneity of the between participants variance#
    varratio[i] = varx / (varx + vary)#
#
 # Measure the accuracy of the between participants variance#
    varaccuracy[i] = (varx + vary) / (2 * var + VarAdj)#
#
 # Measure the accuracy of the difference data variance#
	 vardiffaccuracy[i]=vardiff/((2*var+VarAdj)*(1-r))#
#
    rvalues[i] = (varx + vary - vardiff) / (2 * sqrt(varx * vary))#
#
    if (rvalues[i] <= 0)#
      rnegative = rnegative + 1#
#
    # Do a t-test of the difference between the means#
    ttest.results = stats::t.test(Mydataraw$x, Mydataraw$y)#
    tSig[i] = if (ttest.results$p.value < 0.05)#
      1#
    else#
      0#
#
  }#
#
  rnegative = 100 * rnegative / reps#
  raverage = mean(rvalues)#
  rmedian = stats::median(rvalues)#
  Varr = stats::var(rvalues)#
  meanvarrat = mean(varratio)#
  varvarrat = stats::var(varratio)#
#
  meanvaracc = mean(varaccuracy)#
  varvaracc = stats::var(varaccuracy)#
#
  meanvardiffacc=mean(vardiffaccuracy)#
  varvardiffacc=stats::var(vardiffaccuracy)#
  PercentSig = 100 * mean(tSig)#
#
  if (plothist)#
  {#
    graphics::par(mfrow = c(4, 2))#
    graphics::hist(rvalues, main = "Correlation Histogram", xlab = "Correlations")#
    graphics::boxplot(rvalues, main = "Correlation Boxplot")#
#
    graphics::hist(varratio, main = "Variance Ratio Histogram", xlab = "Variance Ratio Values")#
    graphics::boxplot(varratio, main = "Variance Ratio Boxplot")#
    graphics::hist(varaccuracy, main = "Variance Accuracy Histogram", xlab = "Variance Accuracy Values")#
    graphics::boxplot(varaccuracy, main = "Variance Accuracy Boxplot")#
#
    	graphics::hist(vardiffaccuracy,main="Diff Var Accuracy Histogram",xlab="Diff Var Accuracy Values")#
	graphics::boxplot(vardiffaccuracy, main="Diff Var Accuracy Boxplot")#
#
  }#
#
  count = 0#
  TheoreticalVarRatio = var / (2*var + VarAdj)#
  LowerVarRatio = TheoreticalVarRatio / 2#
  UpperVarRatio = TheoreticalVarRatio + TheoreticalVarRatio / 2#
  for (i in 1:reps)#
  {#
    if (varratio[i] < LowerVarRatio |#
        varratio[i] > UpperVarRatio)#
      count = count + 1#
  }#
  acccount = 0#
  for (i in 1:reps)#
  {#
    if (varaccuracy[i] < 0.5 | varaccuracy[i] > 1.5)#
      acccount = acccount + 1#
  }#
#
  diffacccount = 0#
  for (i in 1:reps)#
  {#
    if (vardiffaccuracy[i] < 0.5 | vardiffaccuracy[i] > 1.5)#
      diffacccount = diffacccount + 1#
  }#
#
  if (!returndata)#
  {#
    output = data.frame(#
      r.Mean = raverage,#
      r.Median = rmedian,#
      Var.r = Varr,#
      PercentNegative = rnegative,#
      Mean.VarProp = meanvarrat,#
      Variance.VarProp = varvarrat,#
      Percent.VarProp.Anomalies = 100 * count / reps,#
      Mean.Varaccuracy = meanvaracc,#
      Var.varaccuracy = varvaracc,#
      Percent.VarAccuracy.Anomalies = 100 * acccount / reps,#
      Mean.VarDiffAccuracy=meanvardiffacc,#
      Var.VarDiffAccuracy= varvardiffacc,#
      Percent.DiffVarAccuracy.Anomalies=100*diffacccount / reps#
    )#
    if (returntSignificant)#
      output = data.frame(cbind(output, PercentSig = PercentSig))#
#
  }#
  else {#
    output = data.frame(cbind(#
      rvalues = rvalues,#
      VarProp = varratio,#
      VarAccuracy = varaccuracy,#
      VarDiffAccuracy=vardiffaccuracy,#
      tSig = tSig#
    ))#
  }#
  return(output)#
#
}
ataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM
q()
#' script to obtain correlation coefficients#
#
#' @title CalculateRLevel1#
#' @description This function calculates the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for each sequence group and each outcome metric. The function returns both the exact r value and the r value based on pooled variances for each sequnce group and outcome metric#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export CalculateRLevel1#
#' @param Dataset This holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data set should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Groups This is a list that defined the sequence group identifiers used in the dataset.#
#' @param Metrics This is a list of metrics, e.g., ("Correctness","Time","Efficiency").#
#' @param ExperimentName This an identifiers used to define the specific experiment in the output from this function.#
#' @param Type this is a character string specifying whether the experiment is a two sequence group of four sequence group experiment.#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return table this is a tibble holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' # importFrom stats#
#' # importFrom var#
#' # importFrom tibble#
#' @examples#
#' ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' Metrics=c("Comprehension","Modification")#
#' Type=c("4G", "4G", "4G", "4G")#
#' Groups=c("A","B","C","D")#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from a file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,#
#'   idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric.#
#'  CalculateRLevel1(Dataset=ReshapedData[[1]], StudyID, Groups=c("A","B","C","D"),#
#'    ExperimentName=ShortExperimentNames[1],Metrics,Type=Type[1],Control)#
#' # A tibble: 8 x 15#
#' # # A tibble: 8 x 15#
#' # Study Exp   Group Metric Id        n ControlFirst    var1   var2#
#' # <chr> <chr> <chr> <chr>  <chr> <int> <lgl>          <dbl>  <dbl>#
#' #   1 S2    E1    A     Compr… S2E1A     6 FALSE        0.0183  0.0163#
#' # 2 S2    E1    B     Compr… S2E1B     6 TRUE         0.0201  0.0326#
#' # 3 S2    E1    C     Compr… S2E1C     6 FALSE        0.00370 0.0155#
#' # 4 S2    E1    D     Compr… S2E1D     6 TRUE         0.0173  0.0201#
#' # 5 S2    E1    A     Modif… S2E1A     6 FALSE        0.0527  0.0383#
#' # 6 S2    E1    B     Modif… S2E1B     6 TRUE         0.0185  0.0482#
#' # 7 S2    E1    C     Modif… S2E1C     6 FALSE        0.00655 0.0244#
#' # 8 S2    E1    D     Modif… S2E1D     6 TRUE         0.0222  0.0266#
#' # # … with 6 more variables: varp <dbl>, ControlVarProp <dbl>,#
#' # #   VarProp <dbl>, vardiff <dbl>, r <dbl>, r.p <dbl>#
CalculateRLevel1 = function(Dataset,#
                            StudyID,#
                            Groups = c("A", "B", "C", "D"),#
                            ExperimentName,#
                            Metrics,#
                            Type,#
                            Control) {#
  if (Type == "2G")#
    Groups = Groups[1:2]#
  else#
    Groups = Groups[1:4]#
#
  NumReps = length(Groups)#
  table = NULL#
#
  r.table = NULL#
  NumMets = length(Metrics)#
  N = length(Dataset$SequenceGroup.1)#
  rvalidreps = 0#
  for (j in 1:NumMets) {#
    for (i in 1:NumReps) {#
      # Analyse the wide data for each Sequence Group in a specific experiment for a specific metric#
      #ss = base::subset(Dataset, SequenceGroup.1 == Groups[i])#
      #Fix LM, BAK Agreed:#
      ss = base::subset(Dataset, Dataset$SequenceGroup.1 == Groups[i])#
      # Select the data for the specific metric corresponding to each time period#
      name1 = base::paste(Metrics[j], "1", sep = ".")#
      name2 = base::paste(Metrics[j], "2", sep = ".")#
      res1 = base::subset(ss, select = name1)#
      n1 = length(ss$SequenceGroup.1)#
      res2 = base::subset(ss, select = name2)#
      n2 = length(ss$SequenceGroup.2) # n1 should equal n2#
      if (n1 != n2)#
        return("Incorrect subset")#
      # Analyse the difference data for each Metric#
      diff = res2 - res1#
#
      if (n1 > 1) {#
        vardiff = as.numeric(stats::var(diff))#
        var1 = as.numeric(stats::var(res1))#
        var2 = as.numeric(stats::var(res2))#
      }#
      else {#
        # Cannot construct a sensible variance#
        vardiff = 0#
        var1 = 0#
        var2 = 0#
      }#
#
	 if (vardiff > 0 & var1 > 0 & var2 > 0) {#
#
       # Ensure all variances are non-zero otherwise do not add to the row#
#
      # Identify which variance corresponds to the control condition#
      ControlFirst = FALSE#
      if (ss$Treatment.1[1] == Control)#
        ControlFirst = TRUE#
      if (ControlFirst)#
        controlvar = var1#
      else#
        controlvar = var2#
#
     	ControlVarProp = controlvar / (var1 + var2)#
      	TimePeriodVarProp = var1 / (var1 + var2)#
#
        # Calculate r without assuming variance homogeneity#
        r = as.numeric((var1 + var2 - vardiff) / (2 * sqrt(var1 * var2)))#
#
        #r = signif(r, 4)#
        #  Calculate r assuming variance homogeneity#
        pooled.var = (var1 + var2) / 2#
        r.p = as.numeric((2 * pooled.var - vardiff) / (2 * pooled.var))#
        #r.p = signif(r.p, 4)#
        #var1 = signif(var1, 4)#
        #var2 = signif(var2, 4)#
        #vardiff = signif(vardiff, 4)#
        #ControlVarProp = signif(ControlVarProp, 3)#
        #TimePeriodVarProp = signif(TimePeriodVarProp, 3)#
    # Construct an Id that is unique for a specific study, experiment and sequence group but treats r-values for different metrics as repeated measures.#
        Id = base::paste(StudyID, ExperimentName, sep = "")#
        Id = base::paste(Id, Groups[i], sep = "")#
        row = base::cbind(#
          Study = StudyID,#
          Exp = ExperimentName,#
          Group = Groups[i],#
          Metric = Metrics[j],#
          Id = Id,#
          n = n1,#
          ControlFirst = ControlFirst,#
          var1 = var1,#
          var2 = var2,#
          varp = pooled.var,#
          ControlVarProp = ControlVarProp,#
          VarProp = TimePeriodVarProp,#
          vardiff = vardiff,#
          r = r,#
          r.p = r.p#
        )#
        table = tibble::as_tibble(rbind(table, row))#
      }#
    }#
#
  }#
#
  	#Coerce table columns to correct format#
	table$n=as.integer(table$n)#
	table$ControlFirst=as.logical(table$ControlFirst)#
	table$var1=as.numeric(table$var1)#
	table$var2=as.numeric(table$var2)#
	table$varp=as.numeric(table$varp)#
	table$ControlVarProp=as.numeric(table$ControlVarProp)#
	table$VarProp=as.numeric(table$VarProp)#
	table$vardiff=as.numeric(table$vardiff)#
	table$r=as.numeric(table$r)#
	table$r.p=as.numeric(table$r.p)#
  return(table)#
#
}#
############################################################################################################
#' @title ExtractGroupSizeData#
#' @description This function constructs a table identifying the number of participants in each sequence group for a set of experiments each of which used a crossover design.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractGroupSizeData#
#' @param ExpDataWide this is a list of tibbles each comprising data from one experiment in its wide format#
#' @param StudyID an identifer for the group of related experiments (i.e., a family).#
#' @param ShortExperimentNames a list of character strings identifying each experiment.#
#' @param Type A list identifying the type of crossover "2G" or "4G" for each experiment in the family#
#' @param Groups a list of the terms used to specify sequence groups in the experiments.#
#' @return A tibble containing the number of participants in each sequence group in each experiment.#
#' @examples#
#' ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' Metrics=c("Comprehension","Modification")#
#' Type=c("4G", "4G", "4G", "4G")#
#' Groups=c("A","B","C","D")#
#' StudyID="S2"#
#' Control="SC"#
#' # Obtain experimental data from a file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,#
#'   idvar="ParticipantID", timevar="Period", ConvertToWide=TRUE#
#'   )#
#' ExtractGroupSizeData(ReshapedData, StudyID, ShortExperimentNames, Type, Groups=Groups)#
#' # A tibble: 16 x 4#
#' #  Study Exp   Group     n#
#' #  <chr> <chr> <chr> <int>#
#' #1 S2    Exp1  A         6#
#' #2 S2    Exp1  B         6#
#' #3 S2    Exp1  C         6#
#' #4 S2    Exp1  D         6#
#' #5 S2    Exp2  A         6#
#' #6 S2    Exp2  B         6#
#' #7 S2    Exp2  C         5#
#' #8 S2    Exp2  D         5#
#' #9 S2    Exp3  A         5#
#'#10 S2    Exp3  B         5#
#'#11 S2    Exp3  C         6#
#'#12 S2    Exp3  D         6#
#'#13 S2    Exp4  A         5#
#'#14 S2    Exp4  B         5#
#'#15 S2    Exp4  C         4#
#'#16 S2    Exp4  D         4#
ExtractGroupSizeData = function(ExpDataWide,#
                                StudyID,#
                                ShortExperimentNames,#
								Type,#
                                Groups=c("A","B","C","D"))#
  {#
#
  NumExp = length(ShortExperimentNames)#
#
  GroupDataTable = NULL#
  for (i in 1:NumExp) {#
    # ss = base::subset(#
    #   Table,#
    #   (Exp == ExperimentNames[i] &#
    #      Metric == FirstMetric),#
    #   select = c("Study", "Exp", "Group", "n")#
    # )#
    # Fix LM:#
    #ss = base::subset(#
    #  Table,#
    #  (Table$Exp == ExperimentNames[i] &#
    #     Table$Metric == FirstMetric),#
    #  select = c("Study", "Exp", "Group", "n")#
    #)#
    # Fix BAK - rewrite the entire algorithm to work on the basic data#
    # Find the wide format data for a specific experiment#
    ExpData=as.data.frame(ExpDataWide[[i]])#
#
    # Identify how many sequence groups there are in the specific experiment (either 2 or 4) for a specific crossover#
   if (Type[i]=="4G")  NumGroups=4 else  NumGroups=2#
#
    # Find the names used to distinguish the sequence groups#
    ExpGroups=Groups[1:NumGroups]#
#
    # Set up a variable to hold the number of participants in each sequence group#
    n=c(NumGroups,NA)#
    for (j in 1:NumGroups){#
    	#Find the data for each sequence group#
    	ss=base::subset(ExpData,ExpData$SequenceGroup.1==ExpGroups[j])#
    	# Find the number of participants in the specific sequence group#
    	n[j]=length(ss$ParticipantID)#
    	# Put the data together to specify the Study, Experiment, Sequence group and number of participants#
    	row= cbind(Study=StudyID,Exp=ShortExperimentNames[i],Group=ExpGroups[j],n=n[j])#
        GroupDataTable = as.data.frame(rbind(GroupDataTable, row))#
       }#
  }#
 ################# Change to tibble ###########################################
  GroupDataTable=tibble::as_tibble(GroupDataTable)#
#
  GroupDataTable$n=as.integer(GroupDataTable$n)#
  GroupDataTable$Study=as.character(GroupDataTable$Study)#
  GroupDataTable$Exp=as.character(GroupDataTable$Exp)#
  GroupDataTable$Group=as.character(GroupDataTable$Group)#
  return(GroupDataTable)#
}#
#
############################################################################################
#
#' @title ConstructLevel1ExperimentRData#
#' @description This function returns the r value for a 2-group (2G) or 4-Group (4G) Crossover experiment for a group of 1 or more experiments for each sequence group and each outcome metric. For sets of 2 or more experiments, the experiments are assumed to be replicates and to report the same sets of Metrics and have the same Control treatment and use the same squence Group identifiers, but are not necessarily the same Type. We return both the exact r value and the r value based on pooled variances for each sequence group and outcome metric.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ConstructLevel1ExperimentRData#
#' @param Data This is a list parameter each entry in the list holds the data for each participant in a 2-group or 4-group crossover experiment in the "wide" format. I.e., there is only one entry per participant. The data should have been generated from a long version of the data based on a variable labelled "Period" which is used to define which participant data was collected in the first period of the experiment - see function ExtractLevel1ExperimentRData.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param Groups This is a list that defined the sequence group identifiers used in the dataset.#
#' @param ExperimentNames This a list of identifiers used to define each experiment in the output from this function.#
#' @param Metrics This is a list of of character strings identifying each outcome metric reported in each of the experiments in the set of replicated experiments.#
#' @param Type this is a list of character strings specifying for each experiment whether the experiment is a 2-group or 4-group experiment#
#' @param Control this is a character string that defines the control treatment in the experiment.#
#' @return R.Data.Table this is a tibble holding information identifying for each metric and sequence group the first time period and second time period variance, the pooled variance, the variance of the difference values and the exact r and pooled r.#
#' @examples#
#' ##
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#'# Obtain experimental data from each file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData=ExtractExperimentData(dataset2, ExperimentNames=FullExperimentNames,#
#'   idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' # Calculate the correlations for each sequence group and each metric in each experiment#
#' ConstructLevel1ExperimentRData(Data=ReshapedData, StudyID=StudyID,#
#'   ExperimentNames=ShortExperimentNames, Groups=Groups, Metrics=Metrics, Type=Type,#
#'   Control=Control#
#' )#
#' # # A tibble: 32 x 15#
#' # Study Exp   Group Metric Id        n ControlFirst    var1   var2    varp#
#' # <chr> <chr> <chr> <chr>  <chr> <int> <lgl>          <dbl>  <dbl>   <dbl>#
#' #   1 S2    E1    A     Compr… S2E1A     6 FALSE        0.0183  0.0163 0.0173#
#' # 2 S2    E1    B     Compr… S2E1B     6 TRUE         0.0201  0.0326 0.0263#
#' # 3 S2    E1    C     Compr… S2E1C     6 FALSE        0.00370 0.0155 0.00962#
#' # 4 S2    E1    D     Compr… S2E1D     6 TRUE         0.0173  0.0201 0.0187#
#' # 5 S2    E1    A     Modif… S2E1A     6 FALSE        0.0527  0.0383 0.0455#
#' # 6 S2    E1    B     Modif… S2E1B     6 TRUE         0.0185  0.0482 0.0333#
#' # 7 S2    E1    C     Modif… S2E1C     6 FALSE        0.00655 0.0244 0.0155#
#' # 8 S2    E1    D     Modif… S2E1D     6 TRUE         0.0222  0.0266 0.0244#
#' # 9 S2    E2    A     Compr… S2E2A     6 FALSE        0.0194  0.0425 0.0309#
#' # 10 S2    E2    B     Compr… S2E2B     6 TRUE         0.0198  0.0192 0.0195#
#' # # … with 22 more rows, and 5 more variables: ControlVarProp <dbl>,#
#' # #   VarProp <dbl>, vardiff <dbl>, r <dbl>, r.p <dbl>#
#
ConstructLevel1ExperimentRData = function(Data,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Groups,#
                                          Metrics,#
                                          Type,#
                                          Control) {#
  # This calls the algorithm that constructs the correlation values for each metric and each independent sequence group in an experiment. It identifies the data for each sequnce group in each experiment for each experiment and collates the reurned r-values for each sequence group#
  NumExp = length(ExperimentNames)#
#
  R.Data.Table = NULL#
#
  for (i in 1:NumExp) {#
    r.data = CalculateRLevel1(#
      Data[[i]],#
      Groups = Groups,#
      StudyID = StudyID,#
      ExperimentName = ExperimentNames[i],#
      Metrics,#
      Type = Type[i],#
      Control = Control#
    )#
#
    R.Data.Table = tibble::as_tibble(rbind(R.Data.Table, r.data))#
  }#
#
  return(R.Data.Table)#
}#
#
##############################################################################################################
#
#' @title ExtractExperimentData#
#' @description This function reads datasets from a defined directory in the reproducer package that hold the results of a fmaily crossover experiments in the long format. It converts the data to the wide format if required.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractExperimentData#
#' @param DataSet This is a tibble holding the data for each crossover experiment in a family (a family can include only one experiment).#
#' @param ExperimentNames This is a list with the full names of each experiment.#
#' @param idvar This is the name of the column that contains the data for specific participants. It is only assumed to be unique within an experiment (default idvar="ParticipantID").#
#' @param timevar This is the name of the table column that defines which data was collected in a specific time period. This function assumes that there are only two time periods (default timevar="Period").#
#' @param ConvertToWide This determine whether the function converts the data to the wide format (default ConvertToWide=TRUE).#
#' @return A list with an entry for the data for each experiment. If ConvertToWide is TRUE, it returns the data in the wide format otherwise it returns the data as it was read. Within each list item the data is returned as a tibble#
#' #importFrom stats#
#' # importFrom tibble#
#' # importFrom base#
#' @examples#
#' ExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' # Obtain experimental data from each file and put in wide format#
#' dataset2= KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
#' ReshapedData = ExtractExperimentData(dataset2, ExperimentNames=ExperimentNames,#
#'   idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' ReshapedData[[1]]#
#'#
#'# A tibble: 24 x 15#
#'# ParticipantID ExperimentID.1 SequenceGroup.1 System.1 Treatment.1 Comprehension.1#
#'# <fct>         <fct>          <fct>           <fct>    <fct>                 <dbl>#
#'#   1 1             EUBAS          A               S1       AM                     0.77#
#'# 2 5             EUBAS          A               S1       AM                     0.61#
#'# 3 9             EUBAS          A               S1       AM                     0.61#
#'# 4 13            EUBAS          A               S1       AM                     0.52#
#'# 5 17            EUBAS          A               S1       AM                     0.43#
#'# 6 21            EUBAS          A               S1       AM                     0.77#
#'# 7 2             EUBAS          B               S1       SC                     0.92#
#'# 8 6             EUBAS          B               S1       SC                     0.63#
#'# 9 10            EUBAS          B               S1       SC                     0.51#
#'# 10 14            EUBAS          B               S1       SC                     0.64#
#'# … with 14 more rows, and 9 more variables: Modification.1 <dbl>, CrossOverID.1 <fct>,#
#'#   ExperimentID.2 <fct>, SequenceGroup.2 <fct>, System.2 <fct>, Treatment.2 <fct>,#
#'#   Comprehension.2 <dbl>, Modification.2 <dbl>, CrossOverID.2 <fct>#
ExtractExperimentData = function(DataSet,#
                                 ExperimentNames,#
                                 idvar = "ParticipantID",#
                                 timevar = "Period",#
                                 ConvertToWide = TRUE) {#
  # This algorithm reads the data for each experiment in a family (or a study)  and converts to the "wide" data format#
#
  NumExp = length(ExperimentNames)#
  ExpData = list()#
  for (i in 1:NumExp) {#
    # Read each of the data sets into a separate list entry#
#
		#tempdata =as.data.frame(base::subset(DataSet,ExperimentID==ExperimentNames[i]))#
    #Fix LM BAK agreed :#
    tempdata =as.data.frame(base::subset(DataSet,DataSet$ExperimentID==ExperimentNames[i]))#
#
# Find the wide version of the dataset based on the Period variable if required#
		if (ConvertToWide) {#
			tempdata=stats::reshape(tempdata,idvar=idvar,timevar=timevar,direction="wide")}#
# Return the data in the tibble format#
		ExpData[[i]] =tibble::as_tibble(tempdata)#
#
	}#
#
	return(ExpData)#
}#
#
########################################################################################################
#' @title CalculateLevel2ExperimentRData#
#' @description This function analyses data on r values obtained in the format obtained from  the ConstructLevel1ExperimentRData function and finds the r-value for each metric for each experiment.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export CalculateLevel2ExperimentRData#
#' @param Level1Data a tibble in the format produced by the ConstructLevel1ExperimentRData function which has r-values for each sequence group in a crossover experiment#
#' @param Groups This is a list that defines the sequence group labels used in the dataset.#
#' @param StudyID This holds an identifer used to identify the origin of the experimental data in the output from this function.#
#' @param ExperimentNames This a list of identifiers used to define each experiment in the output from this function.#
#' @param Metrics This is a list of of character strings identifying each outcome metric reported in each of the experiments in the set of replicated experiments.#
#' @param Type this is a list of character strings specifying for each experiment whether the experiment is a two sequence group "2G" or four sequence group "4G" experiment.#
#' return RExp.Table This is a table containing the pooled data variance and the pooled difference variance for the experiment and the value r for the eperiment for eachm metric#
#' @examples#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' # Obtain experimental data from each file and put in wide format#
#' ReshapedData = ExtractExperimentData(#
#'   KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM,#
#'   ExperimentNames=FullExperimentNames, idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' Lev1Data= ConstructLevel1ExperimentRData(ReshapedData, StudyID, ShortExperimentNames, Groups,#
#'   Metrics, Type, Control)#
#' CalculateLevel2ExperimentRData(Lev1Data,Groups=Groups,StudyID=StudyID,#
#'   ExperimentNames=ShortExperimentNames,Metrics=Metrics,Type=Type)#
#
#' # A tibble: 8 x 10#
#' #  StudyID ExpID     N Metric        PooledVar1 PooledVar2 VarProp PooledVar PooledDiffVar    r.Exp#
#' #  <chr>   <chr> <int> <chr>              <dbl>      <dbl>   <dbl>     <dbl>         <dbl>    <dbl>#
#' # 1 S2      S2E1     24 Comprehension     0.0148     0.0212   0.412    0.0180        0.0248  0.311#
#' # 3 S2      S2E2     22 Comprehension     0.0487     0.0224   0.684    0.0356        0.0534  0.250#
#' # 4 S2      S2E2     22 Modification      0.0445     0.0266   0.626    0.0356        0.0628  0.117#
#' # 5 S2      S2E3     22 Comprehension     0.0353     0.0402   0.467    0.0377        0.105  -0.391#
#' # 6 S2      S2E3     22 Modification      0.0433     0.0414   0.511    0.0424        0.0997 -0.176#
#' # 7 S2      S2E4     18 Comprehension     0.0439     0.0237   0.649    0.0338        0.0355  0.475#
#' # 8 S2      S2E4     18 Modification      0.0322     0.0592   0.353    0.0457        0.0894  0.0222#
CalculateLevel2ExperimentRData = function(Level1Data,#
                                          Groups,#
                                          StudyID,#
                                          ExperimentNames,#
                                          Metrics,#
                                          Type) {#
  NumExp = length(ExperimentNames)#
#
  RExp.Table = NULL#
  NumMets = length(Metrics)#
  for (i in 1:NumExp) {#
    # Extract the variance data for a specific experiment#
    # LM fix 1: ExpData = base::subset(Level1Data, Exp == ExperimentNames[i])#
    # BAK Checked and agreed#
    ExpData = base::subset(Level1Data, Level1Data$Exp == ExperimentNames[i])#
    if (Type[i] == "2G")#
      SequenceGroups = Groups[1:2]#
    else#
      SequenceGroups = Groups[1:4]#
#
    NumGroups = length(SequenceGroups)#
    ExpID = ExpData$Exp[1]#
    ID = base::paste(StudyID, ExpID, sep = "")#
    for (j in 1:NumMets) {#
      # Find the sequence group statistics for a specific Metric#
      # LM fix 2: MetData = base::subset(ExpData, Metric == Metrics[j])#
      # Wrong#
      # BAK Fix 3: MetData = base::subset(ExpData, Level1Data$Metric == Metrics[j])#
#
     MetData = base::subset(ExpData, ExpData$Metric == Metrics[j])#
#
      # Find the number of participants in the experiment#
      NumParticipants = sum(MetData$n)#
#
      # Calculate the pooled sequence group variance and difference variance#
      PooledVar1 = sum(MetData$var1 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      PooledVar2 = sum(MetData$var2 * (MetData$n - 1)) / (NumParticipants -#
                                                            NumGroups)#
      VarProp = PooledVar1 / (PooledVar1 + PooledVar2)#
      PooledVar = sum(MetData$varp * (MetData$n - 1)) / (NumParticipants -#
                                                           NumGroups)#
      PooledDiffVar = sum(MetData$vardiff * (MetData$n - 1)) / (NumParticipants -#
                                                                  NumGroups)#
      # Calculate r#
      r = (2 * PooledVar - PooledDiffVar) / (2 * PooledVar)#
      # Tidy up calculated values - premature restrictions may cause rounding errors#
      #r = signif(r, 4)#
      # PooledVar1 = signif(PooledVar1, 4)#
      # PooledVar2 = signif(PooledVar2, 4)#
      # PooledVar = signif(PooledVar, 4)#
      #VarProp = signif(VarProp, 4)#
      #PooledDiffVar = signif(PooledDiffVar, 4)#
      # Put the output into a format that identifes the study, experiment, experiment size and metric and the average r for the experiment#
      row =#
        cbind(#
          PooledVar1 = PooledVar1,#
          PooledVar2 = PooledVar2,#
          VarProp = VarProp,#
          PooledVar = PooledVar,#
          PooledDiffVar = PooledDiffVar,#
          r.Exp = r#
        )#
      RExp.Table = rbind(#
        RExp.Table,#
        cbind(#
          StudyID = StudyID,#
          ExpID = ID,#
          N = NumParticipants,#
          Metric = Metrics[j],#
          row#
        )#
      )#
#
    }#
#
  }#
#
	RExp.Table=tibble::as_tibble(RExp.Table)#
#
	# Coerce the data items to the correct formats#
	RExp.Table$N=as.integer(RExp.Table$N)#
	RExp.Table$PooledVar1=as.numeric(RExp.Table$PooledVar1)#
	RExp.Table$PooledVar2=as.numeric(RExp.Table$PooledVar2)#
	RExp.Table$PooledVar=as.numeric(RExp.Table$PooledVar)#
	RExp.Table$VarProp=as.numeric(RExp.Table$VarProp)#
	RExp.Table$PooledDiffVar=as.numeric(RExp.Table$PooledDiffVar)#
	RExp.Table$r.Exp=as.numeric(RExp.Table$r.Exp)#
	return(RExp.Table)#
#
}#
#
#############################################################################################################
#
# Functions to help produce tables for the paper#
#
#' @title ExtractSummaryStatisticsRandomizedExp#
#' @description This function extracts data obtained from the lme4 package lmer function. It assumes a simple randomized experiment with each element having one or more repeated measures. It outputs the mean together with its standard error and confidence interval bounds.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractSummaryStatisticsRandomizedExp#
#' @param lmeRA The output from the lmer function#
#' @param N The total number of observations#
#' @param alpha the probability level to be used when constructing the confidence interval bounds.#
#' @return REA.Summary A dataframe holding the number of observations N, the overall mean value as its standard errorreported as by the lmer function, and its confidence interval bounds.#
#' @examples#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' ReshapedData= ExtractExperimentData(#
#'   KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM,#
#'   ExperimentNames=FullExperimentNames, idvar="ParticipantID", timevar="Period",#
#'   ConvertToWide=TRUE#
#' )#
#' NewTable= ConstructLevel1ExperimentRData(ReshapedData, StudyID, ShortExperimentNames, Groups,#
#'   Metrics,Type,Control#
#'   )#
#' resRe=lme4::lmer(r~(1|Id),data=NewTable)#
#' summary(resRe)#
#' # Linear mixed model fit by REML ['lmerMod']#
#' # Formula: r ~ (1 | Id)#
#' # REML criterion at convergence: 47.8#
#' # Scaled residuals:#
#' #    Min      1Q  Median      3Q     Max#
#' # -1.4382 -0.9691  0.2190  0.8649  1.4761#
#' ##
#' # Random effects:#
#' #  Groups   Name        Variance Std.Dev.#
#' #   Id       (Intercept) 0.03978  0.1994#
#' #   Residual             0.20974  0.4580#
#' #  Number of obs: 32, groups:  Id, 16#
#' ##
#' #  Fixed effects:#
#' #             Estimate Std. Error t value#
#' #  (Intercept)  0.06175    0.09508   0.649#
#' #  N=length(NewTable$r)#
#'  ExtractSummaryStatisticsRandomizedExp(lmeRA=resRe,N=32,alpha=0.05)#
#' #      N    Mean      SE LowerBound UpperBound#
#' #   1 32 0.06175 0.09508    -0.1319     0.2554#
ExtractSummaryStatisticsRandomizedExp = function(lmeRA, N, alpha = 0.05) {#
  REA.fe.t = stats::coef(summary(lmeRA))#
  REA.Mean = REA.fe.t[1, "Estimate"]#
  REA.SE = REA.fe.t[1, "Std. Error"]#
  # Calculate confidence interval for mean r#
  vv = stats::qt(alpha / 2, N)#
  REA.LowerBound = REA.Mean + vv * REA.SE#
  REA.UpperBound = REA.Mean - vv * REA.SE#
#
  # Truncate the values for readability#
  REA.Mean = signif(REA.Mean, 4)#
  REA.SE = signif(REA.SE, 4)#
  REA.LowerBound = signif(REA.LowerBound, 4)#
  REA.UpperBound = signif(REA.UpperBound, 4)#
  REA.Summary = data.frame(#
    cbind(#
      N = N,#
      Mean = REA.Mean,#
      SE = REA.SE,#
      LowerBound = REA.LowerBound,#
      UpperBound = REA.UpperBound#
    )#
  )#
#
  return(REA.Summary)#
}#
#########################################################################################################################
#' @title calculateBasicStatistics#
#' @description This function calculates the following statistcs for a set of data: length, mean, median, variance, standard error of the mean, and confidence interval bounds. The input data imust be a vector of 2 or more numerical values.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export calculateBasicStatistics#
#' @param x The data to be summarized#
#' @param alpha The probability level to be used when constructing the confidence interval bounds.#
#' @return A dataframe comprising the length, mean, variance, standard error and confidence limit bounds of the input data x.#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' ReshapedData= ExtractExperimentData(KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM,ExperimentNames=FullExperimentNames, idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#' NewTable= ConstructLevel1ExperimentRData(ReshapedData,StudyID,ShortExperimentNames,Groups,Metrics,Type,Control)#
#' calculateBasicStatistics(NewTable$r)#
#'  #    N    Mean Median Variance      SE LowerBound UpperBound#
#'  # 1 32 0.06175 0.1688   0.2482 0.08808    -0.1109     0.2344#
#
calculateBasicStatistics = function(x, alpha = 0.05) {#
  # Finds the basic statistics for a set of data#
  N = length(x)#
  mean_x = mean(x)#
  var_x = stats::var(x)#
  median_x = stats::median(x)#
  se_x = sqrt(var_x / N)#
  crit <- stats::qnorm(alpha / 2)#
  lowerbound_x = mean_x + crit * se_x#
  upperbound_x = mean_x - crit * se_x#
  summary_x = data.frame(#
    cbind(#
      N = N,#
      Mean = mean_x,#
      Median = median_x,#
      Variance = var_x,#
      SE = se_x,#
      LowerBound = lowerbound_x,#
      UpperBound = upperbound_x#
    )#
  )#
  summary_x = signif(summary_x, 4)#
  return(summary_x)#
}#
#############################################################################################################
#' @title calculateGroupSummaryStatistics#
#' @description This function calculates the following statistics data within groups: length, mean, median, variance, standard error of the mean, and confidence interval bounds.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export calculateGroupSummaryStatistics#
#' @param x The data to be summarized. This must be a vector of 2 or more numerical values#
#' @param Group The categorical data data defining the groups. This must vector of the same length as x containing factors specifying the data groups#
#' @return A dataframe comprising the number, mean, variance, standard error and confidence limit bounds of the data in each category#
#' @examples#
#' ShortExperimentNames=c("E1","E2","E3","E4")#
#' FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
#' Metrics=c("Comprehension","Modification")#
#' Groups=c("A","B","C","D")#
#' Type=c(rep("4G",4))#
#' StudyID="S2"#
#' Control="SC"#
#' ReshapedData= ExtractExperimentData(#
#'   KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM,#
#'   ExperimentNames=FullExperimentNames, idvar="ParticipantID",timevar="Period",#
#'   ConvertToWide=TRUE#
#' )#
#' NewTable= ConstructLevel1ExperimentRData(ReshapedData, StudyID,#
#'   ShortExperimentNames, Groups, Metrics, Type, Control#
#' )#
#' SeqGroupLev=NULL#
#' N.NT=length(NewTable$r)#
#' for (i in 1:N.NT) {#
#' 	if (NewTable$n[i]<=8) SeqGroupLev[i]=as.character(NewTable$n[i])#
#' 	if (NewTable$n[i]>8) SeqGroupLev[i]=as.character(9)#
#'  }#
#'  calculateGroupSummaryStatistics(NewTable$r,Group=SeqGroupLev)#
#' #     N    Mean  Median Variance  StDev     SE#
#' #  1  4 -0.0833 -0.1699   0.2314 0.4810 0.2405#
#' #  2 12  0.3658  0.4477   0.2109 0.4592 0.1326#
#' #  3 16 -0.1300 -0.2214   0.1933 0.4397 0.1099#
calculateGroupSummaryStatistics = function(x, Group) {#
  # Calculates summary statistics for grouped data#
  agg.mean = stats::aggregate(x, by = list(Group), FUN = mean)#
  agg.mean = reshape::rename(agg.mean, c(x = "Mean"))#
  agg.median = stats::aggregate(x, by = list(Group), FUN = stats::median)#
  agg.median = reshape::rename(agg.median, c(x = "Median"))#
  agg.length = stats::aggregate(x, by = list(Group), FUN = length)#
  agg.length = reshape::rename(agg.length, c(x = "N"))#
  agg.var = stats::aggregate(x, by = list(Group), FUN = stats::var)#
  agg.var = reshape::rename(agg.var, c(x = "Variance"))#
  agg.StDev = sqrt(agg.var$Variance)#
  agg.SE = sqrt(agg.var$Variance / agg.length$N)#
#
  summaryTable = NULL#
  summaryTable = data.frame(#
    cbind(#
      N = agg.length$N,#
      Mean = agg.mean$Mean,#
      Median = agg.median$Median,#
      Variance = agg.var$Variance,#
      StDev = agg.StDev,#
      SE = agg.SE#
    )#
  )#
  summaryTable = signif(summaryTable, 4)#
#
  return(summaryTable)#
}#
#
#############################################################################################################################
#
#' @title rSimulations#
#' @description This function simulates many datasest from the same bivariate distibution to investigate the distribution of corrleations for specific sample sizes.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export rSimulations#
#' @param mean The mean used for one of bivariate distruibutions - assumed to be the control condition in an experiment.#
#' @param var The variance used for both treatment groups. It must be a real value greater than 0.#
#' @param diff This value is added to the parameter mean to specify the mean for the other bivariate distribution - assumed to be the treatment condition in an experiment.#
#' @param r This specifies the correlation coefficient to be used for the bivariate normal distribution it must be a value in the range [-1,1].#
#' @param N The number of observations in each simulated bivariate normal data set.#
#' @param VarAdj This value will be added to the variance of the treatment condition.#
#' @param reps The number of bivariate data sets that will be simulated.#
#' @param seed This specifies the seed value for the simulations and allows the experiment to be repeated.#
#' @param returntSignificant If set to true the percentage of times the t-test delivered a value signiicant at the 0.05 level is reported (default returntSignificant=F).#
#' @param returndata If set to FALSE, the function returns the summary information across all the replications  (default returndata=F). If set to TRUE the function outputs the r and variance ratio, and variance accuracy values generated in each replication.#
#' @param plothist If set to T, the function outputs a histogram of the r-values, the varprop values and the accuracy values (default plothist=F).#
#' @return output If returndata=F, the output returns summary information about the average of r and the variance properties across the replicated data sets.If returndata=T, the function returns the r-values obtained for each of the simulated data sets to gether with the varaiance ratio, the variance accuracy measure and a dummy variable indicating whether a test of significance between the mean values was significant (which is indicated by the dummy variable being set to 1) or not (which is indicated by the dummy variable being set to 0)#
#' @examples#
#' output=rSimulations(mean=0,var=1,diff=0,r=0.25,N=4,reps=10000)#
#' output=signif(output,4)#
#' output#
#' #  r.Mean r.Median  Var.r PercentNegative Mean.VarProp Variance.VarProp ...#
#' # 1 0.2132   0.3128 0.3126           34.21       0.5036          0.06046 ...#
#' output=rSimulations(mean=0,var=1,diff=0.8,r=0.25,N=60,reps=10000,returntSignificant=TRUE)#
#' output=signif(output,4)#
#' output#
#' #   r.Mean r.Median   Var.r PercentNegative Mean.VarProp Variance.VarProp ...#
#' # 1 0.2492   0.2534 0.01529            2.62       0.5009         0.003897 ...#
#' output=rSimulations(mean=0,var=1,diff=0,r=0.25,N=30,reps=10,returndata=TRUE)#
#' output#
#' #     rvalues   VarProp VarAccuracy VarDiffAccuracy tSig#
#' #1  0.3981111 0.4276398   0.8630528       0.6974386    0#
#' #2  0.2104742 0.4994285   0.7812448       0.8224174    0#
#' #3  0.4252424 0.4933579   1.1568545       0.8866058    0#
#' #4  0.3502651 0.6004373   0.8710482       0.7628923    0#
#' #5  0.3845145 0.6029086   0.9618363       0.7998859    0#
#' #6  0.1397217 0.4201069   1.1817022       1.3582855    0#
#' #7  0.2311455 0.3894894   0.8322239       0.8594886    0#
#' #8  0.3725047 0.5985897   1.1742117       0.9938662    0#
#' #9  0.4881618 0.2712268   0.7585261       0.5723671    0#
#' #10 0.1568071 0.3936400   0.9869924       1.1143561    0#
rSimulations = function(mean,#
                        var,#
                        diff,#
                        r,#
                        N,#
                        reps,#
                        VarAdj = 0,#
                        seed = 123,#
                        returntSignificant = F,#
                        returndata = F,#
                        plothist = F) {#
  # This function simulates bivariate normal distributions in order to investigate the distribution of given r-values for different sample sizes.#
  set.seed(seed)#
#
  # Set up variables to hold the data from each replication#
#
  rvalues = c(rep(NA, reps))#
  varratio = c(rep(NA, reps))#
  varaccuracy = c(rep(NA, reps))#
  vardiffaccuracy=c(rep(NA,reps))#
  tSig = c(rep(NA, reps))#
#
  rnegative = 0#
#
  # Set up the parameters for the bivariate normal distribution#
  meanvec = c(mean + diff, mean)#
  covar = r * sqrt(var) * sqrt(var + VarAdj)#
  sigma = matrix(c(var + VarAdj, covar, covar, var),#
                 nrow = 2,#
                 ncol = 2)#
  for (i in 1:reps) {#
    # Generate and analyse each simulation#
    Mydataraw = MASS::mvrnorm(N, meanvec, sigma)#
    Mydataraw = as.data.frame(Mydataraw)#
    names(Mydataraw) = c("y", "x")#
#
    # Find the variance and r values#
    varx = stats::var(Mydataraw$x)#
    vary = stats::var(Mydataraw$y)#
    Mydatadiff = Mydataraw$y - Mydataraw$x#
#
    vardiff=stats::var(Mydatadiff)#
#
# Measure the heterogeneity of the between participants variance#
    varratio[i] = varx / (varx + vary)#
#
 # Measure the accuracy of the between participants variance#
    varaccuracy[i] = (varx + vary) / (2 * var + VarAdj)#
#
 # Measure the accuracy of the difference data variance#
	 vardiffaccuracy[i]=vardiff/((2*var+VarAdj)*(1-r))#
#
    rvalues[i] = (varx + vary - vardiff) / (2 * sqrt(varx * vary))#
#
    if (rvalues[i] <= 0)#
      rnegative = rnegative + 1#
#
    # Do a t-test of the difference between the means#
    ttest.results = stats::t.test(Mydataraw$x, Mydataraw$y)#
    tSig[i] = if (ttest.results$p.value < 0.05)#
      1#
    else#
      0#
#
  }#
#
  rnegative = 100 * rnegative / reps#
  raverage = mean(rvalues)#
  rmedian = stats::median(rvalues)#
  Varr = stats::var(rvalues)#
  meanvarrat = mean(varratio)#
  varvarrat = stats::var(varratio)#
#
  meanvaracc = mean(varaccuracy)#
  varvaracc = stats::var(varaccuracy)#
#
  meanvardiffacc=mean(vardiffaccuracy)#
  varvardiffacc=stats::var(vardiffaccuracy)#
  PercentSig = 100 * mean(tSig)#
#
  if (plothist)#
  {#
    graphics::par(mfrow = c(4, 2))#
    graphics::hist(rvalues, main = "Correlation Histogram", xlab = "Correlations")#
    graphics::boxplot(rvalues, main = "Correlation Boxplot")#
#
    graphics::hist(varratio, main = "Variance Ratio Histogram", xlab = "Variance Ratio Values")#
    graphics::boxplot(varratio, main = "Variance Ratio Boxplot")#
    graphics::hist(varaccuracy, main = "Variance Accuracy Histogram", xlab = "Variance Accuracy Values")#
    graphics::boxplot(varaccuracy, main = "Variance Accuracy Boxplot")#
#
    	graphics::hist(vardiffaccuracy,main="Diff Var Accuracy Histogram",xlab="Diff Var Accuracy Values")#
	graphics::boxplot(vardiffaccuracy, main="Diff Var Accuracy Boxplot")#
#
  }#
#
  count = 0#
  TheoreticalVarRatio = var / (2*var + VarAdj)#
  LowerVarRatio = TheoreticalVarRatio / 2#
  UpperVarRatio = TheoreticalVarRatio + TheoreticalVarRatio / 2#
  for (i in 1:reps)#
  {#
    if (varratio[i] < LowerVarRatio |#
        varratio[i] > UpperVarRatio)#
      count = count + 1#
  }#
  acccount = 0#
  for (i in 1:reps)#
  {#
    if (varaccuracy[i] < 0.5 | varaccuracy[i] > 1.5)#
      acccount = acccount + 1#
  }#
#
  diffacccount = 0#
  for (i in 1:reps)#
  {#
    if (vardiffaccuracy[i] < 0.5 | vardiffaccuracy[i] > 1.5)#
      diffacccount = diffacccount + 1#
  }#
#
  if (!returndata)#
  {#
    output = data.frame(#
      r.Mean = raverage,#
      r.Median = rmedian,#
      Var.r = Varr,#
      PercentNegative = rnegative,#
      Mean.VarProp = meanvarrat,#
      Variance.VarProp = varvarrat,#
      Percent.VarProp.Anomalies = 100 * count / reps,#
      Mean.Varaccuracy = meanvaracc,#
      Var.varaccuracy = varvaracc,#
      Percent.VarAccuracy.Anomalies = 100 * acccount / reps,#
      Mean.VarDiffAccuracy=meanvardiffacc,#
      Var.VarDiffAccuracy= varvardiffacc,#
      Percent.DiffVarAccuracy.Anomalies=100*diffacccount / reps#
    )#
    if (returntSignificant)#
      output = data.frame(cbind(output, PercentSig = PercentSig))#
#
  }#
  else {#
    output = data.frame(cbind(#
      rvalues = rvalues,#
      VarProp = varratio,#
      VarAccuracy = varaccuracy,#
      VarDiffAccuracy=vardiffaccuracy,#
      tSig = tSig#
    ))#
  }#
  return(output)#
#
}
#script to obtain correlation coefficients#
# The data must be in the new format i.e. one data set for each study with each experiment identified by a ExperimentID column.#
#
# Until the functions are available in the reproducer package run the file rFunctions from the tempory reproducer directory before running this script#
#***************************************************************************#
#
Level1Results.Table=NULL#
Level2Results.Table=NULL#
GroupSize.Table=NULL#
StudyDets.Table=NULL#
#
Experiments=0#
#***************************************************************************#
ShortName="Scan2015EMSE"#
#
FullExperimentNames=c("USB2")#
ExperimentNames=c("E1")#
#
Experiments=Experiments+length(ExperimentNames)#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello15EMSE#
#
Type=c("4G")#
Groups=c("A","B","C","D")#
#
Treat="CL"#
Control="NOCL"#
Metrics=c("Correctness","Time","Efficiency")#
#
StudyID="S1"#
#
# Obtain experimental data from each file and put in wide format#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
# Collect data about the number of participants in each experiment and sequence group#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
# Collect Information about each study#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=1,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
###########################################################################
#
NewTable=NULL#
ShortName="Scan2014TOSEM"#
FullExperimentNames=c("EUBAS","R1UCLM","R2UCLM","R3UCLM")#
ExperimentNames=c("E1","E2","E3","E4")#
#
Experiments=Experiments+length(ExperimentNames)#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14TOSEM#
Metrics=c("Comprehension","Modification")#
#
Groups=c("A","B","C","D")#
#
Type=c(rep("4G",4))#
#
StudyID="S2"#
Treat="AM"#
Control="SC"#
# Obtain experimental data from each file and put in wide format#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequence group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=4,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
##################################################################################
#
ShortName="Abra2013"#
#
Type=c(rep("4G",5))#
#
FullExperimentNames = c("Italy1", "Italy2", "Spain1", "Spain2", "Spain3")#
ExperimentNames=c("E1","E2","E3","E4","E5")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Filenames = c("Italy1.csv",#
              "Italy2.csv",#
              "Spain1.csv",#
              "Spain2.csv",#
              "Spain3.csv")#
#
Groups=c("A","B","C","D")#
#
Metrics = c("Comprehension")#
Treat = "DM"#
Control = "NODM"#
#
# Experience collected for first two experiments#
#
NewTable=NULL#
#
StudyID="S3"#
#
# Obtain experimental data from each file and put in wide format#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Abrahao13TSE#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=5,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
#
#############################################################################
ShortName="Tor2017"#
#
Groups=c("A","B","C","D")#
#
FullExperimentNames=c("UniBas","PoliTo2","UniGe")#
ExperimentNames=c("E1","E2","E3")#
#
Experiments=Experiments+length(ExperimentNames)#
Type=c("4G","4G","2G")#
#
Metrics=c("Comprehension")#
NewTable=NULL#
StudyID="S4"#
Treat="OD"#
Control="NoOD"#
#
# Discussed experience with UML and SE experience#
# Obtain experimental data from each file and put in wide format#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Torchiano17JVLC#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=1,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################################
#
# Experiment I and ExperimentII used same participant teams, so results need to be aggregated#
#
ShortName="Scan2014JVLC"#
#
FullExperimentNames=c("Experiment.I","Experiment.II")#
ExperimentNames=c("E1","E2")#
#
Experiments=Experiments+1#
#
Type=c("2G","2G")#
#
Metrics=c("Quality","Time")#
#
Groups=c("A","B")#
#
NewTable=NULL#
#
StudyID="S5"#
Treat="TPS"#
Control="F2F"#
# Obtain experimental data from each file and put in wide format#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14JVLC#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequence group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
# Aggregate so only one set of data for the same participants for each metric#
#
CombinedVar1=stats::aggregate(NewTable$var1,by=list(NewTable$Group,NewTable$Metric),FUN="mean")#
CombinedVar2=stats::aggregate(NewTable$var2,by=list(NewTable$Group,NewTable$Metric),FUN="mean")#
CombinedDiffvar=stats::aggregate(NewTable$vardiff,by=list(NewTable$Group,NewTable$Metric),FUN="mean")#
#
CombinedData=NewTable[1:4,]#
#
CombinedData$var1=CombinedVar1$x#
CombinedData$var2=CombinedVar2$x#
CombinedData$vardiff=CombinedDiffvar$x#
#
PooledVar=(CombinedData$var1+CombinedData$var2)/2#
#
CombinedData$r=(CombinedData$var1+CombinedData$var2-CombinedData$vardiff)/(2*sqrt(CombinedData$var1*CombinedData$var2))#
PooledVar=(CombinedData$var1+CombinedData$var2)/2#
#
CombinedData$r=(CombinedData$var1+CombinedData$var2-CombinedData$vardiff)/(2*sqrt(CombinedData$var1*CombinedData$var2))#
CombinedData$r.p=(2*PooledVar-CombinedData$vardiff)/(2*PooledVar)#
#
CVP=(rep(NULL,4))#
for (i in 1:4) {if (CombinedData$ControlFirst[i]) #
	{CVP[i]=CombinedData$var1[i]/(2*PooledVar[i]) #
		} #
	else #
		{CVP[i]=CombinedData$var2[i]/(2*PooledVar[i])#
			}#
	}#
CombinedData$ControlVarProp=CVP#
#
TPVP=(rep(NULL,4))#
 TPVP=CombinedData$var1/(2*PooledVar)#
#
CombinedData$var1=as.numeric(signif(CombinedData$var1,4))#
CombinedData$var2=as.numeric(signif(CombinedData$var2,4))#
CombinedData$vardiff=as.numeric(signif(CombinedData$vardiff,4))#
CombinedData$r=as.numeric(signif(CombinedData$r,4))#
CombinedData$r.p=as.numeric(signif(CombinedData$r.p,4))#
CombinedData$ControlVarProp=as.numeric(signif(CVP,4))#
CombinedData$VarProp=as.numeric(signif(TPVP,4))#
#
# End of special code to combined the two experiments#
#####################################################
# The data is a repeated 2G experiment where the participants are the same in both parts. So only count the participants in the first part.#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=c("Exp1"),Type=c("2G"),Groups=Groups)#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,CombinedData)#
#
StudyDets.row=as.data.frame(base::cbind(NumExps=1,NumMets=length(Metrics),Num4GExps=0,Num2GExps=1,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=as.data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################################
ShortName="Scan2014EASE"#
FullExperimentNames=c("EUBAS","R1UGOT1")#
ExperimentNames=c("E1","E2")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("4G","4G")#
Metrics=c("Comprehension","Time")#
Groups=c("A","B","C","D")#
NewTable=NULL#
#
StudyID="S6"#
#
Treat="RD"#
Control="NORD"#
# Obtain experimental data from each file and put in wide format#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello14EASE#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
###################################################################################
ShortName="Gra2015"#
#
FullExperimentNames=c("UniBas","UniSa")#
ExperimentNames=c("E1","E2")#
#
Experiments=Experiments+length(ExperimentNames)#
Type=c("4G","4G")#
#
Metrics=c("Comprehension","Time")#
#
Groups=c("A","B","C","D")#
NewTable=NULL#
#
StudyID="S7"#
#
Treat="Mo"#
Control="NOMo"#
#
# Obtain experimental data from each file and put in wide format#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Gravino15JVLC#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
#
###################################################################################
#
ShortName="Ric2014"#
#
FullExperimentNames=c("UniBas1","UniBas2","PoliTo")#
ExperimentNames=c("E1","E2","E3")#
#
SubjectTypes=c("UGY2","UGY3","UGY2")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("4G","4G","4G")#
#
Metrics=c("Comprehension","Efficiency","Time")#
Groups=c("A","B","C","D")#
NewTable=NULL#
#
StudyID="S8"#
Treat="S"#
Control="T"#
#
# Obtain experimental data from each file and put in wide format#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Ricca14TOSEM#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=3,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
###################################################################################
#
ShortName="Reg2015"#
#
Groups=c("A","B","C","D")#
#
FullExperimentNames=c("UniBA","UniBZ")#
#
ExperimentNames=c("E1","E2")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("4G","4G")#
#
Metrics=c("Comprehension","Time")#
#
NewTable=NULL#
#
StudyID="S9"#
Treat="Precise"#
Control="Light"#
#
# Obtain experimental data from each file and put in wide format#
#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Reggio15SSM#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
#################################################################
#
ShortName="Mad2010"#
FullExperimentNames=c("P2007")#
ExperimentNames=c("E1")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("2G")#
#
Metrics=c("PATP","NATPPH")#
#
Groups=c("A","B")#
#
NewTable=NULL#
#
StudyID="S10"#
#
Treat = "TFSP" #
Control = "TLSP"#
#
# Obtain experimental data from each file and put in wide format#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Madeyski10#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequence group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=0,Num2GExps=1,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
###########################################################################################
# Extra Dataset Ricca et al. 2010. Although there are four experiments, drop outs meant that some sequence groups had only 1 participant from which variance data cannot be constructed, so we include data from only two of the experiments.#
ShortName=c("Ricca2010")#
FullExperimentNames=c("Exp1","Exp2")#
ExperimentNames=c("E1","E2")#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("4G","4G")#
#
Metrics=c("FMeasure","Time")#
#
Groups=c("A","B","C","D")#
#
NewTable=NULL#
#
StudyID="S11"#
#
Treat = "Conallen" #
Control = "UML"#
# New to obtain the data and change it into the format required by the analysis functions.#
newdata=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Ricca10TSE#
#
File=NULL#
#
# Need to remove single entries - where participants only took part in one of the two lab sessions#
for  (i in 1:length(newdata$ParticipantID)) {#
	tempchar=newdata$ParticipantID[i]#
	tempdata=base::subset(newdata,ParticipantID==tempchar)#
	if (length(tempdata$ParticipantID)==2) File=base::rbind(File,newdata[i,])#
}#
#
# Extract data from the first two experiments and convert to wide format#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=as.data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=as.data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################
# Scanniello et al. 2017 Errors in C and Java#
#
ShortName=c("Scan2017")#
FullExperimentNames=c("POLINA","PROF","UNIBAS","UNINA")#
ExperimentNames=c("E1","E2","E3","E4")#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("4G","4G","4G","4G")#
#
Metrics=c("Time","FMeasure","Efficiency")#
#
Groups=c("A","B","C","D")#
#
NewTable=NULL#
#
StudyID="S12"#
#
Treat = "CO" #
Control = "NOCO"#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Scanniello17ACMTOTW#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=as.data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=4,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=as.data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################
# Romano Noise data#
#
ShortName=c("Rom2018")#
#
FullExperimentNames=c("EXP1")#
ExperimentNames=c("E1")#
#
Experiments=Experiments+length(ExperimentNames)#
#
Type=c("2G")#
#
Metrics=c("Fc","Avg")#
#
Groups=c("A","B")#
#
NewTable=NULL#
#
StudyID="S13"#
#
Treat = "NOISE" #
Control = "NORMAL"#
File=reproducer::KitchenhamEtAl.CorrelationsAmongParticipants.Romano18ESEM#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
# Calculate the correlations for each sequence group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=0,Num2GExps=1,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################
# Fernandez et al. data#
#
ShortName="Fern2012"#
#
Groups=c("A","B","C","D")#
#
FullExperimentNames=c("CSI2010","CS12011")#
#
ExperimentNames=c("E2","E3")#
#
Experiments=Experiments+length(ExperimentNames)#
Type=c("4G","4G")#
#
Metrics=c("Effectiveness","Efficiency")#
#
NewTable=NULL#
#
StudyID="S14"#
#
Treat="WUEP"#
Control="HE"#
#
# Obtain experimental data from each file and put in wide format#
#
File=read.table("/Users/barbarakitchenham/Dropbox/CrossOverIssues/dataSets/Confidential.Fernandez13JSS.csv",head=T,sep=",")#
#
File=tibble::as_tibble(File)#
#
File$ParticipantID=as.character(File$ParticipantID)#
#
ExpData=reproducer::ExtractExperimentData(File,ExperimentNames=FullExperimentNames,idvar="ParticipantID",timevar="Period",ConvertToWide=TRUE)#
#
# Calculate the correlations for each sequnce group and each experiment and each metric in the family#
#
NewTable=reproducer::ConstructLevel1ExperimentRData(Data=ExpData,StudyID=StudyID,ExperimentNames=ExperimentNames,Groups=Groups,Metrics=Metrics,Type=Type,Control=Control)#
#
utils::write.table(NewTable,"/Users/barbarakitchenham/Dropbox/CrossOverIssues/dataSets/KitchenhamEtAl.CorrelationsAmongParticipants.Fernandez13JSS.Level1Results.txt",row.names=F,quote=F)#
GroupSize=reproducer::ExtractGroupSizeData(ExpData,StudyID,ShortExperimentNames=ExperimentNames,Type,Groups=Groups)#
#
GroupSize.Table=base::rbind(GroupSize.Table,GroupSize)#
Level1Results.Table=base::rbind(Level1Results.Table,NewTable)#
StudyDets.row=data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=2,Num2GExps=0,NumParticipants=sum(GroupSize$n)))#
StudyDets.row=data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)#
#
Level2Exp.Table=NULL#
#
Level2Exp.Table=reproducer::CalculateLevel2ExperimentRData(Level1Data=NewTable,Groups=Groups,StudyID=StudyID,ExperimentNames=ExperimentNames,Metrics=Metrics,Type=Type)#
#
Level2Results.Table=base::rbind(Level2Results.Table,Level2Exp.Table)#
################################################################
# Laitenberg et al. data#
ExperimentNames=c("QuasiExperiment","Replication1","Replication2")#
#
ExpID=c("E1","E2","E3")#
#
ShortName="Lait2001"#
StudyID="S15"#
N=c(9,10,10)#
#
Experiments=Experiments+length(ExperimentNames)#
MetricsNames=c("DefectDetectionEffectiveness","CostPerDefectDuringDetection","CostPerDefectDuringMeeting","OverallCostPerDEfect")#
#
Metrics=c("M1","M2","M3","M4")#
#
SID=c(rep(StudyID,12))#
ExpID=c(rep(ExpID[1],4),rep(ExpID[2],4),rep(ExpID[3],4))#
MetID=c(rep(Metrics,3))#
ID=paste(SID,ExpID,sep="")#
Num=c(rep(9,4),rep(10,4),rep(10,4))#
PooledDiffVar=rep(NA,12)#
PooledVar1=rep(NA,12)  #
PooledVar2=rep(NA,12)#
VarProp=rep(NA,12)  #
PooledVar=as.numeric(c(0.16^2,24.03^2,1.8^2,26.83^2,0.21^2,22.59^2,2.99^2,24.88^2,0.14^2,12.59^2,1.65^2,18.81^2))#
#
r.Exp=as.numeric(c(0.77,0.62,0.55,0.65,0.78,-0.02,0.48,0.21,0.03,0.03,0.17,0.27))#
#
LaitData=as.data.frame(base::cbind(StudyID=SID,ExpID=ID,N=Num,Metric=MetID,PooledVar1=PooledVar1,PooledVar2=PooledVar2, VarProp=VarProp,PooledVar=PooledVar, PooledDiffVar=PooledDiffVar,r.Exp=r.Exp))#
#
LaitData=tibble::as_tibble(LaitData)#
#
# Ensure all the data values have the correct type#
LaitData$N=as.integer(LaitData$N)#
LaitData$PooledVar1=as.numeric(LaitData$PooledVar1)#
LaitData$PooledVar2=as.numeric(LaitData$PooledVar2)#
LaitData$VarProp=as.numeric(LaitData$VarProp)#
LaitData$PooledVar=as.numeric(LaitData$PooledVar)#
LaitData$PooledDiffVar=as.numeric(LaitData$PooledDiffVar)#
LaitData$r.Exp=as.numeric(LaitData$r.Exp)#
#
Level2Results.Table=base::rbind(Level2Results.Table,LaitData)#
#
StudyDets.row=as.data.frame(base::cbind(NumExps=length(ExperimentNames),NumMets=length(Metrics),Num4GExps=0,Num2GExps=3,NumParticipants=sum(N)))#
StudyDets.row=as.data.frame(base::cbind(StudyName=ShortName,StudyID=StudyID,StudyDets.row))#
#
StudyDets.Table=base::rbind(StudyDets.Table,StudyDets.row)
dataset=Level1Results.Table#
#
L2dataset=Level2Results.Table
N=length(dataset$r)#
#
rsummary=reproducer::calculateBasicStatistics(dataset$r)#
#
DescSummary=base::subset(rsummary,select=c(N,Mean,Median,Variance,SE))#
#
DescStats.Table=as.data.frame(base::rbind(DescStats.Table,base::cbind(Source="All data",Type="r.e",DescSummary)))#
# Find r estimate from Random Effects Analysis#
resRe=lme4::lmer(r~(1|Id),data=dataset)#
base::sink("REAnalysisr_e.txt")#
split=TRUE#
summary(resRe)#
base::sink()#
#
# Extract summary statistics allowing for the repeated measures obtained from each metric#
REA.r.e=ExtractSummaryStatisticsRandomizedExp(resRe,N=N)#
#
SE=REA.r.e$SE#
#
DescStats.Table=as.data.frame(base::rbind(DescStats.Table,base::cbind(Source="REAnalysis",Type="r.e",N=REA.r.e$N,Mean=REA.r.e$Mean,Median="",Variance="",SE=SE)))#
#
# Prepare a table with the sample statistics for r_p this contributes to TABLE in the main paper#
#
rsummary=calculateBasicStatistics(dataset$r.p)#
DescSummary=base::subset(rsummary,select=c(N,Mean,Median,Variance,SE))#
#
DescStats.Table=as.data.frame(base::rbind(DescStats.Table,base::cbind(Source="All data",Type="r.p",DescSummary)))#
# Perform an RE analysis of the r_p data#
#
# Random Effect Analysis of r_p values to include in Supplementary Material#
resRp=lme4::lmer(r.p~(1|Id),data=dataset)
q()
# Barbara, please find below a complete example of a well documented function.#
# Apart from the functions which are of general use (i.e., reusable), which we put in this file and add to reproducer,#
# we need functions (documented in the same way) to reproduce tables and figures, which we could keep in the paper or a separate script related to a specific paper.#
#' @title calculateSmallSampleSizeAdjustment#
#' @description Function calculates the small sample size adjustment for standardized mean effect sizes#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export calculateSmallSampleSizeAdjustment#
#' @param df A vector of degrees of freedom#
#' @param exact Default value=TRUE, if exact==TRUE the function returns the exact value of the adjustment(s) which is suitable for small values of df, if exact==FALSE the function returns the approximate version of the adjustment(s). See Hedges and Olkin 'Statistical methods for Meta-Analysis' Academic Press 1985.#
#' @return small sample size adjustment value#
#' @examples#
#' df <- 2#
#' a <- calculateSmallSampleSizeAdjustment(df)#
#' # > a#
#' # [1] 0.5641896#
#'#
#' df=c(5,10,17)#
#' adjexact=calculateSmallSampleSizeAdjustment(df)#
#' # > adjexact#
#' # [1] 0.8407487 0.9227456 0.9551115#
#' # Hedges and Olkin values 0.8408, 0.9228,0.9551#
#' adjapprox=calculateSmallSampleSizeAdjustment(df,FALSE)#
#' # > adjapprox#
#' # [1] 0.8421053 0.9230769 0.9552239#
#' # Another example:#
#' df=c(10,25,50)#
#' calculateSmallSampleSizeAdjustment(df,exact=TRUE)#
#' # [1] 0.9227456 0.9696456 0.9849119#
#' calculateSmallSampleSizeAdjustment(df,exact=FALSE)#
#' # [1] 0.9230769 0.9696970 0.9849246#
calculateSmallSampleSizeAdjustment = function(df, exact = TRUE) {#
  exactvec = c(rep(exact, length(df)))#
  # If exact is TRUE but the df is too large gamma cannot be calculated and the approximate value is used#
  c = ifelse(exactvec &#
               df < 340,#
             sqrt(2 / df) * gamma(df / 2) / gamma((df - 1) / 2) ,#
             (1 - 3 / (4 * df - 1)))#
  return(c)#
}#
#' @title varStandardizedEffectSize#
#' @description Function calculates the exact variance of a standardized effect size based on the relationship between t and the standardized effect size, see Morris and DeShon, Combining Effect Size Estimates in Meta-Analysis With Repeated Measures and Independent-Groups Designs, Psychological Methods, 7 (1), pp 105-125.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export varStandardizedEffectSize#
#' @param d An unadjusted standardized effect size#
#' @param A The squared constant linking t and d i.e. t*sqrt(A)=d#
#' @param f The degrees of freedom of the t value#
#' @param returnVarg if set to TRUE return the variance of the small sample size adjusted standardized effect size (g), otherwise returns var(d) where d is the input parameter#
#' @return if returnVarg if set to TRUE, return var(g) otherwise var(d)#
#' @examples#
#' d=0.5#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=FALSE)#
#' # [1] 0.2007699#
#' varStandardizedEffectSize(d,sqrt(1/30),29,returnVarg=TRUE)#
#' # [1] 0.1904167#
varStandardizedEffectSize = function(d, A, f, returnVarg = TRUE) {#
  c = reproducer::calculateSmallSampleSizeAdjustment(f)#
  g = d * c # g is a better estimate of the population standardized effect size delta than d#
  var = (f / (f - 2)) * (A + g ^ 2) - g ^ 2 / c ^ 2 # best estimate of the variance of d#
  if (returnVarg)#
    var = c ^ 2 * var # best estimate of the variance of g#
  return(var)#
}#
#' @title RandomizedBlocksAnalysis#
#' @description The function performs a heteroscedastic test of a two treatment by J blocks randomized blocks effect size. The data are assumed to be stored in $x$ in list mode, a matrix or a data frame. If in list mode, length(x) is assumed to correspond to the total number of groups. All groups are assumed to be independent. Missing values are automatically removed.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedBlocksAnalysis#
#' @param x the structure holding the data. In list format, for a 2 treatment by J block randomized blocks experiments, there are  2J list elements each one specifying the outcome for a specific block and a specific treatment.#
#' @param con is a 2J list containing the contrast coefficients that are used to calculate the mean effet size.#
#' @param alpha=0.05 is the Type 1 error level used for the test of significance#
#' @return The t-test and its associated metrics (i.e., critical value stansard error and degrees of freedom) and the estimate of the contrast with its upper and lower confidence interval bounds and p-value.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' vec=c(-1,1,-1,1)/2#
#' RandomizedBlocksAnalysis(x,con=vec,alpha=0.05)#
#' # $n#
#' # [1] 10 10 10 10#
#' # $test#
#' #      test     crit        se       df#
#' # [1,] 4.432644 2.038622 0.2798104 31.33793#
#' # $psihat#
#' #      psihat  ci.lower ci.upper      p.value#
#' # [1,] 1.2403 0.6698721 1.810728 0.0001062952#
#' # dat=c(x[[1]],x[[2]],x[[3]],x[[4]])#
#' # matx=matrix(dat,nrow=10,ncol=4)#
#' # RandomizedBlocksAnalysis(matx,con=c(-1,1,-1,1),alpha=0.05)#
#' #$n#
#' #[1] 10 10 10 10#
#
#' #$test#
#' #         test     crit        se       df#
#' #[1,] 1.755752 2.038351 0.6729856 31.44242#
#
#' #$psihat#
#' #       psihat  ci.lower ci.upper    p.value#
#' #[1,] 1.181596 -0.190185 2.553377 0.08886958#
#
RandomizedBlocksAnalysis <-#
  function(x,#
           con = c(-0.5, 0.5, -0.5, 0.5),#
           alpha = .05) {#
    if (base::is.data.frame(x))#
      x = base::as.matrix(x)#
#
    flag <- TRUE#
    if (alpha != .05 && alpha != .01)#
      flag <- FALSE#
#
    if (base::is.matrix(x)) {#
      # Corrected to cope with matrix format#
      y = list()#
      for (j in 1:ncol(x))#
        y[[j]] <- x[, j]#
      x = list()#
      x = y#
    }#
#
    if (!base::is.list(x))#
      stop("Data must be stored in a matrix or in list mode.")#
    con <- base::as.matrix(con)#
    if (ncol(con) > 1)#
      stop("Only one linear contrast permitted for a standard randomized blocks experiment")#
    J <- length(x)#
    sam = NA#
    h <- base::vector("numeric", J)#
    w <- base::vector("numeric", J)#
    xbar <- base::vector("numeric", J)#
    for (j in 1:J) {#
      xx <- !is.na(x[[j]])#
      val <- x[[j]]#
      x[[j]] <- val[xx]  # Remove missing values#
      sam[j] = length(x[[j]])#
      h[j] <- length(x[[j]])#
      # h is the number of observations in the jth group.#
      w[j] <-#
        ((length(x[[j]]) - 1) * stats::var(x[[j]])) / (h[j] * (h[j] - 1)) # The variance of the jth group#
      xbar[j] <-#
        base::mean(x[[j]]) # The mean of the jth group#
    }#
    #if(sum(con^2)>0){#
    if (nrow(con) != length(x)) {#
      stop("The number of groups does not match the number of contrast coefficients.")#
    }#
    psihat <- base::matrix(0, 1, 4)#
    dimnames(psihat) <-#
      list(NULL, c("psihat", "ci.lower", "ci.upper",#
                   "p.value"))#
    test <- matrix(0, 1, 4)#
    dimnames(test) <- list(NULL, c("test", "crit", "se", "df"))#
    df <- 0#
    psihat[1, 1] <- sum(con[, 1] * xbar)#
    sejk <-#
      sqrt(sum(con[, 1] ^ 2 * w)) # The pooled standard error of the contrast#
#
    test[1, 1] <-#
      sum(con[, 1] * xbar) / sejk # The value of the t-test#
    df <-#
      (sum(con[, 1] ^ 2 * w)) ^ 2 / sum(con[, 1] ^ 4 * w ^ 2 / (h - 1)) # Degrees of freeedom allowing for heterogeneity#
    vv = (1 - alpha / 2)#
    crit <-#
      stats::qt(vv, df) #The critical value of the t-test for the degrees of freedom#
    test[1, 2] <- crit#
    test[1, 3] <- sejk#
    test[1, 4] <- df#
    psihat[1, 2] <- psihat[1, 1] - crit * sejk#
    psihat[1, 3] <- psihat[1, 1] + crit * sejk#
    psihat[1, 4] <- 2 * (1 - stats::pt(abs(test[1, 1]), df))#
    list(n = sam,#
         test = test,#
         psihat = psihat)#
  }#
#'  @title Kendalltaupb#
#'  @description  Computes point bi-serial version of  Kendall's tau plus a 1-alpha confidence interval using the method recommended by Long and Cliff (1997).  The algorithm is based on Wilcox's code but was extended to return the consistent variance and the confidence intervals based on the t-distribution. Also added a Diagnostic parameter to output internal calculations.#
#' @author Rand Wilcox, Barbara Kitchenham and Lech Madeyski#
#' @export Kendalltaupb#
#' @param x either a matrix with two columns containg two correlated variables or a vector of variables#
#' @param y if y=NULL, assume x is a matrix with two columns, otherwise y is a vector of variables with x[i] and x[i] being from the same experimental unit#
#' @param alpha=0.05, the Type 1 error level used for statistical tests#
#' @return list containing the estimate of Kendall's tau, it hypothesis testing variance, and the t-test value obtained from it, the significnce of the t-test, the consistent variance of tau and its confidence intervals based on both the normal dstribution and the t-test (recommended by Long and Cliff)#
#' @examples#
#' x=c(1.2,3,1.8,2,2,0.5,0.5,1,3,1)#
#' y=c(1,1,1,1,1,0,0,0,0,0)#
#
#' Kendalltaupb(x,y,alpha=.05)#
#' # $cor#
#' # [1] 0.3555556#
#' # $ci#
#' # [1] -0.04198026  0.55555556#
#' # $cit#
#' # [1] -0.1240567  0.5555556#
#' # $test#
#' # [1] 1.431084#
#' # $sqse#
#' # [1] 0.0617284#
#' # $consistentvar#
#' # [1] 0.04113925#
#' # $siglevel#
#' # [1] 0.1524063#
Kendalltaupb <- function(x, y = NULL, alpha = .05) {#
  if (length(x) <= 3)#
    stop("Too few data points")#
  if (length(x) != length(y))#
    stop("Invalid input vectors")#
  if (length(x) != length(x[!is.na(x)]))#
    stop("Missing values not permitted")#
  if (length(y) != length(y[!is.na(y)]))#
    stop("Missing values not permitted")#
#
  # Needs a test to ensure one of the variables contains only 1 or 0 values#
#
  m = cbind(x, y)#
#
  x = m[, 1]#
  y = m[, 2]#
  xdif <- base::outer(x, x, FUN = "-")#
  ydif <- base::outer(y, y, FUN = "-")#
  tv <- sign(xdif) * sign(ydif)#
#
  #Corrects error in Wilcox's algorithm#
  n <- length(x)#
  dbar <- base::apply(tv, 1, sum) / (n - 1)#
#
  tau <- sum(tv) / (n * (n - 1))#
#
  A <- sum((dbar - tau) ^ 2) / (n - 1)#
  B <-#
    (n * (n - 1) * (-1) * tau ^ 2 + sum(tv ^ 2)) / (n ^ 2 - n - 1)#
  C <-#
    (4 * (n - 2) * A + 2 * B) / (n * (n - 1)) # C is the consistent variance#
#
  # Confidence interval based on normal distribution - not recommended by Long and Cliff#
  crit <- stats::qnorm(alpha / 2)#
  cilow <- tau + crit * sqrt(C)#
  cihi <- tau - crit * sqrt(C)#
  if (cilow < (-n / (2 * (n - 1))))#
    cilow = -n / (2 * (n - 1)) # Applies limits assuming a point bi-serial tau#
#
  if (cihi > n / (2 * (n - 1)))#
    cihi = n / (2 * (n - 1)) # Applies limits assuming a point bi-serial tau#
  # Confidence interval based on t distribution - recommended by Long and Cliff#
  vv = stats::qt(alpha / 2, n - 3)#
#
  cilowt = tau + vv * sqrt(C)#
  if (cilowt < (-n / (2 * (n - 1))))#
    cilowt = -n / (2 * (n - 1))#
  cihit = tau - vv * sqrt(C)#
  if (cihit > n / (2 * (n - 1)))#
    cihit = n / (2 * (n - 1))#
#
  # t-test based on hypothesis test variance - not recommended by Long and Cliff#
  se = sqrt((2 * (2 * n + 5)) / (9 * n * (n - 1)))#
  test <- tau / se#
#
  siglevel <- 2 * (1 - stats::pnorm(abs(test)))#
  list(#
    cor = tau,#
    ci = c(cilow, cihi),#
    cit = c(cilowt, cihit),#
    test = test,#
    sqse = se ^ 2,#
    consistentvar = C,#
    siglevel = siglevel#
  )#
#
}#
#' @title Cliffd#
#' @description This function implements finds Cliff's d and its confidence intervals. The null hypothesis is that for two independent group, P(X<Y)=P(X>Y). The function reports a 1-alpha confidence interval for P(X>Y)-P(X<Y). The algorithm computes a confidence interval for Cliff's d using the method in Cliff, 1996, p. 140, eq 5.12. The function is based on code produce by Rand Wilcox but has been amended. The plotting function has been removed and the dependency on Wilcox's binomci function has been removed. Construction of confidence intervals if values in one group are all larger than values in the other group has been amended to use the smallest non-zero variance method. Upper and lower confidence interval bounds cannot assume invalid values, i.e. values <-1 or >1.#
#' @author Rand Wilcox, amendments Barbara Kitchenham and Lech Madeyski#
#' @export Cliffd#
#' @param x is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @param sigfig is the number of significant digit. If sigfig>0 the data in x and y is truncated to the specified value.#
#' @return list including the value of Cliffs d its consistent variance and confidence intervals and the equivalent probability of superiority value and its confidence intervals.#
# ' @examples#
#' x=c(1.2,3,2.2,4,2.5,3)#
#' y=c(3,4.2,4,6,7,5.9)#
#' # Cliffd(x,y)#
#' #  $n1#
#' # [1] 6#
#' # $n2#
#' # [1] 6#
#' # $cl#
#' # [1] -0.9772519#
#' # $cu#
#' # [1] -0.3476461#
#' # $d#
#' # [1] -0.8611111#
#' # $sqse.d#
#' # [1] 0.02017931#
#' # $phat#
#' # [1] 0.06944444#
#' # $summary.dvals#
#' #        P(X<Y)     P(X=Y)     P(X>Y)#
#' # [1,] 0.8888889 0.08333333 0.02777778#
#' # $p.cl#
#' # [1] 0.01137405#
#' # $p.cu#
#' # [1] 0.326177#
#' ##
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#' Cliffd(z,y)#
#' # $n1#
#' # [1] 4#
#' # $n2#
#' # [1] 4#
#' # $cl#
#' # [1] -1#
#' # $cu#
#' # [1] -0.4368172#
#' # $d#
#' # [1] -1#
#' # $sqse.d#
#' # [1] 0.009765625#
#' # $phat#
#' # [1] 0#
#' # $summary.dvals#
#' #      P(X<Y) P(X=Y) P(X>Y)#
#' # [1,]      1      0      0#
#' # $p.cl#
#' # [1] 0#
#' # $p.cu#
#' # [1] 0.2815914#
#
Cliffd <- function(x,#
                   y,#
                   alpha = .05,#
                   sigfig = -1) {#
  # Check that the data is valid#
  if (length(x) <= 1)#
    stop("Too few data points")#
  if (length(y) <= 1)#
    stop("Too few data points")#
  if (length(x) != length(x[!is.na(x)]))#
    stop("Missing values not permitted")#
  if (length(y) != length(y[!is.na(y)]))#
    stop("Missing values not permitted")#
#
  # Truncate the data if necessary#
  if (sigfig > 0) {#
    x = signif(x, sigfig)#
    y = signif(y, sigfig)#
  }#
#
  m <- base::outer(x, y, FUN = "-")#
  msave <- m#
  m <- sign(m)#
#
  d <- base::mean(m)#
#
  phat <- (1 + d) / 2#
#
  flag = TRUE#
  if (phat == 0 || phat == 1)#
    flag = FALSE#
  q0 <- sum(msave == 0) / length(msave)#
  qxly <- sum(msave < 0) / length(msave)#
  qxgy <- sum(msave > 0) / length(msave)#
  c.sum <- base::matrix(c(qxly, q0, qxgy), nrow = 1, ncol = 3)#
  dimnames(c.sum) <- list(NULL, c("P(X<Y)", "P(X=Y)", "P(X>Y)"))#
  if (flag) {#
    # This is appropriate for the consistent variance of d#
    sigdih <- sum((m - d) ^ 2) / (length(x) * length(y) - 1)#
#
    di <- NA#
    for (i in 1:length(x))#
      di[i] <-#
      sum(x[i] > y) / length(y) - sum(x[i] < y) / length(y)#
#
    dh <- NA#
    for (i in 1:length(y))#
      dh[i] <-#
      sum(y[i] < x) / length(x) - sum(y[i] > x) / length(x)#
    sdi <- stats::var(di)#
    sdh <- stats::var(dh)#
    # sh is the consistent variance of d#
    sh <-#
      ((length(y) - 1) * sdi + (length(x) - 1) * sdh + sigdih) / (length(x) *#
                                                                    length(y))#
    zv <- stats::qnorm(alpha / 2)#
    cu <-#
      (d - d ^ 3 - zv * sqrt(sh) * sqrt((1 - d ^ 2) ^ 2 + zv ^ 2 * sh)) /#
      (1 - d ^ 2 + zv ^ 2 * sh)#
    cl <-#
      (d - d ^ 3 + zv * sqrt(sh) * sqrt((1 - d ^ 2) ^ 2 + zv ^ 2 * sh)) /#
      (1 - d ^ 2 + zv ^ 2 * sh)#
  }#
  if (!flag) {#
    # phat=0 and d=-1 or phat=1 and d=1#
    # Cannot calculate standard error of d, use alternative minimum change approach to calculate a "close" slightly too large estimate. Calculates the smallest non-zero consistent variance#
    sh = 0#
    di = NA#
    dh = NA#
    nx = length(x)#
    ny = length(y)#
#
    tempn = nx + ny - 1#
#
    if (phat == 1) {#
      cu <- 1#
      tempn = nx + ny - 1#
      disturbedx = c(nx:tempn)#
      disturbedy = c(1:nx)#
      disturbedanalysis = Cliffd(disturbedx, disturbedy)#
      cl = disturbedanalysis$cl#
#
    }#
#
    if (phat == 0) {#
      cl = -1#
      disturbedy = c(nx:tempn)#
      disturbedx = c(1:nx)#
      disturbedanalysis = Cliffd(disturbedx, disturbedy)#
      cu = disturbedanalysis$cu#
    }#
#
    sh = disturbedanalysis$sqse.d#
  }#
  # Need to construct confidence intervals on phat equivalent to the d confidence intervals#
  pci = c((1 + cu) / 2, (1 + cl) / 2)#
#
  cid.results = list(#
    n1 = length(x),#
    n2 = length(y),#
    cl = cl,#
    cu = cu,#
    d = d,#
    sqse.d = sh,#
    phat = phat,#
    summary.dvals = c.sum,#
    p.cl = pci[2],#
    p.cu = pci[1]#
  )#
  return(cid.results)#
}#
#
#' @title   calculatePhat#
#' @description This function extract the probability of superiority (i.e., Phat) and its confidence interval based on Brunner and Munzel (2000) heteroscedastic analog of WMW test. It is based on Wilcox'x bmp function with some amendments. It does not include a plotit facility. It uses the smallest non-zero variance to idetify confidence intervals and statistical significance for values of Phat=0 and Phat=1. It ensure that confidence intervals do not take on invalid values such as values <0 or >1.#
#' @author Rand Wilcox amendments by Barbara Kitchenham and Lech Madeyski#
#' @export calculatePhat#
#' @param x is a vector of values from group 1#
#' @param y is a vector of values from group 2#
#' @param alpha is the Type 1 error level for statistical tests#
#' @param sigfig If sigfig>0 the data in x and y is truncated to the specified number of significant digits.#
#' @return list including the value of the t-test for PHat, the estimate of PHat and Cliff's d, and the confidence intervals for PHat.#
#' @examples#
#' x=c(1.2, 3.0, 2.2, 4.0, 2.5, 3.0)#
#' y=c(3,4.2,4,6,7,5.9)#
#' calculatePhat(x,y)#
#' # $test.stat#
#' # [1] 6.381249#
#' # $phat#
#' # [1] 0.9305556#
#' # $dhat#
#' # [1] 0.8611111#
#' # $sig.level#
#' # [1] 0.0001191725#
#' # $s.e.#
#' # [1] 0.06747199#
#' # $ci.p#
#' # [1] 0.7783001 1.0000000#
#' # $df#
#' # [1] 9.148489#
#' # Another example:#
#' z=c(1,2,3,4)#
#' y=c(5,6,7,8)#
#' calculatePhat(z,y)#
#' # $test.stat#
#' # [1] 10.6066#
#' # $phat#
#' # [1] 1#
#' # $dhat#
#' # [1] 1#
#' # $sig.level#
#' # [1] 4.135921e-05#
#' # $s.e.#
#' # [1] 0.04419417#
#' # $ci.p#
#' # [1] 0.8918608 1.0000000#
#' # $df#
#' # [1] 6#
calculatePhat <- function(x,#
                          y,#
                          alpha = .05,#
                          sigfig = -1) {#
  if (sigfig > 0) {#
    x = signif(x, sigfig)#
    y = signif(y, sigfig)#
  }#
#
  x <- x[!is.na(x)]  # Remove any missing values#
  y <- y[!is.na(y)]#
  n1 <- length(x)#
  n2 <- length(y)#
  N <- n1 + n2#
  n1p1 <- n1 + 1#
  flag1 <- c(1:n1)#
  flag2 <- c(n1p1:N)#
  R <- rank(c(x, y))#
  R1 <- mean(R[flag1])#
  R2 <- mean(R[flag2])#
  phat <- (R2 - (n2 + 1) / 2) / n1#
  dhat <- 2 * phat - 1#
#
  flag = TRUE # TRUE if phat ne 0 and phat ne 1#
  if (phat == 0 | phat == 1)#
    flag = FALSE#
#
  if (flag) {#
    Rg1 <- base::rank(x)#
    Rg2 <- base::rank(y)#
    S1sq <-#
      sum((R[flag1] - Rg1 - R1 + (n1 + 1) / 2) ^ 2) / (n1 - 1)#
    S2sq <-#
      sum((R[flag2] - Rg2 - R2 + (n2 + 1) / 2) ^ 2) / (n2 - 1)#
    sig1 <- S1sq / n2 ^ 2#
    sig2 <- S2sq / n1 ^ 2#
    se <- sqrt(N) * sqrt(N * (sig1 / n1 + sig2 / n2))#
    bmtest <- (R2 - R1) / se#
#
    df <-#
      (S1sq / n2 + S2sq / n1) ^ 2 / ((S1sq / n2) ^ 2 / (n1 - 1) + (S2sq /#
                                                                     n1) ^ 2 / (n2 - 1))#
    sig <- 2 * (1 - pt(abs(bmtest), df))#
    vv <- stats::qt(alpha / 2, df)#
    ci.p <- c(phat + vv * se / N, phat - vv * se / N)#
#
  }#
  else {#
    # Calculate the smallest non-negative variance and use results to approximate variance and confidence intervals of phat. Gives a more realistic confidence interval than other methods#
    Nl1 = N - 1#
    newx = c(1:n1)#
    newy = c(n1:Nl1)#
    newres = calculatePhat(newx, newy)#
    se = newres$s.e. * N#
    df = newres$df#
    sig = newres$sig.level#
    vv <- stats::qt(alpha / 2, df)#
    ci.p = c(phat + vv * se / N, phat - vv * se / N)#
    bmtest = newres$test.stat#
    if (sum(x) > sum(y))#
      bmtest = bmtest * -1#
  }#
  # Ensure confidence intervals dont assume impossible values#
  if (ci.p[1] < 0)#
    ci.p[1] = 0#
  if (ci.p[2] > 1)#
    ci.p[2] = 1#
  list(#
    test.stat = bmtest,#
    phat = phat,#
    dhat = dhat,#
    sig.level = sig,#
    s.e. = se / N,#
    ci.p = ci.p,#
    df = df#
  )#
}#
#
#' @title  Calc4GroupNPStats#
#' @description This function does a non-parametric analysis of a randomized blocks experiment assuming 2 blocks and 2 treatment conditions.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export Calc4GroupNPStats#
#' @param x1 is the data associated with  treatment A in one block 1#
#' @param x2 is the data associated with treatement B in block 1#
#' @param x3 is the data associated with treatment A in block 2#
#' @param x4  is the data associated with treatment B in block 2#
#' @param signif is the number of significant digits in the data. If >0 the datav will be appropriately truncated#
#' @param alpha is he significance level for all statistical tests#
#' @return The function returns the point biserial version of Kendall's tau and its variance, Cliff's d and its variance, the probability of superiority, phat, and its variance, for the 4 group experiment experiment.#
#' @examples#
#' set.seed(123)#
#' x=list()#
#' x[[1]]=rnorm(10,0,1)#
#' x[[2]]=rnorm(10,0.8,1)#
#' x[[3]]=rnorm(10,0.5,1)#
#' x[[4]]=rnorm(10,1.3,1)#
#' Calc4GroupNPStats(x[[1]],x[[2]],x[[3]],x[[4]],sigfig=-1,alpha=0.05)#
#' #   N    phat    phat.var    phat.df     phat.test   phat.pvalue     phat.sig    d   vard...#
#' #1  40   0.17    0.004966667 31.00131    -4.682539   5.324252e-05    TRUE    -0.66   0.02060121..#
Calc4GroupNPStats = function(x1,#
                             x2,#
                             x3,#
                             x4,#
                             sigfig = -1,#
                             alpha = 0.05) {#
  #     Check the significant digits to ensure that equal values are properly detected#
  if (sigfig > 0) {#
    x1 = signif(x1, sigfig)#
    x2 = signif(x2, sigfig)#
    x3 = signif(x3, sigfig)#
    x4 = signif(x4, sigfig)#
  }#
  # Set up a dummy variable such that the observations using treatment A in block 1 are associated with the value 1 and observations using treatment B in block 1 are associated with the value 0#
  dummy1 = c(rep(1, length(x1)), rep(0, length(x2)))#
  # Concatenate the observations in block 1#
  xCO1 = c(x1, x2)#
  #	Use Wilcox's function to find tau and its two variances for block 1#
  tau1 = Kendalltaupb(xCO1, dummy1)#
  n1 = length(x1) + length(x2)#
  # Set up a dummy variable such that the observations using treatment A in block 2 are associated with the value 1 and observations using treatment B in block 2 are associated with the value 0#
#
  dummy2 = c(rep(1, length(x3)), rep(0, length(x4)))#
  # Concatenate the observations in block 12#
  xCO2 = c(x3, x4)#
#
  #	Use Wilcox's function to find tau and its two variances for block 2#
#
  tau2 = Kendalltaupb(xCO2, dummy2)#
  n2 = length(x3) + length(x4)#
  N = n1 + n2#
#
  average.tau = (tau1$cor + tau2$cor) / 2#
  combinedsqse = (tau1$sqse + tau2$sqse) / 4#
#
  ctvar = (tau1$consistentvar + tau2$consistentvar) / 4#
#
  # Find the confidence limits on the combined tau using t-distribution#
  vv = stats::qt(alpha / 2, N - 6)#
  ci.t <-#
    c(average.tau + vv * sqrt(combinedsqse),#
      average.tau - vv * sqrt(combinedsqse))#
#
  sigCVt = ci.t[1] > 0 | ci.t[2] < 0#
#
  # Find the confidence limits on the combined tau using normal distribution#
#
  ci.n <-#
    c(#
      average.tau + qnorm(alpha / 2) * sqrt(combinedsqse),#
      average.tau - qnorm(alpha / 2) * sqrt(combinedsqse)#
    )#
#
  sigCVn = ci.n[1] > 0 | ci.n[2] < 0#
  # Find the average d and the combined variances for the full experiment#
  d = (Cliffd(x1, x2)$d + Cliffd(x3, x4)$d) / 2#
  vard = (Cliffd(x1, x2)$sqse.d + Cliffd(x3, x4)$sqse.d) / 4#
#
  zv <- stats::qnorm(alpha / 2)#
  d.cu <-#
    (d - d ^ 3 - zv * sqrt(vard) * sqrt((1 - d ^ 2) ^ 2 + zv ^ 2 * vard)) /#
    (1 - d ^ 2 + zv ^ 2 * vard)#
  d.cl <-#
    (d - d ^ 3 + zv * sqrt(vard) * sqrt((1 - d ^ 2) ^ 2 + zv ^ 2 * vard)) /#
    (1 - d ^ 2 + zv ^ 2 * vard)#
  d.sig = d.cu < 0 | d.cl > 0#
#
  # Find the average phat and the combined variances for the full experiment#
#
  B1.BMP = calculatePhat(x2, x1)#
  B2.BMP = calculatePhat(x4, x3)#
  phat1 = B1.BMP$phat#
  phat2 = B2.BMP$phat#
  phat = (phat1 + phat2) / 2#
  se1 = B1.BMP$s.e.#
  se2 = B2.BMP$s.e.#
  se = 4 * N * sqrt((se1 ^ 2 + se2 ^ 2) / 4)#
  phat.se = sqrt((B1.BMP$s.e ^ 2 + B2.BMP$s.e. ^ 2) / 4)#
  phat.var = phat.se ^ 2#
  phat.test = 4 * N * (phat - 0.5) / se#
  phat.df = B1.BMP$df + B2.BMP$df#
  phat.pvalue = 2 * (1 - pt(abs(phat.test), phat.df))#
  phat.sig = phat.pvalue < 0.05#
  output = tibble::tibble(#
    N = N,#
    phat = phat,#
    phat.var = phat.var,#
    phat.df = phat.df,#
    phat.test = phat.test,#
    phat.pvalue = phat.pvalue,#
    phat.sig = phat.sig,#
    d = d,#
    vard = vard,#
    d.sig = d.sig,#
    cor = average.tau,#
    sqse = combinedsqse,#
    ctvar = ctvar,#
    n1 = n1,#
    n2 = n2,#
    sigCVt = sigCVt,#
    sigCVn = sigCVn#
  )#
#
  return(output)#
}#
#' @title LaplaceDist#
#' @description Returns a sample of N observations from a Laplace distribution with specified mean and spread.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export LaplaceDist#
#' @param N is the required sample size#
#' @param mean is the required mean#
#' @param spread is the spread of the function#
#' @return N values from a Laplace distribution#
#' @examples#
#' set.seed(123)#
#' LaplaceDist(10,0,1)#
#' # [1] -0.55311564  0.85946218 -0.20094937  1.45258293  2.12808209 -2.39565480  0.05785263...#
LaplaceDist = function(N,#
                       mean,#
                       spread,#
                       max = 0.5,#
                       min = -0.5) {#
  y = stats::runif(N, min, max) # Get data from a uniform distribution#
  x = mean - spread * sign(y) * log(1 - 2 * abs(y))#
#
  return(x)#
}#
# Functions for parametric analysis#
#
#' @title ExtractMAStatistics#
#' @description This function extracts summary statistics from meta-analysis results obtained from the rma function of the metafor R package. If required the function transform back to standardized mean difference (effect size type "d" i.e. Hg) or point biserial correlations (effect size type "r").#
#' Warning: the `ExtractMAStatistics` function works with `metafor` version 2.0-0, but changes to metafor's method of providing access to its individual results may introduce errors into the function.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export ExtractMAStatistics#
#' @param maresults is the output from the rma function.#
#' @param Nc is the number of participants in the control condition group.#
#' @param Nt is the number of participants in the treatment condition group.#
#' @param Transform is a boolean value indicating whether the outcome values need to be transformed back to standardized mean difference ("d" i.e. Hg or d) or point biserial correlations ("r"). It is defaulted to TRUE. If this parameter is set to FALSE, no transformation will be applied.#
#' @param type this indicates the type of transformation required - it defaults to "d" which requests transformation from Zr to Hg, using "r" requests transformation from Zr to r.#
#' @param sig indicates the number of significant digits requested in the output, the default is 4; it rounds the values of mean, pvalue, upper and lower bound to the specified number of significant digits.#
#' @param returnse=FALSE if set to TRUE returns the standard error of the effect size#
#' @return data frame incl. summary statistics from meta-analysis results: overall mean value for the effect sizes, the p-value of the mean, the upper and lower confidence interval bounds (UB and LB), QE which is the heterogeneity test statistic and QEp which the the p-value of the heterogeneity statistic#
#' @examples#
#' ExpData=reproducer::KitchenhamMadeyskiBrereton.ExpData#
#' #Extract the experiment basic statics#
#' S1data=subset(ExpData,ExpData=="S1")#
#' #Use the descriptive data to construct effect size#
#' S1EffectSizes = reproducer::PrepareForMetaAnalysisGtoR(#
#'   S1data$Mc,S1data$Mt,S1data$SDc,S1data$SDt,S1data$Nc,S1data$Nt)#
#' # Do a random effect meta-analysis of the transformed r_pbs effect size#
#' S1MA = metafor::rma(S1EffectSizes$zr, S1EffectSizes$vi)#
#' # Extract summary statistics from meta-analysis results and transform back to Hg scale#
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4)#
#' #     mean   pvalue    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 1.122 0.2384  4 0.41#
#' ExtractMAStatistics(S1MA, sum(S1data$Nc),sum(S1data$Nt), TRUE, "d", 4,TRUE)#
#' #     mean   pvalue     se    UB     LB QE  QEp#
#' # 1 0.6658 0.002069 0.2128 1.122 0.2384  4 0.41#
ExtractMAStatistics = function(maresults,#
                               Nc,#
                               Nt,#
                               Transform = TRUE,#
                               type = "d",#
                               sig = 4,#
                               returnse = FALSE) {#
  pvalue = as.numeric(maresults$pval)#
#
  se = as.numeric(maresults$se)#
#
  QE = as.numeric(maresults$QE)#
  QEp = as.numeric(maresults$QEp)#
  mean = as.numeric(maresults$beta)#
  UB = as.numeric(maresults$ci.ub)#
  LB = as.numeric(maresults$ci.lb)#
#
  if (Transform & type == "d") {#
    mean = reproducer::transformZrtoHg(mean, Nc, Nt)#
    se = reproducer::transformZrtoHg(se, Nc, Nt)#
    UB = reproducer::transformZrtoHg(UB, Nc, Nt)#
    LB = reproducer::transformZrtoHg(LB, Nc, Nt)#
#
  }#
  if (Transform & type == "r") {#
    mean = reproducer::transformZrtoR(mean)#
    se = reproducer::transformZrtoR(se)#
#
    UB = reproducer::transformZrtoR(UB)#
    LB = reproducer::transformZrtoR(LB)#
  }#
  mean = signif(mean, sig)#
  pvalue = signif(pvalue, sig)#
  se = signif(se, sig)#
#
  UB = signif(UB, sig)#
  LB = signif(LB, sig)#
  QE = signif(QE, 2)#
  QEp = signif(QEp, 2)#
  if (returnse)#
    metaanalysisresults = tibble::tibble(mean, pvalue, se, UB, LB, QE, QEp)#
  else#
    metaanalysisresults = tibble::tibble(mean, pvalue, UB, LB, QE, QEp)#
  return(metaanalysisresults)#
#
}#
################################################################################
# Simulation functions#
#' @title simulateRandomizedDesignEffectSizes#
#' @description This simulates one of four distributions, and finds the values of ktau, phat and Cliffs d and their variances. It assumes equal group sizes. It returns values of the effect sizes and their variance for a simulated randomized experiment with two treatments.  It returns whether to not each non-parametric effect size was significant. It also returns the parametric (unstandardized and unstandardized) Effect Size and the whether the t-test was signficiant.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export simulateRandomzedDesignEffectSizes#
#' @param mean The mean used for one of the treatment groups#
#' @param sd The spread used for both treatment groups. It mus be a real value greater than 0.#
#' @param diff This is added to the parameter mean, to define the mean of the other treatment group. It can be a real value avd can take the value zero.#
#' @param N this is the number of observations in each group. It must be an integer greater than 3.#
#' @param type this specifies the underlying distribution used to generate the data. it takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param StdAdj this specifes the extent of variance instability introduced by the treatment.#
#' @return data frame incl. the non-parametric and parametric effect sizes and whether the effect sizes are significant at the 0.05 level.#
#' @examples#
#' set.seed(123)#
#' simulateRandomzedDesignEffectSizes(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)#
#' #   phat    varphat   dfphat sigphat   d       vard  sigd       cor ...#
#' # 1 0.75 0.01522222 17.46405   FALSE 0.5 0.06237576 FALSE 0.2631579 ...#
#' simulateRandomzedDesignEffectSizes(mean=0,sd=1,diff=0.8,N=10,type="l",StdAdj=0)#
#' #   phat     varphat   dfphat sigphat    d       vard sigd       cor      varcor ...#
#' # 1 0.91 0.004644444 13.53727    TRUE 0.82 0.02002909 TRUE 0.4315789 0.006413237#
simulateRandomzedDesignEffectSizes = function(mean,#
                                              sd,#
                                              diff,#
                                              N,#
                                              type = "n",#
                                              StdAdj = 0) {#
  if (type == "n")#
  {#
    x = stats::rnorm(N, mean, sd)#
    y = stats::rnorm(N, mean + diff, sd + StdAdj)#
  }#
  if (type == "g")#
  {#
    rate = mean#
    shape = sd#
    x = stats::rgamma(N, shape, rate)#
    y = stats::rgamma(N, shape + StdAdj, rate + diff)#
  }#
  if (type == "l")#
  {#
    x = stats::rlnorm(N, mean, sd)#
    y = stats::rlnorm(N, mean + diff, sd + StdAdj)#
    transx = log(x)#
    transy = log(y)#
  }#
#
  if (type == "lap")#
  {#
    # Changed to use built-in defaults for max and min#
    x = LaplaceDist(N, mean, sd)#
    y = LaplaceDist(N, mean + diff, sd + StdAdj)#
  }#
#
  # Calculate the Kendall's tau value for the data#
#
  dummy = c(rep(0, N), rep(1, N))#
  xy = c(x, y)#
  # Calculate ktau for the data set#
  ktest = Kendalltaupb(dummy, xy)#
#
  ktau = ktest$cor#
  sigCVt = ktest$cit[1] > 0 | ktest$cit[2] < 0#
#
  # Calculate Cliff's d for the data set and determine whether the value is significant#
  ctest = Cliffd(y, x)#
#
  d = ctest$d#
  dvar = ctest$sqse#
#
  d.lb = ctest$cl#
  d.ub = ctest$cu#
#
  dsig = d.lb > 0 | d.ub < 0#
#
  # Calculate phat statistics for the data set#
#
  ptest = calculatePhat(x, y)#
#
  phat = ptest$phat#
#
  phat.sig = ptest$sig.level < 0.05#
  phat.df = ptest$df#
  phat.var = ptest$s.e. ^ 2#
  # Add the results of a t-test as a baseline#
#
  res = t.test(x, y)#
#
  UES = base::mean(y) - base::mean(x)#
  Var = (stats::var(x) + stats::var(y)) / 2#
  MedDiff = stats::median(y) - stats::median(x)#
  EffectSize = UES / sqrt(Var)#
  pval = res$p.value#
#
  if (type == "l")#
  {#
    # Check that log-normal datra gives appropriate values after transformation#
    trans.t = stats::t.test(transx, transy)#
    pval.trans = trans.t$p.value#
    ES.trans = base::mean(transy) - base::mean(transx)#
    VarTrans = (stats::var(transx) + stats::var(transy)) / 2#
    StdES.trans = ES.trans / sqrt(VarTrans)#
  }#
#
  if (type == "l")#
    output = tibble::tibble(#
      phat = phat,#
      varphat = phat.var,#
      dfphat = phat.df,#
      sigphat = phat.sig,#
      d = d,#
      vard = dvar,#
      sigd = dsig,#
      cor = ktau,#
      varcor = ktest$consistentvar,#
      sigCVt = sigCVt,#
      ttestp = pval,#
      ES = UES,#
      Variance = Var,#
      StdES = EffectSize,#
      MedDiff = MedDiff,#
      transttest = pval.trans,#
      EStrans = ES.trans,#
      StdEStrans = StdES.trans,#
      VarTrans = VarTrans#
    )#
  else#
    output = tibble::tibble(#
      phat = phat,#
      varphat = phat.var,#
      dfphat = phat.df,#
      sigphat = phat.sig,#
      d = d,#
      vard = dvar,#
      sigd = dsig,#
      cor = ktau,#
      varcor = ktest$consistentvar,#
      sigCVt = sigCVt,#
      ttestp = pval,#
      ES = UES,#
      Variance = Var,#
      StdES = EffectSize,#
      MedDiff = MedDiff#
    )#
  return(output)#
}#
#
#############################################################################################################################
##############################################################################################################################
#' @title RandomExperimentSimulations#
#' @description This function performs multiple simulations of two-group balanced experiments for one of four distributions and a specific group size. It identifies the average value of phat, Cliff' d and Kendall's point biserial tau and their variances. It either returns the effect sizes for each non-parametric effect size or it reports the number of times the each non-parametric effect size is assessed to be significantly different from zero. We also present the values for the t-test as a comparison. For log-normal data the results of analysing the transformed data are also reported.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomExperimentSimulations#
#' @param mean The default mean used for both groups (one treatment group and one control group). It can be changed for the treatment group using  the parameter diff#
#' @param sd This is the default spread for both groups. It must be a real value greater than 0. It can be adjusted for the treatment group using the parameter StdAdj#
#' @param diff This is added to the treatment group mean. It can be a real value and can take the value zero.#
#' @param N this is the number of observations in each group. It must be an integer greater than 3.#
#' @param reps this identifies the number of times each experiment simulation is replicated.#
#' @param type this specifies the underlying distribution used to generate the data. It takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param seed This specifies the initial seed for the set of replications (default 123).#
#' @param StdAdj this specifes the extent of variance instability introduced by the treatment and it must be non-negative but can be 0.#
#' @param returnData If TRUE, the function returns the individual effect sizes and their variances, otherwise it returns summary statistics (default FALSE).#
#' @examples#
#' RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)#
#' #      phat     phatvar sigphat emp.phat.var      d       dvar  sigd  emp.d.var#
#' # 1 0.63915 0.007925803    0.33  0.007904962 0.2783 0.03235111 0.306 0.03161985#
#' RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)#
#' RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=10,type="n",seed=123,StdAdj=0,returnData=TRUE)#
RandomExperimentSimulations = function(mean,#
                                       sd,#
                                       diff,#
                                       N,#
                                       reps,#
                                       type = "n",#
                                       seed = 123,#
                                       StdAdj = 0,#
                                       returnData = FALSE) {#
  phatsum = 0 # This is used to sum the value of phat across the replications#
  phatvarsum = 0  # This is used to sum the value of the variance of phat across the replications#
  sig.phat = 0 # This is used to sum the number of times pat is significant across the replications#
  phatsquare = 0 # This  is used to sum phat^2 and construct an empirical variance of phat#
  dsum = 0 # This is used to sum the value of Cliff's d across the replications#
  dvarsum = 0  # This is used to sum the value of the variance of Cliff's d across the replications#
  sig.d = 0 # This is used to sum the number of times d is significant across the replications#
  dsquare = 0 # This  is used to sum d^2 and construct an empirical variance of d.#
#
  ksum = 0 # This is used to sum the value of the point biserial tau across the replications#
  kvarsum = 0 # This is used to sum the value of the variance of the point biserial tau across the replications#
  ksquare = 0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
  ksigCVt = 0#
  tsig = 0 # This is used to count the number of significant t values across the replications#
  ES = 0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
  StdES = 0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
  Var = 0 # This is used to sum the value of the variance across replications#
  ES.l.trans = 0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
  StdES.l.trans = 0 # This is used to sum the transformed standardized effect size for lognormal data sets#
  Var.l.trans = 0 # This is used to sum the transformed variance for lognormal data sets#
  MedDiff = 0 # Used to hold the median difference#
#
  DataTable = NULL#
#
  base::set.seed(seed)#
  for (i in 1:reps) {#
    # Call the program that generates the random data sets and calculates the sample statistics.#
    res = simulateRandomzedDesignEffectSizes(mean, sd, diff, N, type, StdAdj)#
#
    if (returnData == FALSE) {#
      # Aggregate data to provide counts of significance and overall effect size averages#
      # Cliff's d#
      dsum = dsum + res$d#
      dvarsum = dvarsum + res$vard#
      if (res$sigd)#
        sig.d = sig.d + 1#
      dsquare = dsquare + res$d ^ 2#
#
      # Probability of superiority#
      phatsum = phatsum + res$phat#
      phatvarsum = phatvarsum + res$varphat#
      if (res$sigphat)#
        sig.phat = sig.phat + 1#
      phatsquare = phatsquare + res$phat ^ 2#
#
      # Point biserial Kendall's tau#
      ksum = ksum + res$cor#
      kvarsum = kvarsum + res$varcor#
      ksquare = ksquare + res$cor ^ 2#
      ksigCVt = ksigCVt + if (res$sigCVt)#
        1#
      else#
        0#
#
      # Parametric statistics#
      ES = ES + res$ES#
      StdES = StdES + res$StdES#
      MedDiff = MedDiff + res$MedDiff#
      tsig = tsig + if (res$ttestp < .05)#
        1#
      else#
        0#
      Var = Var + res$Variance#
#
      if (type == "l") {#
        ES.l.trans = ES.l.trans + res$EStrans#
        StdES.l.trans = StdES.l.trans + res$StdEStrans#
        Var.l.trans = Var.l.trans + res$VarTrans#
      }#
    }#
    else {#
      # Store the outcome from each replication#
      DataTable = tibble::tibble(base::rbind(#
        DataTable,#
        base::cbind(#
          Cliffd = res$d,#
          PHat = res$phat,#
          StdES = res$StdES#
        )#
      ))#
    }#
#
  }#
#
  if (returnData == FALSE) {#
    #Calculate averages of the statistics across the replications#
    d = dsum / reps#
    dvar = dvarsum / (reps)#
    sigd = sig.d / (reps)#
    emp.d.var = (dsquare - reps * d ^ 2) / (reps - 1)#
#
    phat = phatsum / reps#
    phatvar = phatvarsum / (reps)#
    sigphat = sig.phat / (reps)#
    emp.phat.var = (phatsquare - reps * phat ^ 2) / (reps - 1)#
    ktau = ksum / reps#
    ktauvar = kvarsum / reps#
    emp.tau.var = (ksquare - reps * ktau ^ 2) / (reps - 1)#
#
    kpowerCVt = ksigCVt / reps#
#
    ES = ES / reps#
    StdES = StdES / reps#
    MedDiff = MedDiff / reps#
    Variance = Var / reps#
    tpower = tsig / reps#
#
    if (type == "l")#
    {#
      # This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
      ESLog = ES.l.trans / reps#
      StdESLog = StdES.l.trans / reps#
      VarLog = Var.l.trans / reps#
    }#
#
    if (type == "l")	{#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff,#
        ESLog = ESLog,#
        StdESLog = StdESLog,#
        VarLog = VarLog#
      )#
    }#
    else {#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff#
      )#
    }#
  }#
  else {#
    outcome = DataTable#
  }#
  return(outcome)#
#
}#
#' @title simulateRandomizedBlockDesignEffectSizes#
#' @description This simulates one of four distributions, and finds the values of ktau and Cliffs d and their variances. It simulates a randomised blocks experiment with two treatment groups and two control groups each of which being divided into two blocks. By default it assumes equal group sizes but  group spread (standard deviation can be changed, see Stadj). It returns values of both parametric and non=parametric effect sizes and their variance for simulated experiments. It returns the number of times each effect size was signficiant. For the logarithmic distribution it calcates effect sizes based on the log transformed data as well as the raw data.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export simulateRandomizedBlockDesignEffectSizes#
#' @param mean The default value for all groups which can be changed for the two treatment groups using the parameter diff and for the two block 2 groups using the parameter Blockmean#
#' @param sd The default spread used for all four groups unless adjusted by the StdAdj. It must be a real value greater than 0.#
#' @param diff This is added to the parameter mean to obtain the required mean for treatment groups. It can be a real value and can take the value zero.#
#' @param N this is the number of observations in each group. It must be an integer greater than 3.#
#' @param type this specifies the underlying distribution used to generate the data. it takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param Blockmean if >0 an adjusment made to both group means in Block 2#
#' @param BlockStdAdj if >0, an adjustment that can be made to the sd of each group in block 2#
#' @param StdAdj this specifes the extent of variance instability introduced by the treatment and if >0 will be used to amend the sd parameter for both treatment groups.#
#' @return data frame incl. the non-parametric and parametric effect sizes and whether the effect sizes are significant at the 0.05 level.#
#' @examples#
#' set.seed(123)#
#' simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="n",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)#
#' # N phat    phat.var  phat.df phat.test  phat.pvalue phat.sig    d       vard d.sig       cor       sqse       ctvar n1 n2 sigCVt sigCVn ttest.sig#
#' # 1 40 0.79 0.005866667 30.15715  3.786189 0.0006806094     TRUE 0.58 0.02430788  TRUE 0.3052632 0.01315789 0.006953352 20 20   TRUE   TRUE      TRUE#
#' #    ES  Variance   StdES BlockEffect MedianDiff#
#' # 1 0.9402999 0.7829385 1.06268    0.307119   1.313642#
#' set.seed(123)#
#' simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="l",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)#
#' #    N phat    phat.var  phat.df phat.test  phat.pvalue phat.sig    d       vard d.sig       cor       sqse       ctvar n1 n2 sigCVt sigCVn ttest.sig#
#' # 1 40 0.79 0.005866667 30.15715  3.786189 0.0006806094     TRUE 0.58 0.02430788  TRUE 0.3052632 0.01315789 0.006953352 20 20   TRUE   TRUE      TRUE#
#' # ES Variance     StdES BlockEffect MedianDiff Log.sig  ES.Trans StdES.Trans  VarTrans#
#' # 1 1.953912 4.630789 0.9079823   0.5684415   2.543203    TRUE 0.9402999     1.06268 0.7829385#
simulateRandomizedBlockDesignEffectSizes = function(mean,#
                                                    sd,#
                                                    diff,#
                                                    N,#
                                                    type = "n",#
                                                    alpha = 0.05,#
                                                    Blockmean = 0,#
                                                    BlockStdadj = 0,#
                                                    StdAdj = 0) {#
  # Generate data. x and x2 hold control data, y and y2 to hold treatment data.  x and y are ib block 1 and x2 and y2 are in block 2#
#
  if (type == "n")#
  {#
    x = stats::rnorm(N, mean, sd)#
    y = stats::rnorm(N, mean + diff, sd + StdAdj)#
    x2 = stats::rnorm(N, mean + Blockmean, sd + BlockStdadj)#
    y2 = stats::rnorm(N, mean + diff + Blockmean, sd + BlockStdadj + StdAdj)#
#
  }#
  if (type == "g")#
  {#
    shape = sd#
    rate = mean#
    # For a Gamma distribution shapeand rate must always be greater than zero#
#
    if (BlockStdadj == 0)#
      BlockStdadj = 1#
#
    x = stats::rgamma(N, shape, rate)#
    y = stats::rgamma(N, shape + StdAdj, rate + diff)#
    x2 = stats::rgamma(N, shape * BlockStdadj + Blockmean, rate)#
    y2 = stats::rgamma(N, shape * BlockStdadj + StdAdj + Blockmean, rate + diff)#
  }#
  if (type == "l")#
  {#
    x = stats::rlnorm(N, mean, sd)#
    y = stats::rlnorm(N, mean + diff, sd + StdAdj)#
    x2 = stats::rlnorm(N, mean + Blockmean, sd + BlockStdadj)#
    y2 = stats::rlnorm(N, mean + diff + Blockmean, sd + BlockStdadj + StdAdj)#
  }#
#
  if (type == "lap")#
  {#
    x = LaplaceDist(N, mean, sd)#
    y = LaplaceDist(N, mean + diff, sd + StdAdj)#
    x2 = LaplaceDist(N, mean + Blockmean, sd + BlockStdadj)#
    y2 = LaplaceDist(N, mean + diff + Blockmean, sd + BlockStdadj + StdAdj)#
#
  }#
#
  res = Calc4GroupNPStats(y, x, y2, x2, alpha = alpha)#
#
  res = tibble::as_tibble(res)#
#
  # Add the results of a t-test as a baseline#
  UES = (base::mean(y) + base::mean(y2) - base::mean(x) - base::mean(x2)) /#
    2#
#
  BlockEffect = (base::mean(x2) + base::mean(y2) - base::mean(x) - base::mean(y)) /#
    2#
#
  MedDiff = (stats::median(y) + stats::median(y2) - stats::median(x) - stats::median(x2)) /#
    2#
  Var = (stats::var(y) + stats::var(y2) + stats::var(x) + stats::var(x2)) /#
    4#
  # Estimate of Cohen's d#
  StdES = UES / sqrt(Var)#
  # Need to use the linear contrast method for the significance test of randomized blocks data that allows for variance heterogeneity.#
  newlist = list()#
  newlist[[1]] = x#
  newlist[[2]] = y#
  newlist[[3]] = x2#
  newlist[[4]] = y2#
  vec = c(-1, 1, -1, 1) / 2#
  res.t = RandomizedBlocksAnalysis(newlist, con = vec, alpha = 0.05)#
  df.ttest = base::data.frame(res.t$psihat)#
  pval = df.ttest$p.value#
  ttest.sig = pval < 0.05#
#
  StandardMetrics = list(#
    ttest.sig = ttest.sig,#
    ES = UES,#
    Variance = Var,#
    StdES = StdES,#
    BlockEffect = BlockEffect,#
    MedianDiff = MedDiff#
  )#
  StandardMetrics = tibble::as_tibble(StandardMetrics)#
#
  if (type == "l")#
  {#
    loglist = list()#
#
    loglist[[1]] = log(x)#
    loglist[[2]] = log(y)#
    loglist[[3]] = log(x2)#
    loglist[[4]] = log(y2)#
#
    vec = c(-1, 1, -1, 1) / 2#
    logres.t = RandomizedBlocksAnalysis(loglist, con = vec, alpha = 0.05)#
    df.logttest = base::data.frame(logres.t$psihat)#
    pval.log = df.logttest$p.value#
    Log.sig = pval.log < 0.05#
    ES.trans = (base::mean(log(y)) + base::mean(log(y2)) - base::mean(log(x)) -#
                  base::mean(log(x2))) / 2#
#
    VarTrans = (stats::var(log(x)) + stats::var(log(x2)) + stats::var(log(y)) +#
                  stats::var(log(y2))) / 4#
    StdES.trans = ES.trans / sqrt(VarTrans)#
#
    AdditionalMetrics = list(#
      Log.sig = Log.sig,#
      ES.Trans = ES.trans,#
      StdES.Trans = StdES.trans,#
      VarTrans = VarTrans#
    )#
    AdditionalMetrics = tibble::as_tibble(AdditionalMetrics)#
    StandardMetrics = dplyr::bind_cols(StandardMetrics, AdditionalMetrics)#
#
  }#
  res = dplyr::bind_cols(res, StandardMetrics)#
#
  return(res)#
#
}#
#
# Example#
set.seed(123)#
simulateRandomizedBlockDesignEffectSizes(#
  mean = 0,#
  sd = 1,#
  diff = .5,#
  N = 10,#
  type = "n",#
  alpha = 0.05,#
  Blockmean = 0.5,#
  BlockStdadj = 0,#
  StdAdj = 0#
)#
set.seed(123)#
simulateRandomizedBlockDesignEffectSizes(#
  mean = 0,#
  sd = 1,#
  diff = .5,#
  N = 10,#
  type = "l",#
  alpha = 0.05,#
  Blockmean = 0.5,#
  BlockStdadj = 0,#
  StdAdj = 0#
)#
#
#' title RandomizedBlocksExperimentSimulations#
#' description This function performs multiple simulations of 4 group balanced randomised Block experiments with two control groups and two treatment groups where one control group and one treatment group are assigned to block 1 and the other control group and treatment group are assigned to block 2.  The simulations are based on one of four distributions and a specific group size. The function identifies the average value of the non-paramtric effect sizes P-hat, Cliff' d and Kendall's point biserial tau and their variances and whether ot not the statistics were significant at the 0.05 level. We also present the values of the t-test as a comparison.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedBlocksExperimentSimulations#
#' @param mean The default mean for all 4 groups. The default for the two treatment groups can be altered using the parameter diff and the block mean for block 2 can be altered using the parameter Blockmean.#
#' @param sd The default spread for all 4 groups. It must be a real value greater than 0. If can be altered for treatment groups using the parameter StdAdj and for Block 2 groups using BlockStdAdj#
#' @param diff The is is added to the parameter mean, to define the mean of the other treatment group. It can be a real value ad can take the value zero.#
#' @param N this is the number of observations in each group. It must be an interger greater than 3.#
#' @param reps this identifies the number of tiume the simulation is replicated.#
#' @param type this specifies the underlying distribution used to generate the data. it takes the values "n" for a normal distribution, "l" for lognormal distribution,"g" for a gamma distribution, "lap" for a Laplace distribution.#
#' @param alpha is the Type 1 error level used for constructing confidence intervals#
#' @param Blockmean is the effect of having two different blocks#
#' @param BlockStdAdj is the variance associated with the Block mean. If Blockvar is zero it means we are treat the block effect as a fixed effect. If BlockStdAdj>0, we treat the block effect as a random effect.#
#' @param StdAdj The value used to introduce heterogeneity into the treatment groups variance if required.#
#' @param seed this specifies the seed value for the simulations and allows the experiment to be repeated.#
#' @param returnData=FALSE if TRUE the function returns the generated data otherwise it returns summary statistics.#
#' @return depending on the parameter returnData it returns the generated nonparametric and parametric values or the summary statistics#
#' examples#
#' RandomizedBlocksExperimentSimulations(mean=0,sd=1,diff=0.5,N=10, reps=500,type="n",alpha=0.05,Blockmean=0.5, BlockStdadj=0,StdAdj=0,seed=123)#
#'  # phat     varphat sigphat emp.phat.var       d       vard  sigd  emp.d.var      ktau kconsistentvar emp.tau.var kpowerCVt     StdES        ES#
#'  # 1 0.63967 0.008322856   0.326  0.007728698 0.27934 0.03430328 0.282 0.03091479 0.1470211    0.009562829 0.008563655     0.168 0.5130732 0.5029075#
#'  #        Var emp.StdESvar   MedDiff tpower#
#'  # 1 1.001602    0.1116687 0.5110203  0.334#
#' RandomizedBlocksExperimentSimulations(mean=0,sd=1,diff=0.5,N=10, reps=10,type="n",alpha=0.05,Blockmean=0.5, BlockStdadj=0,StdAdj=0,seed=123,returnData=T)#
#'  #    Cliffd  PHat     StdES#
#'  # 1    0.58 0.790 1.0626801#
#'  # 2    0.21 0.605 0.3832224#
#'  # 3    0.37 0.685 0.7605261#
#'  # 4    0.44 0.720 0.8208361#
#'  # 5    0.13 0.565 0.2395682#
#'  # 6    0.16 0.580 0.2221613#
#'  # 7    0.38 0.690 0.5803148#
#'  # 8    0.48 0.740 0.8823308#
#'  # 9    0.11 0.555 0.1811224#
#'  # 10  -0.03 0.485 0.1243767#
#
RandomizedBlocksExperimentSimulations = function(mean,#
                                                 sd,#
                                                 diff,#
                                                 N,#
                                                 reps,#
                                                 type = "n",#
                                                 alpha = 0.05,#
                                                 Blockmean = 0,#
                                                 BlockStdadj = 0,#
                                                 StdAdj = 0,#
                                                 seed = 123,#
                                                 returnData = FALSE) {#
  ksum = 0 # This is used to sum the value of the point biserial tau across the replications#
  k.ss = 0 # This is used to sum the square of the point biserial tau across the replications. It can be used to calculate the empirical variance of the average tau.#
#
  kconsistentvar = 0 # This is used to sum the variance BiSerial Kendall's tau  values  across the replications#
  ksigCVt = 0  # This is used to sum the number of significant Point BiSerial Kendall's tau  values  across the replications#
  dsum = 0 # This is used to sum the value of Cliff's d across the replications#
  dvarsum = 0 # This is used to aggregate the variance of d across the replications#
  d.sig = 0 # This is used to sum the number of significant d values  across the replications#
  d.ss = 0 # This is used to sum the squared value of Cliff's d across the replications. It can be used to calculate the empirical variance of the average d.#
#
  phatsum = 0 # This is used to sum the value of phat across the replications#
  phatvarsum = 0 # This is used to aggregate the variance of phat across the replications#
  phat.sig = 0 # This is used to sum the number of significant phat values  across the replications#
  phat.ss = 0  # This is used to sum the squared value of phat across the replications. It can be used to calculate the empirical variance of the average phat.#
  tsig = 0 # This is used to count the number of significant t values across the replications#
  ES = 0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
  StdES = 0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
  StdES.ss = 0 # This is used to sum the squared value of the parametric effect size (standardized) across the replications. It can be used to calculate the empirical variance of the overall mean value#
#
  Var = 0 # This is used to sum the estimate of the pooled variance across replications#
  MedDiff = 0 # This is used to sum the median across replications#
#
  ES.l.trans = 0 # This is used to sum the unstandardized effect size of the log-normal data after being transformed#
  StdES.l.trans = 0 # This is used to sum the standardized effect size of the log-normal data after being transformed#
  Var.l.trans = 0 # This is used to sum the variance of the log-normal data after being transformed#
#
  trans.sig = 0  # This is used to sum the number of signficant t-tests of the transformed lognormal data#
#
  base::set.seed(seed)#
#
  DataTable = NULL#
#
  for (i in 1:reps) {#
    # Call the program than generates the randomized block experiment data sets and calculates the sample statistics#
    res = simulateRandomizedBlockDesignEffectSizes(#
      mean,#
      sd,#
      diff,#
      N,#
      type,#
      alpha = alpha,#
      Blockmean = Blockmean,#
      BlockStdadj = BlockStdadj,#
      StdAdj = StdAdj#
    )#
    if (!returnData) {#
      # Cliff's d#
      dsum = dsum + res$d#
      dvarsum = dvarsum + res$vard#
      if (res$d.sig)#
        d.sig = d.sig + 1#
      d.ss = d.ss + res$d ^ 2#
#
      # Probability of Speriority (phat)#
      phatsum = phatsum + res$phat#
      if (res$phat.sig)#
        phat.sig = phat.sig + 1#
      phat.ss = phat.ss + res$phat ^ 2#
      phatvarsum = phatvarsum + res$phat.var#
#
      # Point Biserial Kendall's tau#
      ksum = ksum + res$cor#
      k.ss = k.ss + res$cor ^ 2#
      kconsistentvar = kconsistentvar + res$ctvar#
      ksigCVt = ksigCVt + if (res$sigCVt)#
        1#
      else#
        0#
#
      # Standard parametric effect sizes#
      ES = ES + res$ES#
      StdES = StdES + res$StdES#
      StdES.ss = StdES.ss + res$StdES ^ 2#
      Var = Var + res$Variance#
      MedDiff = MedDiff + res$MedianDiff#
      tsig = tsig + if (res$ttest.sig)#
        1#
      else#
        0#
#
      if (type == "l")#
      {#
        trans.sig = trans.sig + if (res$Log.sig)#
          1#
        else#
          0#
#
        ES.l.trans = ES.l.trans + res$ES.Trans#
        StdES.l.trans = StdES.l.trans + res$StdES.Trans#
        Var.l.trans = Var.l.trans + res$VarTrans#
      }#
#
    }#
    else {#
      # Store the outcome from each replication#
      DataTable = tibble::tibble(dplyr::bind_rows(#
        DataTable,#
        #base::cbind(#
        dplyr::bind_cols(#
          Cliffd = res$d,#
          PHat = res$phat,#
          StdES = res$StdES#
        )#
      ))#
    }#
#
  }#
  if (!returnData) {#
    #Calculate averages.#
    d = dsum / reps#
    vard = dvarsum / reps#
    sigd = d.sig / reps#
    emp.d.var = (d.ss - reps * d ^ 2) / (reps - 1)#
#
    phat = phatsum / reps#
    varphat = phatvarsum / reps#
    sigphat = phat.sig / reps#
    emp.phat.var = (phat.ss - reps * phat ^ 2) / (reps - 1)#
#
    ktau = ksum / reps#
    emp.tau.var = (k.ss - reps * ktau ^ 2) / (reps - 1)#
    kconsistentvar = kconsistentvar / reps#
    kpowerCVt = ksigCVt / reps#
#
    ES = ES / reps#
    StdES = StdES / reps#
    emp.StdESvar = (StdES.ss - reps * StdES ^ 2) / (reps - 1)#
    Var = Var / reps#
    tpower = tsig / reps#
    MedDiff = MedDiff / reps#
#
    if (type == "l")#
    {#
      ESLog = ES.l.trans / reps#
      StdESLog = StdES.l.trans / reps#
      VarLog = Var.l.trans / reps#
      Log.sig = trans.sig / reps#
    }#
  }#
  if (!returnData) {#
    if (type == "l")#
      outcome = tibble::tibble(#
        phat,#
        varphat,#
        sigphat,#
        emp.phat.var,#
        d,#
        vard,#
        sigd,#
        emp.d.var,#
        ktau,#
        kconsistentvar,#
        emp.tau.var,#
        kpowerCVt,#
        StdES,#
        ES,#
        Var,#
        emp.StdESvar,#
        MedDiff,#
        tpower,#
        ESLog,#
        StdESLog,#
        VarLog,#
        Log.sig#
      )#
#
    else#
      outcome = tibble::tibble(#
        phat,#
        varphat,#
        sigphat,#
        emp.phat.var,#
        d,#
        vard,#
        sigd,#
        emp.d.var,#
        ktau,#
        kconsistentvar,#
        emp.tau.var,#
        kpowerCVt,#
        StdES,#
        ES,#
        Var,#
        emp.StdESvar,#
        MedDiff,#
        tpower#
      )#
  }#
  else {#
    outcome = DataTable#
  }#
#
  return(outcome)#
#
}#
#
#Example#
# RandomizedBlocksExperimentSimulations(#
#     mean = 0,#
#     sd = 1,#
#     diff = 0.5,#
#     N = 10,#
#     reps = 500,#
#     type = "n",#
#     alpha = 0.05,#
#     Blockmean = 0.5,#
#     BlockStdadj = 0,#
#     StdAdj = 0,#
#     seed = 123#
# )#
#
###############################################################
#' @title NP4GroupMetaAnalysisSimulation#
#' @description This function simulates data from a family of experiments, where the number of experiments in a family is defined by ther parameter Exp. It simulates data from one of four distributions and uses the data to construct four of groups of equal size (GroupSize). Two groups are assigned as control groups and their distribution is based on the parameter mean and the parameter spread, however, the mean and spread for the control group in Block 2 can be adjusted using the parameters BlockEffect and BlockStdadj respectively. The other two groups are treatment groups and their distribution is based on the mean+diff and the spread parameter, but the distributions can be adjusted using the StdAdj, BlockEffect and BlockStdadj parameters. The data from each experiment is analysed separately to estimate the non-parametric statistics P-hat, Cliff's d and Kendall's tau and their variances. The statistics are then meta-analysed using the method specified by the MAMethod parameter. We output the average non-pa
rametric effect statistics across the Exp experimet as if from a single large experiment and also the results of meta-analysising each non-parametric effect size. We use the standard parametric effect sizes and their meta-analysis as baselines. All tests of significance are done at the 0.05 level.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export NP4GroupMetaAnalysisSimulation#
#' @param mean The default value used for the group means in the simulated data. It can be any real number including zero.#
#' @param sd The default value used for the spread of the control group and the spread of the treatment group in the simulated data. The value must be a real value greater than 0.#
#' @param diff mean+diff is the value used for the mean of the treatment group. It can be zero.#
#' @param Groupsize is the size of each of the 4 groups comprising one experiment. Groupsize should be an integer of 4 or more#
#' @param Exp is the number of experiments being simulated. Exp should be an integer of 2 or more. It defaults to 5.#
#' @param type specifies the distribution being simulated. The permitted values are "n" for the normal distribution,  "l" for the lognormal distribution, "g" for the gamma distribution and "lap" for the Laplace dsitribution. The parameter defaults to "n".#
#' @param seed specifies the seed to be used to initiate the simulation, so the simulation is repeatable. It defaults to 123.#
#' @param BlockEffect is the effect of having two different blocks#
#' @param BlockStdadj is the variance associated with the Block. If BlockStdadj is zero it means we are treat the block effect as a fixed effect. If BlockStdAdj>0, we treat the block effect as a random effect and increase the variance of Block 2 data.#
#' @param Stdadj The value used to introduce heterogeneity into the treatment groups variance if required.#
#' @param MAMethod defines the method used for meta-analysis#
#' @param returnES=FALSE#
#' @return If returnES is true, the function returns the summary meta-analysis summary statistics otherwise the function returns the effect sizes for each experiment#
#' @examples#
#' NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM")#
#' #    NumExp GroupSize   AveKtau AveKtauctvar tauSigCVt AveCliffd AveCliffdvar AveCliffdsig Avephat  Avephatvar Avephatsig#
#' #  1      5        10 0.1821053  0.001879139      TRUE     0.346  0.006728242         TRUE   0.673 0.001634444       TRUE#
#' #       MAMean       MAvar MASig       QE       QEp HetSig P.mean P.rsig P.hetsig Mean.phat phat.sig    Mean.d d.sig#
#' #  1 0.1948315 0.001858502  TRUE 4.054212 0.3987188  FALSE 0.3078   TRUE    FALSE 0.6856154     TRUE 0.3717783  TRUE#
#' #    Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Cohend.mean Cohend.sig#
#' #  1    0.6321739        TRUE     0.6211023         TRUE   0.6366072       TRUE#
#' ##
#' NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.724,GroupSize=10,Exp=5,type="l",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM")#
#' #    NumExp GroupSize   AveKtau AveKtauctvar tauSigCVt AveCliffd AveCliffdvar AveCliffdsig Avephat  Avephatvar Avephatsig#
#' #  1      5        10 0.2442105  0.001669217      TRUE     0.464  0.005928404         TRUE   0.732 0.001435556       TRUE#
#' #       MAMean       MAvar MASig       QE       QEp HetSig P.mean P.rsig P.hetsig Mean.phat phat.sig    Mean.d d.sig#
#' #  1 0.2571384 0.001610633  TRUE 3.184021 0.5275157  FALSE 0.3103   TRUE    FALSE 0.7455811     TRUE 0.4908146  TRUE#
#' #    Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Cohend.mean Cohend.sig#
#' #  1    0.6274616        TRUE     0.6160074         TRUE   0.6369005       TRUE#
#' ##
#' NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM",returnES=TRUE)#
#' #      MeanExp    VarExp  StdESExp       df      tval       tpval        tciL      tciU Cliffd  Cliffdvar  PHat     PHatvar#
#' #  1 0.9402999 0.7829385 1.0626801 31.33793 3.3604896 0.002059758  0.36987215 1.5107276   0.58 0.02430788 0.290 0.005866667#
#' #  2 0.3722000 0.9433023 0.3832224 34.98802 1.2118558 0.233688387 -0.25131927 0.9957193   0.21 0.03802545 0.105 0.009272222#
#' #  3 0.5984669 0.6192301 0.7605261 28.59165 2.4049948 0.022879092  0.08920925 1.1077246   0.37 0.03360646 0.185 0.008127778#
#' #  4 0.8727956 1.1306083 0.8208361 28.09724 2.5957117 0.014843698  0.18413592 1.5614553   0.44 0.03325374 0.220 0.008133333#
#' #  5 0.2428255 1.0273784 0.2395682 31.50788 0.7575811 0.454330716 -0.41046766 0.8961187   0.13 0.03901253 0.065 0.009461111#
#' #      PHatdf         g gvar.approx Cohendvar#
#' #  1 30.15715 1.0370082  0.11238473 0.1180179#
#' #  2 31.34739 0.3749386  0.09773244 0.1020987#
#' #  3 30.76837 0.7403707  0.10435566 0.1101148#
#' #  4 23.82482 0.7986955  0.10603002 0.1119900#
#' #  5 32.83135 0.2338123  0.09612003 0.1009108#
NP4GroupMetaAnalysisSimulation = function(mean,#
                                          sd,#
                                          diff,#
                                          GroupSize,#
                                          Exp = 5,#
                                          type = "n",#
                                          alpha = 0.05,#
                                          seed = 123,#
                                          StdAdj = 0,#
                                          BlockEffect = 0,#
                                          BlockStdadj = 0,#
                                          StdExp = 0,#
                                          MAMethod,#
                                          returnES = FALSE) {#
  N = GroupSize#
#
  set.seed(seed)#
  ES.cor = rep(NA, Exp) # Used to hold Kendall's tau for each experiment#
  ES.ctvar = rep(NA, Exp) # Used to hold the consistent variance of Kendall's tau for each experiment#
  ES.d = rep(NA, Exp) # Used to hold Cliff's d for each experiment#
  ES.vard = rep(NA, Exp) # Used to hold the variance Cliff's d for each experiment#
  ES.phat = rep(NA, Exp)  # Used to hold phat for each experiment#
  ES.phatvar = rep(NA, Exp) # Used to hold the variance phat for each experiment#
  ES.phat.df = rep(NA, Exp) # Used to hold the degrees of freedom for the test statistics for each experiment#
#
  g = rep(NA, Exp) # Used to hold the small sample size adjusted standardized effect size#
  varg.approx = rep(NA, Exp) # Used to hold the approximate variance of the small sample size adjusted standardized effect size#
  varg.exact = rep(NA, Exp) # Used to hold the exact variance of the small sample size adjusted standardized effect size#
  P.cor = rep(NA, Exp) # This holds the estimate of the point bi-serial correlation effect size#
  P.corvar = rep(NA, Exp) # This holds the variance of normal transformation of the correlation effect size#
  c = rep(NA, Exp) # This holds the small sample size adjustment factor for each experiment#
  df = rep(NA, Exp) # This holds the degrees of freedom of the t test of the unstandardized effect size for each for each experiment#
#
  Cohen.d = rep(NA, Exp) # This holds standardized mean difference effect size of each experiment#
  varCohend.approx = rep(NA, Exp)  # This holds the variance of the standardized mean difference effect size of each experiment#
  tval = rep(NA, Exp) # This holds t test value for each experiment#
  tpval = rep(NA, Exp) # This holds t test p-value for each experiment#
  tciL = rep(NA, Exp) # This holds lower bound for the effect size#
  tciU = rep(NA, Exp) # This holds upper bound for the effect size#
  UES = rep(NA, Exp) # This holds the unstandardized effect size#
  Var = rep(NA, Exp) # This holds the pooled within group variance#
  # Generate 4-group experiments for a family of experiments (the number in the family defined by the "Exp" parameter), with the underlying distribution defined by the "type" parameter#
  for (i in 1:Exp) {#
    if (type == "n") {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = stats::rnorm(N, mean + ExpAdj, sd)#
      y = stats::rnorm(N, mean + ExpAdj + diff, sd + StdAdj)#
      x2 = stats::rnorm(N, mean + ExpAdj + BlockEffect, sd + BlockStdadj)#
      y2 = stats::rnorm(N,#
                        mean + ExpAdj + diff + BlockEffect,#
                        sd + BlockStdadj + StdAdj)#
    }#
    if (type == "g")#
    {#
      shape = sd#
      rate = mean#
#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = abs(rnorm(1, 0, StdExp))#
      if (BlockStdadj == 0)#
        BlockStdadj = 1#
#
      x = stats::rgamma(N, shape, rate + ExpAdj)#
      y = stats::rgamma(N, shape + StdAdj, rate + ExpAdj + diff,)#
      x2 = stats::rgamma(N, shape * BlockStdadj + BlockEffect, rate + ExpAdj)#
      y2 = stats::rgamma(N,#
                         shape * BlockStdadj + StdAdj + BlockEffect,#
                         rate + ExpAdj + diff)#
    }#
    if (type == "l")#
    {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = stats::rlnorm(N, mean + ExpAdj, sd)#
      y = stats::rlnorm(N, mean + ExpAdj + diff, sd + StdAdj)#
      x2 = stats::rlnorm(N, mean + ExpAdj + BlockEffect, sd + BlockStdadj)#
      y2 = stats::rlnorm(N,#
                         mean + ExpAdj + diff + BlockEffect,#
                         sd + BlockStdadj + StdAdj)#
    }#
#
    if (type == "lap")#
      # Changed to use built-in defaults for max and min#
    {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = LaplaceDist(N, mean + ExpAdj, sd + StdAdj)#
      y = LaplaceDist(N, mean + diff + ExpAdj, sd)#
      x2 = LaplaceDist(N, mean + ExpAdj + BlockEffect, sd + BlockStdadj)#
      y2 = LaplaceDist(N,#
                       mean + ExpAdj + diff + BlockEffect,#
                       sd + BlockStdadj + StdAdj)#
    }#
    # The data in each experiment is analysed to obtain the values of three non-parametric effect size: Kendall's tau, its consistent variance, Cliff's d with its consistent variance and phat with its variance plus t-test results based on the Welch method applied to a randomized blocks experiment.#
#
    NPStats = NULL#
#
    # Calc4GroupNPStats finds the average value of various non-parametric statistics for a randomised block experimental design#
    NPStats = Calc4GroupNPStats(y, x, y2, x2)#
    ES.cor[i] = as.numeric(NPStats$cor)#
    ES.ctvar[i] = NPStats$ctvar#
    ES.d[i] = NPStats$d#
    ES.vard[i] = NPStats$vard#
    ES.phat[i] = NPStats$phat - 0.5 #metafor tests difference from zero but zero effect for phat is 0.5#
    ES.phatvar[i] = NPStats$phat.var#
    ES.phat.df[i] = NPStats$phat.df # Holds the degrees of freedom#
#
    # Do a parametric analysis of the data#
    UES[i] = (base::mean(y) + base::mean(y2) - base::mean(x) - base::mean(x2)) /#
      2#
    Var[i] = (stats::var(y) + stats::var(y2) + stats::var(x) + stats::var(x2)) /#
      4#
#
    newlist = list()#
    newlist[[1]] = x#
    newlist[[2]] = y#
    newlist[[3]] = x2#
    newlist[[4]] = y2#
    vec = c(-1, 1, -1, 1) / 2#
#
    # RandomizedBlocksAnalysis does a Welch-based linear contrast analysis#
    res.t = RandomizedBlocksAnalysis(newlist, con = vec, alpha = 0.05)#
#
    ttest.ESdets = base::data.frame(res.t$psihat)#
#
    # Cohen's d is the standardized mean difference#
    Cohen.d[i] = UES[i] / sqrt(Var[i])#
#
    # g is 	small sample size adjusted mean difference#
#
    tdets = base::as.data.frame(res.t$test)#
    df[i] = as.numeric(tdets$df)#
    c[i] = reproducer::calculateSmallSampleSizeAdjustment(df[i])#
    g[i] = Cohen.d[i] * c[i]#
    tval[i] = tdets$test#
    tpval[i] = ttest.ESdets$p.value#
    tciL[i] = ttest.ESdets$ci.lower#
    tciU[i] = ttest.ESdets$ci.upper#
    varg.approx[i] = c[i] ^ 2 / N + g[i] ^ 2 / (2 * df[i])#
#
    varCohend.approx[i] = 1 / N + Cohen.d[i] ^ 2 / (2 * df[i])#
#
    # Calculate r from d#
#
    P.cor[i] = Cohen.d[i] / sqrt(Cohen.d[i] ^ 2 + 4)#
    P.cor[i] = reproducer::transformRtoZr(P.cor[i])#
    P.corvar[i] = 1 / (4 * N - 3)#
#
  }#
  if (!returnES)#
  {#
    NumExp = Exp#
    # Analyse the data from the Exp experiments as a single distributed experiment#
    AveKtau = base::mean(ES.cor)#
    AveKtauctvar = sum(ES.ctvar) / NumExp ^ 2#
    # Use t-based bounds#
    vv = stats::qt(alpha / 2, (4 * N - 3) * NumExp)#
    LowerBoundt = AveKtau + vv * sqrt(AveKtauctvar)#
    UpperBoundt = AveKtau - vv * sqrt(AveKtauctvar)#
#
    tauSigCVt = UpperBoundt < 0 | LowerBoundt > 0#
    AveCliffd = base::mean(ES.d)#
    AveCliffdvar = sum(ES.vard) / NumExp ^ 2#
    # The degrees of freedom per experiment is the number of participants less 3.#
    vv = stats::qt(alpha / 2, (4 * N - 3) * NumExp)#
    LowerBoundd = AveCliffd + vv * sqrt(AveCliffdvar)#
    UpperBoundd = AveCliffd - vv * sqrt(AveCliffdvar)#
    AveCliffdsig = UpperBoundd < 0 | LowerBoundd > 0#
#
    Avephat = base::mean(ES.phat)#
    Avephatvar = sum(ES.phatvar) / NumExp ^ 2#
    phatdf = sum(ES.phat.df)#
    vv = stats::qt(alpha / 2, phatdf)#
    LowerBoundphat = Avephat + vv * sqrt(Avephatvar)#
    UpperBoundphat = Avephat - vv * sqrt(Avephatvar)#
    Avephatsig = UpperBoundphat < 0 | LowerBoundphat > 0#
    # Add back the 0.5#
    Avephat = Avephat + 0.5#
    # Perform a standard meta-analysis of the ktau values for all simulated experiments#
#
    meth = MAMethod#
#
    MA.res = metafor::rma(ES.cor, ES.ctvar, method = meth)#
    # Extract the data from the meta-analysis#
#
    MAMean = as.numeric(MA.res$beta)#
    MAvar = as.numeric(MA.res$se ^ 2)#
    QE = as.numeric(MA.res$QE)#
    QEp = as.numeric(MA.res$QEp)#
    UB = as.numeric(MA.res$ci.ub)#
    LB = as.numeric(MA.res$ci.lb)#
    MASig = UB < 0 | LB > 0#
    HetSig = QEp < 0.05#
#
    #	Meta-analyse Cliff'd d result#
#
    MA.dres = metafor::rma(ES.d, ES.vard, method = meth)#
    pvalue.d = as.numeric(MA.dres$pval)#
    d.sig = pvalue.d < 0.05#
    Mean.d = as.numeric(MA.dres$beta)#
#
    #	Meta-analysis of phat#
#
    MA.phat = metafor::rma(ES.phat, ES.phatvar, method = meth)#
    Mean.phat = as.numeric(MA.phat$beta) + 0.5 # Add the 0.5 back#
    phat.sig = as.numeric(MA.phat$pval) < 0.05#
#
    # Meta-analysis of g values for exact and approx.#
#
    # Note when N is the same for each study the variance for the study based on the average of d is the same for each study. The variance of g is affected by the degrees of freedom which based on a Welch style analysis may be less than a simple ANOVA#
    for (i in 1:Exp)#
      varg.exact[i] = varStandardizedEffectSize(mean(Cohen.d[i]), 4 / N, df[i], returnVarg =#
                                                  TRUE)#
    MA.g.exact = metafor::rma(g, varg.exact, method = meth)#
    Mean.g.exact = as.numeric(MA.g.exact$beta)#
    g.exact.sig = as.numeric(MA.g.exact$pval) < 0.05#
#
    MA.g.approx = metafor::rma(g, varg.approx, method = meth)#
    Mean.g.approx = as.numeric(MA.g.approx$beta)#
    g.approx.sig = as.numeric(MA.g.approx$pval) < 0.05#
    # Do a Pearson correlation analysisn#
#
    MAP.res = metafor::rma(P.cor, P.corvar, method = meth)#
    MAPresults = ExtractMAStatistics(MAP.res, N, N, type = "r")#
    P.mean = MAPresults$mean#
    P.rsig = MAPresults$pvalue <= 0.05#
    P.hetsig = MAPresults$QEp <= 0.05#
#
    # Meta-analysis of Cohen's d#
#
    Cohend.res = metafor::rma(Cohen.d, varCohend.approx)#
    Cohend.mean = as.numeric(Cohend.res$beta)#
    Cohend.sig = as.numeric(Cohend.res$pval) < 0.05#
#
    output = tibble::tibble(#
      NumExp,#
      GroupSize,#
      AveKtau,#
      AveKtauctvar,#
      tauSigCVt,#
      AveCliffd,#
      AveCliffdvar,#
      AveCliffdsig,#
      Avephat,#
      Avephatvar,#
      Avephatsig,#
      MAMean,#
      MAvar,#
      MASig,#
      QE,#
      QEp,#
      HetSig,#
      P.mean,#
      P.rsig,#
      P.hetsig,#
      Mean.phat,#
      phat.sig,#
      Mean.d,#
      d.sig,#
      Mean.g.exact,#
      g.exact.sig,#
      Mean.g.approx,#
      g.approx.sig,#
      Cohend.mean,#
      Cohend.sig#
    )#
  }#
  else{#
    output = tibble::tibble(#
      dplyr::bind_cols(#
        MeanExp = UES,#
        VarExp = Var,#
        StdESExp = Cohen.d,#
        df = df,#
        tval = tval,#
        tpval = tpval,#
        tciL = tciL,#
        tciU = tciU,#
        Cliffd = ES.d,#
        Cliffdvar = ES.vard,#
        PHat = ES.phat,#
        PHatvar = ES.phatvar,#
        PHatdf = ES.phat.df,#
        g = g,#
        gvar.approx = varg.approx,#
        Cohendvar = varCohend.approx#
      )#
    )#
#
  }#
  return(output)#
#
}#
#
##########################################################
#' @title NP2GroupMetaAnalysisSimulation#
#' @description This function simulates data from a family of experiments. The parameter Exp deteremines the number of experiments in the family. The function simulates data from one of four distributions and uses the data to construct two of groups of equal size (GroupSize). The distribution for one  of the groups corresponds to the control and is based on the given mean and spread, the distribution for the other group corresponds to the treatment group and  is based on the mean+diff and the spread plus any variance adjusemtn (determined by the parametrt StdAdj). The data from each experiment is analysed separately to estimate three non-parametric effect sizes: the point bi-serial version of Kendall's tau, Cliff's d and the probability of superiority referred to as phat and their variances. Parametric effect sizes Cohen's d (also known as the standarized means difference, SMD) and the small sample size adjusted standardized mean difference g are also calculated to gether with their variances. The effect si
zes are then meta-analysed using two main methods: the simple avarge of the effect size and the variance weighted average. The function uses the metafor package for formal meta-analysis, and the specific method of formal meta-analysis used is determined by the MAAMethod. All tests of signficance are done at the 0.05 level. If the parameter returnES is TRUE, the function returns it returns the effect sizes for each experiment in the family, otherwise it returns the meta-analysis results.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export NP2GroupMetaAnalysisSimulation#
#' @param mean the value used for the mean of control group in the simulated data. It can be any real number including zero.#
#' @param sd the value used for the spread of the control group and the spread of the treatment group in the simulated data. The value must be a real value greater than 0.#
#' @param diff mean+diff is the value used for the mean of the treatment group. It can be zero.#
#' @param Groupsize is the size of each of the 2 groups comprising one experiment. Groupsize should be an integer of 4 or more#
#' @param Exp is the number of experiments being simulated. Exp should be an integer of 2 or more. It defaults to 5.#
#' @param type specifies the distribution being simulated. The permited values are "n" for the normal distribution,  "l" for the lognormal distribution, "g" for the gamma distribution and "lap" for the Laplace distribution. The parameter defaults to "n".#
#' @param StdAdj specifies a level used to adjust the treatment variance. It allows heterogeneity to be modelled. It defaults to zero meaning no variance heterogeneity is introduced.#
#' @param seed specifies the seed to be used to initiate the simulation, so the simulation is repeatable. It defauls to 123.#
#' @param alpha the Type 1 error rate level use for statistical tests itb defaults to 0.05.#
#' @param MAMethod the meta-analysis method needed for the call to the metafor package rma algorithm#
#' @param returnES Determines the format of the output. It defaults to FALSE which causes the function to output the meta-anaysis results for the family of experiments. If set to TRUE it returns the effect sizes for each experiment.#
#' @return Depending on the value of the returnES parameter, the function either returnd the effect sizes for each experiment or the aggregated resutls for the family#
#' @examples#
#' NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=100,Exp=5,type="n",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=FALSE)#
#' #  NumExp GroupSize   AveKtau AveKtauctvar tauSigCVt AveCliffd AveCliffdvar AveCliffdsig Avephat   Avephatvar#
#' #  1      5       100 0.1255276 0.0003148795      TRUE    0.2498  0.001246261         TRUE  0.6249 0.0003100298#
#' #    Avephatsig MAMean        MAvar MASig  QE   QEp HetSig P.mean P.rsig P.hetsig Mean.phat phat.sig Mean.d d.sig#
#' #  1       TRUE 0.1275 0.0007020728  TRUE 9.5 0.049   TRUE  0.218   TRUE    FALSE    0.6268     TRUE 0.2537  TRUE#
#' #    Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Cohend.mean Cohend.sig#
#' #  1    0.4464658        TRUE        0.4465         TRUE   0.4481545       TRUE#
#
#' NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=TRUE)#
#' #        MeanExp    VarExp    StdESExp       df        tval     tpval       tciL       tciU Cliffd  Cliffdvar PHat#
#' #  1  0.22635002 1.0310004  0.22292108 17.94949 -0.49846668 0.6242081 -1.1805556  0.7278556   0.10 0.08080000 0.55#
#' #  2  1.00148259 0.6222094  1.26962403 15.43759 -2.83896563 0.0121708 -1.7515275 -0.2514377   0.64 0.04004364 0.82#
#' #  3  0.43163008 0.8477417  0.46879172 17.96318 -1.04825016 0.3084234 -1.2968381  0.4335779   0.36 0.07247192 0.68#
#' #  4  0.43376040 0.9672328  0.44104651 13.70035 -0.98620997 0.3411278 -1.3790318  0.5115110   0.22 0.07593212 0.61#
#' #  5 -0.03418706 0.7818597 -0.03866315 14.88047  0.08645344 0.9322586 -0.8092611  0.8776352  -0.20 0.08169697 0.40#
#' #        PHatvar   PHatdf           g gvar.exact gvar.approx Cohendvar#
#' #  1 0.019666667 17.77308  0.21345256         NA   0.1846401 0.2013843#
#' #  2 0.009466667 15.01346  1.20675332         NA   0.2278486 0.2522085#
#' #  3 0.017688889 15.49571  0.44889535         NA   0.1889924 0.2061171#
#' #  4 0.018422222 14.60073  0.41637659         NA   0.1845790 0.2070992#
#' #  5 0.020000000 17.30769 -0.03667547         NA   0.1800097 0.2000502#
#
#' NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.724,GroupSize=10,Exp=5,type="l",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=FALSE)#
#' #    NumExp GroupSize   AveKtau AveKtauctvar tauSigCVt AveCliffd AveCliffdvar AveCliffdsig Avephat  Avephatvar#
#' #  1      5        10 0.1810526  0.003603544      TRUE     0.344   0.01288023         TRUE   0.672 0.003118222#
#' #    Avephatsig MAMean      MAvar MASig  QE  QEp HetSig P.mean P.rsig P.hetsig Mean.phat phat.sig Mean.d d.sig#
#' #  1       TRUE 0.2104 0.00467435  TRUE 5.9 0.21  FALSE 0.2704   TRUE    FALSE    0.7014     TRUE  0.403  TRUE#
#' #    Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Cohend.mean Cohend.sig#
#' #  1    0.5405307        TRUE        0.5387         TRUE   0.5711566       TRUE#
NP2GroupMetaAnalysisSimulation = function(mean,#
                                          sd,#
                                          diff,#
                                          GroupSize,#
                                          Exp = 5,#
                                          type = "n",#
                                          StdAdj = 0,#
                                          alpha = 0.05,#
                                          seed = 123,#
                                          StdExp = 0,#
                                          MAMethod,#
                                          returnES = FALSE) {#
  base::set.seed(seed)#
  N = GroupSize#
#
  ES.cor = rep(NA, Exp) # This will hold the Kendall's tau for each experiment#
  ES.var = rep(NA, Exp) # This will hold the estimated consistent variance of tau for each experiment#
  P.cor = rep(NA, Exp) # This holds the Pearson correlation#
  P.corvar = rep(NA, Exp) # This holds the variance of normal transformation of the Pearson correlation#
#
  VarExp = rep(NA, Exp) # This holds the pooled variance for each experiment#
  MeanExp = rep(NA, Exp) # This holds the meandifference for each experiment#
  StdESExp = rep(NA, Exp) # This holds the standardized mean difference#
#
  Cliffd = rep(NA, Exp) #This holds Cliff's d#
  Cliffdvar = rep(NA, Exp) # This holds the variance of Clff's d#
#
  PHat = rep(NA, Exp) #This holds the probability of superiority phat#
  PHatvar = rep(NA, Exp) #This holds the variance of phat#
  PHatdf = rep(NA) # This holds the degrees of freedom of the t-test for phat#
  g = rep(NA, Exp) # This holds the small sample size adjusted standardized mean difference#
  gvar.exact = rep(NA, Exp) # This holds the exact variance of the small sample size adjusted standardized mean difference#
  gvar.approx = rep(NA, Exp) # This holds the approximate variance of the small sample size adjusted standardized mean difference#
  df = rep(NA, Exp) # This holds the degrees of freedom of the t-test of the mean difference#
  c = rep(NA, Exp) # This holds the small sample size adjustment factor for the standardized mean difference#
  tval = rep(NA, Exp) # This holds the t-test value for each experiment#
  tpval = rep(NA, Exp)  # This holds the p-value of the t-test for each experiment#
  tciL = rep(NA, Exp) # This holds the lower confidence interval bound of the standrdized mean difference for each experiment#
  tciU = rep(NA, Exp)  # This holds the upper confidence interval bound of the standrdized mean difference for each experiment#
  Cohendvar = rep(NA, Exp) # This holds the variance of the standardized mean difference for each experiment#
#
  # For  the point-biserial tau we need a dummy variable that takes the value zero for control group data points and 1 for treatment group data points#
  dummy = c(rep(0, N), rep(1, N))#
#
  for (i in 1:Exp)#
  {#
    if (type == "n") {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = stats::rnorm(N, mean + ExpAdj, sd)#
      y = stats::rnorm(N, mean + ExpAdj + diff, sd + StdAdj)#
    }#
    if (type == "g")#
    {#
      shape = sd#
      rate = mean#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = abs(rnorm(1, 0, StdExp))#
      x = stats::rgamma(N, shape, rate + ExpAdj)#
      y = stats::rgamma(N, shape + StdAdj, rate + ExpAdj + diff)#
#
    }#
    if (type == "l")#
    {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = stats::rlnorm(N, mean + ExpAdj, sd)#
      y = stats::rlnorm(N, mean + ExpAdj + diff, sd + StdAdj)#
    }#
#
    if (type == "lap")#
      # Changed to use built-in defaults for max and min#
    {#
      if (StdExp == 0)#
        ExpAdj = 0#
      else#
        ExpAdj = rnorm(1, 0, StdExp)#
      x = LaplaceDist(N, mean + ExpAdj, sd)#
      y = LaplaceDist(N, mean + diff + ExpAdj, sd + StdAdj)#
    }#
    # This holds the simulated data set with each observation assigned to a group in an experiment#
#
    # The data in each experiment is analysed to obtain the value of Kendall's tau, its consistent variance and its hypothesis testing variance#
#
    xy = c(x, y)#
    expdata = base::data.frame(xy, dummy)#
#
    ktau = Kendalltaupb(expdata$xy, expdata$dummy)#
    ES.cor[i] = ktau$cor#
    ES.var[i] = ktau$consistentvar#
#
    #Analyse the generated data for each member of the family using the cid function to obtain Cliff's d and its variance#
    Cliff = Cliffd(y, x)#
    Cliffd[i] = Cliff$d#
    Cliffdvar[i] = Cliff$sqse.d#
#
    #Analyse the generated data for each member of the family using the bmp fucntion to obtain phat and its variance#
#
    PHat.res = calculatePhat(x, y)#
    PHat[i] = PHat.res$phat#
    PHatvar[i] = PHat.res$s.e. ^ 2#
    PHatdf[i] = PHat.res$df#
#
    # Prepare to do a standard analysis for comparison#
    # We can do both a standardized effect size or correlation. For the paper we use the standardized mean difference effect size#
#
    VarExp[i] = (stats::var(x) + stats::var(y)) / 2#
    MeanExp[i] = base::mean(y) - base::mean(x)#
    StdESExp[i] = (MeanExp[i]) / sqrt(VarExp[i])#
    tempttest = stats::t.test(x, y)#
    df[i] = as.numeric(tempttest$parameter)#
    tval[i] = tempttest$statistic#
    tpval[i] = tempttest$p.value#
    tciL[i] = tempttest$conf.int[1]#
    tciU[i] = tempttest$conf.int[2]#
    c[i] = reproducer::calculateSmallSampleSizeAdjustment(df[i])#
    g[i] = StdESExp[i] * c[i]#
#
    # N in each group#
#
    gvar.approx[i] = 2 * c[i] ^ 2 / N + g[i] ^ 2 / (2 * df[i])#
#
    Cohendvar[i] = 2 / N + StdESExp[i] ^ 2 / (2 * df[i])#
#
    Pearsonr = g[i] / sqrt(g[i] ^ 2 + 4)#
    TempP = as.numeric(Pearsonr)#
    TempP = reproducer::transformRtoZr(TempP)#
    P.cor[i] = TempP#
    P.corvar[i] = 1 / (2 * N - 3)#
#
  }#
#
  if (!returnES)#
  {#
    NumExp = Exp#
#
    # First aggregate as if the family is a planned distributed experiment, i.e. the "experiment" is a blocking factor#
#
    # Aggregate tau_pb values#
    AveKtau = base::mean(ES.cor)#
    AveKtauctvar = sum(ES.var) / NumExp ^ 2#
#
    #Obtain confidence intervals using t-distribtion with 2*N-3*NumExp degrees of freedom#
#
    vv = stats::qt(alpha / 2, (2 * N - 3) * NumExp)#
    LowerBoundt = AveKtau + vv * sqrt(AveKtauctvar)#
    UpperBoundt = AveKtau - vv * sqrt(AveKtauctvar)#
#
    tauSigCVt = UpperBoundt < 0 | LowerBoundt > 0#
    #Aggregate the Cliff's d values#
#
    AveCliffd = base::mean(Cliffd)#
    AveCliffdvar = sum(Cliffdvar) / NumExp ^ 2#
    vvd = stats::qt(alpha / 2, (2 * N - 2) * NumExp)#
    LowerBoundd = AveCliffd + vvd * sqrt(AveCliffdvar)#
    UpperBoundd = AveCliffd - vvd * sqrt(AveCliffdvar)#
#
    AveCliffdsig = UpperBoundd < 0 | LowerBoundd > 0#
#
    #Aggregate the phat values#
#
    Avephat = base::mean(PHat)#
    Avephatadj = Avephat - 0.5 # The null effect for phat is 0.5#
    Avephatvar = sum(PHatvar) / NumExp ^ 2#
    pdf = sum(PHatdf)#
    vvp = stats::qt(alpha / 2, pdf * NumExp)#
    # Test whether Avephatadj is different from zero, tests whether phat is different from 0.5#
    LowerBoundp = Avephatadj + vvp * sqrt(Avephatvar)#
    UpperBoundp = Avephatadj - vv * sqrt(Avephatvar)#
#
    Avephatsig = UpperBoundp < 0 | LowerBoundp > 0#
#
    # Perform a standard meta-analysis of the non-parametric statistics for the family of simulated experiments#
    # tau_pb meta-analysis#
    meth = MAMethod#
#
    MA.res = metafor::rma(ES.cor, ES.var, method = meth)#
    # Extract the data from the analysis#
#
    MAtauResults = ExtractMAStatistics(MA.res, 2 * N, 2 * N, Transform = FALSE)#
    MAMean = MAtauResults$mean#
    UB = MAtauResults$UB#
    LB = MAtauResults$LB#
    MASig = UB < 0 | LB > 0#
    MAvar = as.numeric(MA.res$se ^ 2)#
    QE = MAtauResults$QE#
    QEp = MAtauResults$QEp#
    HetSig = QEp < 0.05#
#
    # Cliff's d meta-analysis#
    MAd.res = metafor::rma(Cliffd, Cliffdvar, method = meth)#
    MAd.Results = ExtractMAStatistics(MAd.res, 2 * N, 2 * N, Transform = FALSE)#
    Mean.d = MAd.Results$mean#
    d.sig = MAd.Results$pvalue < 0.05#
#
    # Probability of Superiority phat meta-analysis#
    PHatAdj = PHat - 0.5#
    MAphat.res = metafor::rma(PHatAdj, PHatvar, method = meth) # meta-analyse phat-0.5#
    MAphat.Results = ExtractMAStatistics(MAphat.res, 2 * N, 2 * N, Transform =#
                                           FALSE)#
    Mean.phat = MAphat.Results$mean + 0.5 # Add back the 0.5 value#
    phat.sig = MAphat.Results$pvalue < 0.05#
    # Do a Pearson correlation analysis as a comparison with tau_pb#
#
    MAP.res = metafor::rma(P.cor, P.corvar, method = meth)#
#
    MAPresults = ExtractMAStatistics(MAP.res, 2 * N, 2 * N, type = "r")#
    P.mean = MAPresults$mean#
    P.rsig = MAPresults$pvalue <= 0.05#
    P.hetsig = MAPresults$QEp <= 0.05#
#
    # Do an analysis of the small sample size standardized effect size as a comparison with Cliff's d#
    MAgres.approx = metafor::rma(g, gvar.approx, method = meth)#
    MAgresults.approx = ExtractMAStatistics(MAgres.approx, 2 * N, 2 * N, Transform =#
                                              FALSE)#
    Mean.g.approx =	MAgresults.approx$mean#
    g.approx.sig = MAgresults.approx$pvalue < 0.05#
    # Note when N is the same for each study, the variance for the study based on the average of d is the same for each study. The variance of g is affected by the degrees of freedom which based on a Welch style analysis may be less than a simple ANOVA#
#
    for (i in 1:Exp)#
      gvar.exact[i] = varStandardizedEffectSize(StdESExp[i], 2 / N, df[i], returnVarg =#
                                                  TRUE)#
    MA.g.exact = metafor::rma(g, gvar.exact, method = meth)#
    Mean.g.exact = as.numeric(MA.g.exact$beta)#
    g.exact.sig = as.numeric(MA.g.exact$pval) < 0.05#
#
    # Cohen's d metaanalysis#
#
    Cohend.res = metafor::rma(StdESExp, Cohendvar)#
    Cohend.mean = as.numeric(Cohend.res$beta)#
    Cohend.sig = as.numeric(Cohend.res$pval) < 0.05#
#
    output = tibble::tibble(#
      NumExp,#
      GroupSize,#
      AveKtau,#
      AveKtauctvar,#
      tauSigCVt,#
      AveCliffd,#
      AveCliffdvar,#
      AveCliffdsig,#
      Avephat,#
      Avephatvar,#
      Avephatsig,#
      MAMean,#
      MAvar,#
      MASig,#
      QE,#
      QEp,#
      HetSig,#
      P.mean,#
      P.rsig,#
      P.hetsig,#
      Mean.phat,#
      phat.sig,#
      Mean.d,#
      d.sig,#
      Mean.g.exact,#
      g.exact.sig,#
      Mean.g.approx,#
      g.approx.sig,#
      Cohend.mean,#
      Cohend.sig#
    )#
  }#
#
  else  {#
    output = tibble::as_tibble(#
      dplyr::bind_cols(#
        MeanExp = MeanExp,#
        VarExp = VarExp,#
        StdESExp = StdESExp,#
        df = df,#
        tval = tval,#
        tpval = tpval,#
        tciL = tciL,#
        tciU = tciU,#
        Cliffd = Cliffd,#
        Cliffdvar = Cliffdvar,#
        PHat = PHat,#
        PHatvar = PHatvar,#
        PHatdf = PHatdf,#
        g = g,#
        gvar.exact = gvar.exact,#
        gvar.approx = gvar.approx,#
        Cohendvar#
      )#
    )#
  }#
  return(output)#
}#
########################################################
#' @title MetaAnalysisSimulations#
#' @description This function simulates data from many families of experiments. The number of families simulated is defined by the Replications parameter. The parameter Exp determines the number of experiments in each family. The function simulates data from one of four distributions and uses the data to construct two of groups of equal size (GroupSize). The experimental design of individual experiments in each family is determined by the FourGroup parameter. If FourGroup=FALSE, the basic experimental design is a balanced two group randomized experiment, otherwise the experimental design is a balanced four group experiment corresponding to a randomized blocks experiment. The function calls either NP2GroupMetaAnalysisSimulation or NP4GroupMetaAnalysisSimulation to generate and analyse data for each individual family. The function either returns the meta-analysed data from each experiment or provides summary statistics.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export MetaAnalysisSimulations#
#' @param mean the value used for the mean of control group in the simulated data. It can be any real number including zero.#
#'	@param sd the value used for the spread of the control group and the spread of the treatment group in the simulated data. The value must be a real value greater than 0.#
#' 	@param diff mean+diff is the value used for the mean of the treatment group. It can be zero.#
#'	@param Groupsize is the size of each of the groups comprising one experiment. Groupsize should be an integer of 4 or more#
#'	@param Exp=5 is the number of experiments being simulated. Exp should be an integer of 2 or more. It defaults to 5.#
#'	@param Replications=500 The number of times the set of experiments is simulated.#
#'	@param type=n specifies the distribution being simulated. The permited values are "n" for the normal distribution,  "l" for the lognormal distribution, "g" for the gamma distribution and "lap" for the Laplace distribution. The parameter defaults to "n".#
#'	@param seed specifies the seed to be used to initiate the simulation, so the simulation is repeatable. It defaults to 456.#
#'	@param FourGroup is a Boolean variable that determines whether the experiment is a two group experiments or a 4-Group randomised block experiment. It defaults to FALSE which means a two group experiment is the default condition#
#'	@param MAMethod specifies the model to be used when experimental effect sizes ar aggregated using the R metafor package.#
#'	@param returnData=FALSE if TRUE the function outputs the summary statistics otherwise it outputs the meta-analysis results for each family#
#'	@return The parameter either returns the meta-analysis values obtained from each family or the average values of the meta-analysis over all replications.#
#'	@examples#
#'	MetaAnalysisSimulations(mean=0,sd=1,diff=0.5,GroupSize=10,type="n",Replications=50,Exp=5,seed=456,alpha=0.05,FourGroup=FALSE,StdAdj=0,BlockEffect=0,BlockStdadj=0,StdExp=0,MAMethod="PM")#
#'	#  Averagektau Averagektauctvar AveragetauSigCVt AverageCliffd AverageCliffdvar AverageCliffdsig Averagephat#
#'	#  1      0.1569         0.003726              0.7        0.2981          0.01334              0.7       0.649#
#'	#    Averagephatvar Averagephatsig MAAverage MAVariance MASignificant HetSignificant PMAAverage PMASignificant#
#'	#  1       0.003236           0.72    0.1755    0.00509           0.7           0.16     0.2523           0.66#
#'	#    PMAHetSignificant Mean.phat phat.sig Mean.d d.sig Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Mean.Cohend#
#'	#  1                 0    0.6678      0.7 0.3356   0.7       0.5163        0.58        0.5183          0.6      0.5432#
#'	#    Cohend.sig#
#'	#  1       0.64#
#
#'	MetaAnalysisSimulations(mean=0,sd=1,diff=0.734,GroupSize=100,type="l",Replications=50,Exp=5,seed=456,alpha=0.05,FourGroup=FALSE,StdAdj=0,BlockEffect=0,BlockStdadj=0,StdExp=0,MAMethod="PM")#
#'	#   Averagektau Averagektauctvar AveragetauSigCVt AverageCliffd AverageCliffdvar AverageCliffdsig Averagephat#
#'	# 1      0.2031        0.0002737                1        0.4042         0.001082                1      0.7021#
#'	#   Averagephatvar Averagephatsig MAAverage MAVariance MASignificant HetSignificant PMAAverage PMASignificant#
#'	# 1      0.0002691              1    0.2049  0.0003088             1           0.02     0.2654              1#
#'	#   PMAHetSignificant Mean.phat phat.sig Mean.d d.sig Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Mean.Cohend#
#'	# 1              0.02    0.7039        1 0.4078     1       0.5496           1        0.5497            1      0.5526#
#'	#   Cohend.sig#
#'	# 1          1#
#' MetaAnalysisSimulations(mean=0,sd=1,diff=0.5,GroupSize=10,type="n",Replications=10,Exp=5,seed=456,alpha=0.05,FourGroup=TRUE,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM",returnES=TRUE)#
#'	#    Family NumExp GroupSize   AveKtau AveKtauctvar tauSigCVt AveCliffd AveCliffdvar AveCliffdsig Avephat  Avephatvar#
#'	# 1       1      5        10 0.1326316  0.002064273      TRUE     0.252  0.007423693         TRUE   0.626 0.001812889#
#'	# 2       2      5        10 0.1610526  0.001898865      TRUE     0.306  0.006806384         TRUE   0.653 0.001652222#
#'	# 3       3      5        10 0.1347368  0.002064942      TRUE     0.256  0.007437042         TRUE   0.628 0.001810222#
#'	# 4       4      5        10 0.1600000  0.001914490      TRUE     0.304  0.006855467         TRUE   0.652 0.001670222#
#'	# 5       5      5        10 0.1326316  0.001781476      TRUE     0.252  0.006359483         TRUE   0.626 0.001540889#
#'	# 6       6      5        10 0.1915789  0.001775461      TRUE     0.364  0.006336824         TRUE   0.682 0.001535111#
#'	# 7       7      5        10 0.1894737  0.001847652      TRUE     0.360  0.006610279         TRUE   0.680 0.001604000#
#'	# 8       8      5        10 0.1263158  0.001989751      TRUE     0.240  0.007153034         TRUE   0.620 0.001738222#
#'	# 9       9      5        10 0.1315789  0.002001834      TRUE     0.250  0.007192178         TRUE   0.625 0.001751778#
#'	# 10     10      5        10 0.1800000  0.001806651      TRUE     0.342  0.006464372         TRUE   0.671 0.001562000#
#'	#    Avephatsig    MAMean       MAvar MASig        QE        QEp HetSig P.mean P.rsig P.hetsig Mean.phat phat.sig#
#'	# 1        TRUE 0.1393726 0.002046832  TRUE  2.794338 0.59281039  FALSE 0.1986   TRUE    FALSE 0.6327711     TRUE#
#'	# 2        TRUE 0.1696669 0.001869083  TRUE  3.299228 0.50905466  FALSE 0.2849   TRUE    FALSE 0.6619020     TRUE#
#'	# 3        TRUE 0.1381987 0.002059278  TRUE  2.435660 0.65619273  FALSE 0.2086   TRUE    FALSE 0.6314274     TRUE#
#'	# 4        TRUE 0.1642904 0.001881646  TRUE  1.011643 0.90802538  FALSE 0.2469   TRUE    FALSE 0.6563764     TRUE#
#'	# 5        TRUE 0.1377732 0.004771547  TRUE 10.584733 0.03164961   TRUE 0.2467   TRUE     TRUE 0.6310782     TRUE#
#'	# 6        TRUE 0.2007925 0.001745227  TRUE  3.262070 0.51496425  FALSE 0.3045   TRUE    FALSE 0.6915418     TRUE#
#'	# 7        TRUE 0.1940596 0.001834888  TRUE  1.731819 0.78492988  FALSE 0.3104   TRUE    FALSE 0.6847398     TRUE#
#'	# 8        TRUE 0.1374584 0.002656895  TRUE  5.596009 0.23141822  FALSE 0.2165   TRUE    FALSE 0.6312184     TRUE#
#'	# 9        TRUE 0.1373058 0.001984973  TRUE  2.893346 0.57582945  FALSE 0.2025   TRUE    FALSE 0.6308095     TRUE#
#'	# 10       TRUE 0.1919994 0.002203644  TRUE  4.984036 0.28893944  FALSE 0.3107   TRUE    FALSE 0.6829583     TRUE#
#'	#       Mean.d d.sig Mean.g.exact g.exact.sig Mean.g.approx g.approx.sig Cohend.mean Cohend.sig#
#'	# 1  0.2658353  TRUE    0.3966576       FALSE     0.3902160         TRUE   0.4000799       TRUE#
#'	# 2  0.3233461  TRUE    0.5831369        TRUE     0.5738217         TRUE   0.5877993       TRUE#
#'	# 3  0.2631414  TRUE    0.4182562       FALSE     0.4119388         TRUE   0.4216884       TRUE#
#'	# 4  0.3127453  TRUE    0.4963182       FALSE     0.4941671         TRUE   0.5070138       TRUE#
#'	# 5  0.2624081  TRUE    0.4998541       FALSE     0.4989203        FALSE   0.5118556      FALSE#
#'	# 6  0.3825139  TRUE    0.6266782        TRUE     0.6175178         TRUE   0.6337251       TRUE#
#'	# 7  0.3692921  TRUE    0.6352013        TRUE     0.6304254         TRUE   0.6459643       TRUE#
#'	# 8  0.2622810  TRUE    0.4329479       FALSE     0.4239390         TRUE   0.4342665       TRUE#
#'	# 9  0.2617025  TRUE    0.4015384       FALSE     0.3956811         TRUE   0.4054131       TRUE#
#'	# 10 0.3662063  TRUE    0.6404286        TRUE     0.6294023         TRUE   0.6444410       TRUE#
MetaAnalysisSimulations = function(mean = 0,#
                                   sd = 1,#
                                   diff = 0.5,#
                                   GroupSize = 10,#
                                   type = "n",#
                                   Replications = 50,#
                                   Exp = 5,#
                                   seed = 456,#
                                   alpha = 0.05,#
                                   FourGroup = FALSE,#
                                   StdAdj = 0,#
                                   BlockEffect = 0,#
                                   BlockStdadj = 0,#
                                   StdExp = 0,#
                                   MAMethod = "PM",#
                                   returnES = F) {#
  # Set up variables to hold the outcome of the simulations#
#
  Averagektau = 0 # holds the average of the estimate of ktau across all the replications#
  Averagektauctvar = 0 # holds the average of the estimation of the consistent variance ktau across all the replications#
  AveragetauSigCVt = 0 # Holds the average number of times the difference between the means was assessed as signifciant using the confidence limits obtained from the consistent variance with the t-distribution#
  AverageCliffd = 0 # Holds the average of Cliff's d#
  AverageCliffdvar = 0 # Holds the average of the variance of Cliff's d#
  AverageCliffdsig = 0 # Holds the average number of times the Cliff's d was assessed as signifciant using average hypothesis testing variance#
  Averagephat = 0 # Holds the average of phat#
  Averagephatvar = 0 # Holds the average of the variance of phat#
  Averagephatsig = 0 # Holds the average number of times phat was assessed as signifciant using average hypothesis testing variance#
  MAAverage = 0 # holds the average of the estimate of ktau obtained from the meta-analysis across all the replications#
  MAVariance = 0 # holds the average of the estimate of the variance of ktau obtained from the meta-analysis across all the replications#
  MASignificant = 0 # Holds the average number of times the meta-analysis estimate of ktau was significant#
  HetSignificant = 0 # Holds the average number of times the meta-analysis heteogenity test was significant#
  MAPcor.mean = 0 # Holds the sum of the mean values of the point biserial correlation using Pearson's formula#
  MAPcor.sig = 0 # Holds the number of times the mean Pr is signficiany#
  MAPcor.hetsig = 0 # Holds the number of times the meta-analysis detected heterogeneity#
  Mean.g.exact = 0 # Holds the standardized effect size based on meta-analysis with the exact variance#
  g.exact.sig = 0 # Holds the number of times the standardized effect size was significant with a meta-analysis using the exact variance#
  Mean.g.approx = 0 # Holds the standardized effect size based on meta-analysis with the approximate variance#
  g.approx.sig = 0 # Holds the number of times the standardized effect size was significant with a meta-analysis using the approximate variance#
  Mean.phat = 0 # Holds the mean phat effect size  based on a meta-analysis#
#
  phat.sig = 0 # Holds the number of times the phat effect size was significant with a meta-analysis#
  Mean.d = 0 # Holds the mean Cliff's d effect size based on meta-analysis#
  d.sig = 0 # Holds the number of times the Cliff's d effect size was significant with a meta-analysis#
#
  Mean.Cohend = 0  # Holds the mean SMD effect size based on meta-analysis#
#
  Cohend.sig = 0 # Holds the number of times the SMD effect size was significant with a meta-analysis#
#
  DataTable = NULL#
#
  FourGroupExp = c(rep(FourGroup, Replications))#
#
  for (i in 1:Replications) {#
    if (!FourGroupExp[i]) {#
      res = NP2GroupMetaAnalysisSimulation(#
        mean,#
        sd,#
        diff,#
        GroupSize,#
        Exp = Exp,#
        type = type,#
        seed = seed + i,#
        alpha = alpha,#
        StdAdj = StdAdj,#
        StdExp = StdExp,#
        MAMethod = MAMethod#
      )#
    }#
    if (FourGroupExp[i]) {#
      res = NP4GroupMetaAnalysisSimulation(#
        mean,#
        sd,#
        diff,#
        GroupSize,#
        Exp = Exp,#
        type = type,#
        seed = seed + i,#
        alpha = alpha,#
        StdAdj = StdAdj,#
        BlockEffect = BlockEffect,#
        BlockStdadj = BlockStdadj,#
        StdExp = StdExp,#
        MAMethod = MAMethod#
      )#
    }#
    if (!returnES)  {#
      #Obtain the values from the simulated family analysis and add to the results of previous simulations#
      Averagektau = Averagektau + res$AveKtau#
      Averagektauctvar = Averagektauctvar + res$AveKtauctvar#
      if (res$tauSigCVt) {#
        AveragetauSigCVt = AveragetauSigCVt + 1#
      }#
#
      AverageCliffd = AverageCliffd + res$AveCliffd#
      AverageCliffdvar = AverageCliffdvar + res$AveCliffdvar#
      if (res$AveCliffdsig) {#
        AverageCliffdsig = AverageCliffdsig + 1#
      }#
#
      Averagephat = Averagephat + res$Avephat#
      Averagephatvar = Averagephatvar + res$Avephatvar#
      if (res$Avephatsig) {#
        Averagephatsig = Averagephatsig + 1#
      }#
      MAAverage = MAAverage + res$MAMean#
      MAVariance = MAVariance + res$MAvar#
      if (res$MASig) {#
        MASignificant = MASignificant + 1#
      }#
      if (res$HetSig) {#
        HetSignificant = HetSignificant + 1#
      }#
      MAPcor.mean = MAPcor.mean + res$P.mean#
      if (res$P.rsig) {#
        MAPcor.sig = MAPcor.sig + 1#
      }#
      if (res$P.hetsig) {#
        MAPcor.hetsig = MAPcor.hetsig + 1#
      }#
      Mean.phat = Mean.phat + res$Mean.phat#
      if (res$phat.sig) {#
        phat.sig = phat.sig + 1#
      }#
#
      Mean.d = Mean.d + res$Mean.d#
      if (res$d.sig) {#
        d.sig = d.sig + 1#
      }#
#
      Mean.g.exact = Mean.g.exact + res$Mean.g.exact#
      if (res$g.exact.sig) {#
        g.exact.sig = g.exact.sig + 1#
      }#
#
      Mean.g.approx = Mean.g.approx + res$Mean.g.approx#
      if (res$g.approx.sig) {#
        g.approx.sig = g.approx.sig + 1#
      }#
      Mean.Cohend = Mean.Cohend + res$Cohend.mean#
      if (res$Cohend.sig) {#
        Cohend.sig = Cohend.sig + 1#
      }#
    }#
    else {#
      # Store the outcome from each replication#
      DataTable = tibble::as_tibble(dplyr::bind_rows(DataTable, dplyr::bind_cols(Family = i, res)))#
    }#
#
  }#
#
  if (!returnES) {#
    #Calculate averages.#
    Averagektau = signif(Averagektau / Replications, 4)#
    Averagektauctvar = signif(Averagektauctvar / Replications, 4)#
    AveragetauSigCVt = signif(AveragetauSigCVt / Replications, 4)#
#
    AverageCliffd = signif(AverageCliffd / Replications, 4)#
    AverageCliffdvar = signif(AverageCliffdvar / Replications, 4)#
    AverageCliffdsig = signif(AverageCliffdsig / Replications, 4)#
#
    Averagephat = signif(Averagephat / Replications, 4)#
    Averagephatvar = signif(Averagephatvar / Replications, 4)#
    Averagephatsig = signif(Averagephatsig / Replications, 4)#
    MAAverage = signif(MAAverage / Replications, 4)#
    MAVariance = signif(MAVariance / Replications, 4)#
    MASignificant = signif(MASignificant / Replications, 4)#
    HetSignificant = signif(HetSignificant / Replications, 4)#
    PMAAverage = signif(MAPcor.mean / Replications, 4)#
    PMASignificant = signif(MAPcor.sig / Replications, 4)#
    PMAHetSignificant = signif(MAPcor.hetsig / Replications, 4)#
#
    Mean.phat = signif(Mean.phat / Replications, 4)#
    phat.sig = signif(phat.sig / Replications, 4)#
#
    Mean.d = signif(Mean.d / Replications, 4)#
    d.sig = signif(d.sig / Replications, 4)#
#
    Mean.g.exact = signif(Mean.g.exact / Replications, 4)#
    g.exact.sig = signif(g.exact.sig / Replications, 4)#
#
    Mean.g.approx = signif(Mean.g.approx / Replications, 4)#
    g.approx.sig = signif(g.approx.sig / Replications, 4)#
#
    Mean.Cohend = signif(Mean.Cohend / Replications, 4)#
    Cohend.sig = signif(Cohend.sig / Replications, 4)#
#
  }#
#
  if (!returnES)#
  {#
    outcome = tibble::tibble(#
      Averagektau,#
      Averagektauctvar,#
      AveragetauSigCVt,#
      AverageCliffd,#
      AverageCliffdvar,#
      AverageCliffdsig,#
      Averagephat,#
      Averagephatvar,#
      Averagephatsig,#
      MAAverage,#
      MAVariance,#
      MASignificant,#
      HetSignificant,#
      PMAAverage,#
      PMASignificant,#
      PMAHetSignificant,#
      Mean.phat,#
      phat.sig,#
      Mean.d,#
      d.sig,#
      Mean.g.exact,#
      g.exact.sig,#
      Mean.g.approx,#
      g.approx.sig,#
      Mean.Cohend,#
      Cohend.sig#
    )#
  }#
#
  else#
  {#
    outcome = DataTable#
  }#
#
  return(outcome)#
}#
#
#####################################################################################################################################################
#
# Function used to find the expected means,variance and effect sizes from different distributions#
#
#' @title CalculateTheoreticalEffectSizes#
#' @description This function constructs the theoretical effect sizes and distribution statistics four (normal, lognormal, Laplace & gamma) given specific parameter values for the distributions#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export CalculateTheoreticalEffectSizes#
#' @param mean The theoretical central location parameter for the distribution specified by the type parameter.#
#' @param std The theoretical spread parameter for the distribution specified by the type parameter.#
#' @param type String identifying the distribution, "n" for normal, "ln" for lognormal, "lap" for Laplace, "g" for Gamm#
#' @return dataframe containing the expected standardized effect size, mean, variance,skewness and kurtosis statistics for samples from the specifie distribution#
#' @example#
#' CalculateTheoreticalEffectSizes(mean=0, std=1, type="l")#
#' #   RawMean RawVariance RawEffectSize RawSkewness RawKurtosis#
#' #1 1.648721    4.670774      0.762874    6.184877    88.54343#
#' CalculateTheoreticalEffectSizes(mean=0, std=1, type="n")#
#' #   RawMean RawVariance RawEffectSize RawSkewness RawKurtosis#
#' # 1       0           1             0           0           3#
#
CalculateTheoreticalEffectSizes = function(mean, std, type = "n") {#
  if (type == "n") {#
    # The expected values of a sample from the normal distribution#
    RawMean = mean#
    RawVariance = std ^ 2#
    RawSkewness = 0#
    RawKurtosis = 3#
  }#
  if (type == "l") {#
    # The expected values of a sample from the lognormal distribution#
    RawMean = exp(mean + std ^ 2 / 2)#
    RawVariance = (exp(std ^ 2) - 1) * exp(2 * mean + std ^ 2)#
    RawSkewness = (exp(std ^ 2) + 2) * sqrt(exp(std ^ 2) - 1)#
    RawKurtosis = exp(4 * std ^ 2) + 2 * exp(2 * std ^ 2) + 3 * exp(2 *#
                                                                      std ^ 2) - 3#
#
  }#
  if (type == "g") {#
    # The expected values of a sample from the gamma distribution#
    shape = std#
    rate = mean#
    RawMean = shape / rate#
    RawVariance = shape / rate ^ 2#
    RawSkewness = 2 / sqrt(shape)#
    RawKurtosis = 6 / shape + 3#
  }#
  if (type == "lap") {#
    # The expected values of a sample from the Laplace distribution#
#
    location = mean#
    scale = std#
    RawMean = location#
    RawVariance = 2 * scale  ^ 2#
    RawSkewness = 0#
    RawKurtosis = 6#
#
  }#
  RawEffectSize = RawMean / sqrt(RawVariance)#
  output = tibble::tibble(RawMean,#
                          RawVariance,#
                          RawEffectSize,#
                          RawSkewness,#
                          RawKurtosis)#
  return(output)#
}#
#
#' @title RandomizedDesignEffectSizes#
#' @description This function creates the theoretical effect sizes for random samples from one of four different distributions for specified parameter values for the diftribution specified by the type parameter. It assumes there are two samples, one corresponding to a control group and the other to the treatment group. It returns the theoretical effect sizes for a fully randomized experiment.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedDesignEffectSizes#
#' @param m1 The theoretical mean for the control group#
#' @param std1 The theoretical variance for the control group#
#' @param m2 The theoretical mean for the treatment group#
#' @param std2 The theoretical variance for the treatment group#
#' @param type String identifying the distribution, "n" for normal, "ln" for lognormal, "lap" for Laplace, "g" for Gamma#
#' @return dataframe containing the expected values of the unstandardized mean difference effect size, the pooled witjin group variance, the standardized mean difference effect size and the point bi-serial correlation.#
#' @example#
#' RandomizedDesignEffectSizes(m1=0, std1=1, m2=1, std2=3, type = "n")#
#' #  ES Var     StdES      rPBS#
#' #1  1   5 0.4472136 0.2182179#
#' RandomizedDesignEffectSizes(m1=0, std1=1, m2=1, std2=3, type = "l")#
#' #        ES       Var     StdES        rPBS#
#' #1 243.0432 242552663 0.0156056 0.007802562#
#'  RandomizedDesignEffectSizes(m1=0, std1=1, m2=0.266, std2=1, type = "l")#
#' #          ES      Var     StdES       rPBS#
#' # 1 0.5024232 6.310995 0.1999957 0.09950162#
#
RandomizedDesignEffectSizes = function(m1, std1, m2, std2, type = "n") {#
  G1.results = CalculateTheoreticalEffectSizes(m1, std1, type = type)#
  G2.results = CalculateTheoreticalEffectSizes(m2, std2, type = type)#
  ES = G2.results$RawMean - G1.results$RawMean#
  Var = (G2.results$RawVariance + G1.results$RawVariance) / 2#
  StdES = ES / sqrt(Var)#
  rPBS = StdES / sqrt(StdES ^ 2 + 4)#
  output = tibble::tibble(ES, Var, StdES, rPBS)#
  return(output)#
}#
#
#' @title RandomizedBlockDesignEffectSizes#
#' @title RandomizedDesignEffectSizes#
#' @description This function finds the theoretical effect sizes for a four-group randomized block experiments assuming one of four different underlying distributions specified by the type parameter. The design assumes two blocks each comprising a control and treatment group. If required a fixed Blocking effect is added to the mean for Block 2.#
#' @author Barbara Kitchenham and Lech Madeyski#
#' @export RandomizedBlockDesignEffectSizes#
#' @param m1 The theoretical mean for the control group in Block 1#
#' @param std1 The theoretical variance for the control group in Block 1#
#' @param m2 The theoretical mean for the treatment group in Block 1#
#' @param std2 The theoretical variance for the treatment group in Block 1#
#' @param m3 The theoretical mean for the control group in Block 2#
#' @param std3 The theoretical variance for the control group in Block 2#
#' @param m4 The theoretical mean for the treatment group in Block 2#
#' @param std4 The theoretical variance for the treatment group in Block 2#
#' @param BE A fixed block effect to be added to the Block 2 mean values.#
#' @param type String identifying the distribution, "n" for normal, "ln" for lognormal, "lap" for Laplace, "g" for Gamma#
#' @return dataframe holing the expected unstandardized mean difference effect size, the pooled within group variance, the standardized effect size and the point bi-serial correlation.#
#' @example#
#' RandomizedBlockDesignEffectSizes(m1=0,std1=1,m2=1,std2=1,m3=0,std3=1,m4=1,std4=1,BE = 1,type = "n")#
#' # ES Var StdES      rPBS#
#'#1  1   1     1 0.4472136#
#' RandomizedBlockDesignEffectSizes(m1=0,std1=1,m2=1,std2=1,m3=0,std3=1,m4=1,std4=1,BE = 1,type = "l")#
#'#        ES      Var     StdES      rPBS#
#'#1 5.266886 82.17791 0.5810004 0.2789675#
#' RandomizedBlockDesignEffectSizes(m1=0,std1=1,m2=0.266,std2=1,m3=0,std3=1,m4=0.266,std4=1,BE = 0,type = "l")#
#'#        ES      Var     StdES       rPBS#
#'#1 0.5024232 6.310995 0.1999957 0.09950162#
RandomizedBlockDesignEffectSizes = function(m1,#
                                            std1,#
                                            m2,#
                                            std2,#
                                            m3,#
                                            std3,#
                                            m4,#
                                            std4,#
                                            BE = 0,#
                                            type = "n") {#
  # Find the basic statistics for each group#
  G1.results = CalculateTheoreticalEffectSizes(m1, std1, type = type)#
  G2.results = CalculateTheoreticalEffectSizes(m2, std2, type = type)#
  if (type != "g") {#
    # Except for the gamma the block effect is applied to the central location parameter#
    G3.results = CalculateTheoreticalEffectSizes(m3 + BE, std3, type = type)#
    G4.results = CalculateTheoreticalEffectSizes(m4 + BE, std4, type = type)#
  }#
#
  if (type == "g") {#
    # This function applies the blocking effect to the shape function for the gamma distibution#
    G3.results = CalculateTheoreticalEffectSizes(m3 , std3 + BE, type = type)#
    G4.results = CalculateTheoreticalEffectSizes(m4 , std4 + BE, type = type)#
  }#
#
  # Calculate the expected unstandardized effect size allowing for the experimental design#
  ES = (G2.results$RawMean - G1.results$RawMean + G4.results$RawMean - G3.results$RawMean) /#
    2#
  # Calculate the within groups pooled variance#
  Var = (#
    G2.results$RawVariance + G1.results$RawVariance + G3.results$RawVariance +#
      G4.results$RawVariance#
  ) / 4#
  # Calculate the standarized mean difference effect size#
  StdES = ES / sqrt(Var)#
  # Calculate the point bi-serial effect size#
  rPBS = StdES / sqrt(StdES ^ 2 + 4)#
  output = tibble::tibble(ES, Var, StdES, rPBS)#
  return(output)#
}
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)#
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=10,type="n",seed=123,StdAdj=0,returnData=TRUE)
DT=NULL
d=0.5
phat=0.75
StdES=0.8
DataTable = tibble::tibble(base::rbind(#
        DataTable,#
        base::cbind(#
          d,#
          phat,#
          StdES)))
DT = tibble::tibble(base::rbind(#
        DT,#
        base::cbind(#
          d,#
          phat,#
          StdES)))
DT
DT=NULL
DT = tibble::tibble(base::rbind(#
+         DT,#
+         base::cbind(#
+           Cliffd=d,#
+           PHat=phat,#
+           StdESStdES)))
DT = tibble::tibble(base::rbind(#
+         DT,#
+         base::cbind(#
+           Cliffd=d,#
+           PHat=phat,#
+           StdES=StdES)))
DT = tibble::tibble(base::rbind(#
+         DT,#
+         base::cbind(#
+           d,#
+           phat,#
+           StdES)))
d
phat
DT=Null
DT=NULL
DT = tibble::tibble(base::rbind(#
+ +         DT,#
+ +         base::cbind(#
+ +           d,#
+ +           phat,#
+ +           StdES)))
DT
DT=tibble::tibble(base::rbind(ST,base::cbind(d,phat,StdES)))
DT=tibble::tibble(base::rbind(DT,base::cbind(d,phat,StdES)))
d=0.3
phat=0.65
StdES=0.6
DT=tibble::tibble(base::rbind(DT,base::cbind(d,phat,StdES)))
base::cbind(d,phat,StdES)
DT
NULL
DT=base::rbind(DT,base::cbind(d,phat,StdES))
DT=Null
DT=NULL
DT=base::rbind(DT,base::cbind(d,phat,StdES))
DT
d=0.5
phat=0.75
StdES=0.8
DT=base::rbind(DT,base::cbind(d,phat,StdES))
DT
RandomExperimentSimulations = function(mean,#
                                       sd,#
                                       diff,#
                                       N,#
                                       reps,#
                                       type = "n",#
                                       seed = 123,#
                                       StdAdj = 0,#
                                       returnData = FALSE) {#
  phatsum = 0 # This is used to sum the value of phat across the replications#
  phatvarsum = 0  # This is used to sum the value of the variance of phat across the replications#
  sig.phat = 0 # This is used to sum the number of times pat is significant across the replications#
  phatsquare = 0 # This  is used to sum phat^2 and construct an empirical variance of phat#
  dsum = 0 # This is used to sum the value of Cliff's d across the replications#
  dvarsum = 0  # This is used to sum the value of the variance of Cliff's d across the replications#
  sig.d = 0 # This is used to sum the number of times d is significant across the replications#
  dsquare = 0 # This  is used to sum d^2 and construct an empirical variance of d.#
#
  ksum = 0 # This is used to sum the value of the point biserial tau across the replications#
  kvarsum = 0 # This is used to sum the value of the variance of the point biserial tau across the replications#
  ksquare = 0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
  ksigCVt = 0#
  tsig = 0 # This is used to count the number of significant t values across the replications#
  ES = 0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
  StdES = 0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
  Var = 0 # This is used to sum the value of the variance across replications#
  ES.l.trans = 0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
  StdES.l.trans = 0 # This is used to sum the transformed standardized effect size for lognormal data sets#
  Var.l.trans = 0 # This is used to sum the transformed variance for lognormal data sets#
  MedDiff = 0 # Used to hold the median difference#
#
  DataTable = NULL#
#
  base::set.seed(seed)#
  for (i in 1:reps) {#
    # Call the program that generates the random data sets and calculates the sample statistics.#
    res = simulateRandomzedDesignEffectSizes(mean, sd, diff, N, type, StdAdj)#
#
    if (returnData == FALSE) {#
      # Aggregate data to provide counts of significance and overall effect size averages#
      # Cliff's d#
      dsum = dsum + res$d#
      dvarsum = dvarsum + res$vard#
      if (res$sigd)#
        sig.d = sig.d + 1#
      dsquare = dsquare + res$d ^ 2#
#
      # Probability of superiority#
      phatsum = phatsum + res$phat#
      phatvarsum = phatvarsum + res$varphat#
      if (res$sigphat)#
        sig.phat = sig.phat + 1#
      phatsquare = phatsquare + res$phat ^ 2#
#
      # Point biserial Kendall's tau#
      ksum = ksum + res$cor#
      kvarsum = kvarsum + res$varcor#
      ksquare = ksquare + res$cor ^ 2#
      ksigCVt = ksigCVt + if (res$sigCVt)#
        1#
      else#
        0#
#
      # Parametric statistics#
      ES = ES + res$ES#
      StdES = StdES + res$StdES#
      MedDiff = MedDiff + res$MedDiff#
      tsig = tsig + if (res$ttestp < .05)#
        1#
      else#
        0#
      Var = Var + res$Variance#
#
      if (type == "l") {#
        ES.l.trans = ES.l.trans + res$EStrans#
        StdES.l.trans = StdES.l.trans + res$StdEStrans#
        Var.l.trans = Var.l.trans + res$VarTrans#
      }#
    }#
    else {#
      # Store the outcome from each replication#
     # DataTable = tibble::tibble(base::rbind(#
     DataTable = base::rbind(#
        DataTable,#
        base::cbind(#
          Cliffd = res$d,#
          PHat = res$phat,#
          StdES = res$StdES#
    #    )#
      ))#
    }#
#
  }#
#
  if (returnData == FALSE) {#
    #Calculate averages of the statistics across the replications#
    d = dsum / reps#
    dvar = dvarsum / (reps)#
    sigd = sig.d / (reps)#
    emp.d.var = (dsquare - reps * d ^ 2) / (reps - 1)#
#
    phat = phatsum / reps#
    phatvar = phatvarsum / (reps)#
    sigphat = sig.phat / (reps)#
    emp.phat.var = (phatsquare - reps * phat ^ 2) / (reps - 1)#
    ktau = ksum / reps#
    ktauvar = kvarsum / reps#
    emp.tau.var = (ksquare - reps * ktau ^ 2) / (reps - 1)#
#
    kpowerCVt = ksigCVt / reps#
#
    ES = ES / reps#
    StdES = StdES / reps#
    MedDiff = MedDiff / reps#
    Variance = Var / reps#
    tpower = tsig / reps#
#
    if (type == "l")#
    {#
      # This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
      ESLog = ES.l.trans / reps#
      StdESLog = StdES.l.trans / reps#
      VarLog = Var.l.trans / reps#
    }#
#
    if (type == "l")	{#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff,#
        ESLog = ESLog,#
        StdESLog = StdESLog,#
        VarLog = VarLog#
      )#
    }#
    else {#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff#
      )#
    }#
  }#
  else {#
    #outcome = DataTable#
    outcome=tibble::tibble(DataTable)#
  }#
  return(outcome)#
#
}
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=10,type="n",seed=123,StdAdj=0,returnData=TRUE)
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="l",seed=123,StdAdj=0)
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=500,type="n",seed=123,StdAdj=0)
simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="n",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)
set.seed(123)
simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="n",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)
set.seed(123)
simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="l",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)
simulateRandomzedDesignEffectSizes(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)
set.seed(123)
simulateRandomzedDesignEffectSizes(mean=0,sd=1,diff=0.8,N=10,type="n",StdAdj=0)
set.seed(123)
simulateRandomzedDesignEffectSizes(mean=0,sd=1,diff=0.8,N=10,type="l",StdAdj=0)
tibble::tibble
quos
DT
tibble::tibble(DT)
tibble::tibble(d=DT$d,Phat=DT$Phat,StdES=DT$StdES)
DT$d
DT
DT=NULL
DT=rbind(DT,cbind(Cliffd=d,Phat=phat, StdES = res$StdES))
DT=rbind(DT,cbind(Cliffd=d,Phat=phat, StdES = StdES))
DT
DT$Cliffd
as.dats.frame(DT)
as.data.frame(DT)
as.data.frame(DT)$Cliffd
DT=rbind(DT,cbind(Cliffd=.6,Phat=.7, StdES = .5))
DT
as.data.frame(DT)$Cliffd
as.data.frame(DT)
RandomExperimentSimulations = function(mean,#
                                       sd,#
                                       diff,#
                                       N,#
                                       reps,#
                                       type = "n",#
                                       seed = 123,#
                                       StdAdj = 0,#
                                       returnData = FALSE) {#
  phatsum = 0 # This is used to sum the value of phat across the replications#
  phatvarsum = 0  # This is used to sum the value of the variance of phat across the replications#
  sig.phat = 0 # This is used to sum the number of times pat is significant across the replications#
  phatsquare = 0 # This  is used to sum phat^2 and construct an empirical variance of phat#
  dsum = 0 # This is used to sum the value of Cliff's d across the replications#
  dvarsum = 0  # This is used to sum the value of the variance of Cliff's d across the replications#
  sig.d = 0 # This is used to sum the number of times d is significant across the replications#
  dsquare = 0 # This  is used to sum d^2 and construct an empirical variance of d.#
#
  ksum = 0 # This is used to sum the value of the point biserial tau across the replications#
  kvarsum = 0 # This is used to sum the value of the variance of the point biserial tau across the replications#
  ksquare = 0  # This is used to sum the square of tau_pb across the replications and construct an empirical variance#
  ksigCVt = 0#
  tsig = 0 # This is used to count the number of significant t values across the replications#
  ES = 0 # This is used to sum the value of the parametric effect size (unstandardized) across the replications#
  StdES = 0 # This is used to sum the value of the parametric effect size (standardized) across the replications#
  Var = 0 # This is used to sum the value of the variance across replications#
  ES.l.trans = 0 # This is used to sum the transformed unstandardized effect size for lognormal data sets#
  StdES.l.trans = 0 # This is used to sum the transformed standardized effect size for lognormal data sets#
  Var.l.trans = 0 # This is used to sum the transformed variance for lognormal data sets#
  MedDiff = 0 # Used to hold the median difference#
#
  DataTable = NULL#
#
  base::set.seed(seed)#
  for (i in 1:reps) {#
    # Call the program that generates the random data sets and calculates the sample statistics.#
    res = simulateRandomzedDesignEffectSizes(mean, sd, diff, N, type, StdAdj)#
#
    if (returnData == FALSE) {#
      # Aggregate data to provide counts of significance and overall effect size averages#
      # Cliff's d#
      dsum = dsum + res$d#
      dvarsum = dvarsum + res$vard#
      if (res$sigd)#
        sig.d = sig.d + 1#
      dsquare = dsquare + res$d ^ 2#
#
      # Probability of superiority#
      phatsum = phatsum + res$phat#
      phatvarsum = phatvarsum + res$varphat#
      if (res$sigphat)#
        sig.phat = sig.phat + 1#
      phatsquare = phatsquare + res$phat ^ 2#
#
      # Point biserial Kendall's tau#
      ksum = ksum + res$cor#
      kvarsum = kvarsum + res$varcor#
      ksquare = ksquare + res$cor ^ 2#
      ksigCVt = ksigCVt + if (res$sigCVt)#
        1#
      else#
        0#
#
      # Parametric statistics#
      ES = ES + res$ES#
      StdES = StdES + res$StdES#
      MedDiff = MedDiff + res$MedDiff#
      tsig = tsig + if (res$ttestp < .05)#
        1#
      else#
        0#
      Var = Var + res$Variance#
#
      if (type == "l") {#
        ES.l.trans = ES.l.trans + res$EStrans#
        StdES.l.trans = StdES.l.trans + res$StdEStrans#
        Var.l.trans = Var.l.trans + res$VarTrans#
      }#
    }#
    else {#
      # Store the outcome from each replication#
     # DataTable = tibble::tibble(base::rbind(#
     DataTable = base::rbind(#
        DataTable,#
        base::cbind(#
          Cliffd = res$d,#
          PHat = res$phat,#
          StdES = res$StdES#
    #    )#
      ))#
    }#
#
  }#
#
  if (returnData == FALSE) {#
    #Calculate averages of the statistics across the replications#
    d = dsum / reps#
    dvar = dvarsum / (reps)#
    sigd = sig.d / (reps)#
    emp.d.var = (dsquare - reps * d ^ 2) / (reps - 1)#
#
    phat = phatsum / reps#
    phatvar = phatvarsum / (reps)#
    sigphat = sig.phat / (reps)#
    emp.phat.var = (phatsquare - reps * phat ^ 2) / (reps - 1)#
    ktau = ksum / reps#
    ktauvar = kvarsum / reps#
    emp.tau.var = (ksquare - reps * ktau ^ 2) / (reps - 1)#
#
    kpowerCVt = ksigCVt / reps#
#
    ES = ES / reps#
    StdES = StdES / reps#
    MedDiff = MedDiff / reps#
    Variance = Var / reps#
    tpower = tsig / reps#
#
    if (type == "l")#
    {#
      # This is used for validation that the algorithms are consistent. The statistics from the transformed lognormal data can be compared with the statistics from the normal data.#
      ESLog = ES.l.trans / reps#
      StdESLog = StdES.l.trans / reps#
      VarLog = Var.l.trans / reps#
    }#
#
    if (type == "l")	{#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff,#
        ESLog = ESLog,#
        StdESLog = StdESLog,#
        VarLog = VarLog#
      )#
    }#
    else {#
      outcome = tibble::tibble(#
        phat,#
        phatvar,#
        sigphat,#
        emp.phat.var,#
        d,#
        dvar,#
        sigd,#
        emp.d.var,#
        ktau,#
        ktauvar,#
        emp.tau.var,#
        kpowerCVt,#
        tpower,#
        ES,#
        Variance,#
        StdES,#
        MedDiff#
      )#
    }#
  }#
  else {#
    #outcome = DataTable#
    DataTable=as.data.frame(DataTable)#
    outcome=tibble::tibble(DataTable)#
  }#
  return(outcome)#
#
}
RandomExperimentSimulations(mean=0,sd=1,diff=0.5,N=20, reps=10,type="n",seed=123,StdAdj=0,returnData=TRUE)
simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="l",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)
set.seed(123)
simulateRandomizedBlockDesignEffectSizes(mean=0,sd=1,diff=.5,N=10,type="l",alpha=0.05,Blockmean=0.5,BlockStdadj=0,StdAdj=0)
set.seed(123)#
simulateRandomizedBlockDesignEffectSizes(#
  mean = 0,#
  sd = 1,#
  diff = .5,#
  N = 10,#
  type = "n",#
  alpha = 0.05,#
  Blockmean = 0.5,#
  BlockStdadj = 0,#
  StdAdj = 0#
)#
set.seed(123)#
simulateRandomizedBlockDesignEffectSizes(#
  mean = 0,#
  sd = 1,#
  diff = .5,#
  N = 10,#
  type = "l",#
  alpha = 0.05,#
  Blockmean = 0.5,#
  BlockStdadj = 0,#
  StdAdj = 0#
)
RandomizedBlocksExperimentSimulations(mean=0,sd=1,diff=0.5,N=10, reps=500,type="n",alpha=0.05,Blockmean=0.5, BlockStdadj=0,StdAdj=0,seed=123)
RandomizedBlocksExperimentSimulations(mean=0,sd=1,diff=0.5,N=10, reps=10,type="n",alpha=0.05,Blockmean=0.5, BlockStdadj=0,StdAdj=0,seed=123,returnData=T)
NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM")
NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.724,GroupSize=10,Exp=5,type="l",alpha=0.05,seed=123,StdAdj=0,BlockEffec#'#t=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM")
NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.724,GroupSize=10,Exp=5,type="l",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM")
NP4GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",alpha=0.05,seed=123,StdAdj=0,BlockEffect=0.5,BlockStdadj=0,StdExp=0,MAMethod="PM",returnES=TRUE)
NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=100,Exp=5,type="n",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=FALSE)
NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.5,GroupSize=10,Exp=5,type="n",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=TRUE)
NP2GroupMetaAnalysisSimulation(mean=0,sd=1,diff=0.724,GroupSize=10,Exp=5,type="l",StdAdj=0,alpha=0.05,seed=123,StdExp=1,MAMethod="PM",returnES=FALSE)
MetaAnalysisSimulations(mean=0,sd=1,diff=0.5,GroupSize=10,type="n",Replications=50,Exp=5,seed=456,alpha=0.05,FourGroup=FALSE,StdAdj=0,BlockEffect=0,BlockStdadj=0,StdExp=0,MAMethod="PM")
q()
reproducer::calculateSmallSampleSizeAdjustment(550,TRUE)
reproducer::calculateSmallSampleSizeAdjustment(250,TRUE)
reproducer::calculateSmallSampleSizeAdjustment(550,FALSE)
